#!/usr/bin/env python3
"""
Evaluate multiple trained models using Promptfoo and SkyPilot.

Example usage:
    python evaluate_models.py
    python evaluate_models.py --config eval_config_test.yaml
"""

import argparse
import os
import subprocess
from typing import Dict, List

import utils
import yaml

import sky


def load_eval_config() -> Dict:
    """Load evaluation configurations."""
    with open(utils.EVAL_CONFIG, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def prepare_model_task(model: Dict,
                       cluster_prefix: str = "eval",
                       serve_template: str = None) -> tuple:
    """Prepare a model task for launching."""
    name = model['name']
    cluster_name = f"{cluster_prefix}-{name}"

    # Get model path, mounts, and volumes
    model_path, file_mounts, volumes = utils.get_model_path_and_mounts(
        model['source'])

    # Use model-specific template if provided, then global template, then default
    template_path = model.get(
        'serve_template') or serve_template or utils.SERVE_TEMPLATE

    # Load task template - this includes all defaults (resources, envs, setup, run, etc.)
    task = sky.Task.from_yaml(template_path)
    task.name = f"serve-{name}"

    # Override environment variables
    task.update_envs({
        'MODEL_PATH': model_path,
        'API_TOKEN': utils.API_TOKEN,
        'HF_TOKEN': os.environ.get('HF_TOKEN', None)
    })

    # Override resources only if explicitly specified in model config
    # The template already has default resources loaded
    if 'accelerators' in model or 'infra' in model:
        # Update each resource in the set with the new configuration
        updated_resources = []
        for resource in task.resources:
            resource_updates = {}
            if 'accelerators' in model:
                resource_updates['accelerators'] = model['accelerators']
            if 'infra' in model:
                # infra can be a string (cloud name) or dict with more options
                resource_updates['infra'] = model['infra']
            updated_resources.append(resource.copy(**resource_updates))
        task.set_resources(updated_resources)

    # Set file mounts if needed
    if file_mounts:
        task.set_file_mounts(file_mounts)

    # Set volumes if needed (for Kubernetes)
    if volumes:
        task.set_volumes(volumes)

    return task, cluster_name, name


def launch_models_parallel(models: List[Dict],
                           cluster_prefix: str = "eval",
                           serve_template: str = None) -> List[Dict]:
    """Launch all models in parallel using futures."""
    print(f"\n{'='*60}")
    print(f"üöÄ LAUNCHING {len(models)} MODELS IN PARALLEL")
    print(f"{'='*60}\n")

    # Check existing clusters and prepare launch requests
    launch_requests = {}
    existing_models = []

    for i, model in enumerate(models, 1):
        try:
            task, cluster_name, name = prepare_model_task(
                model, cluster_prefix, serve_template)

            # Determine the model ID that vLLM will use
            # Get the model path for all sources
            actual_model_id, _, _ = utils.get_model_path_and_mounts(
                model.get('source'))

            # Check if cluster already exists and is serving
            exists, _ = utils.check_cluster_ready(cluster_name, utils.API_TOKEN)

            if exists:
                print(f"[{i}/{len(models)}] {name}: cluster exists")
                # Cancel any running jobs before re-launching
                try:
                    print(
                        f"  Cancelling existing jobs if any on {cluster_name}..."
                    )
                    cancel_request = sky.cancel(cluster_name, all=True)
                    sky.get(cancel_request)
                    print(f"  ‚úì Cancelled existing jobs")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  Could not cancel jobs: {e}")

                print(f"  Re-launching task...")
                request_id = sky.launch(task,
                                        cluster_name=cluster_name,
                                        fast=False)
                launch_requests[name] = {
                    'request_id': request_id,
                    'cluster_name': cluster_name,
                    'model': model,
                    'model_path': actual_model_id
                }
            else:
                print(f"[{i}/{len(models)}] Launching {name}...")
                request_id = sky.launch(task,
                                        cluster_name=cluster_name,
                                        fast=True)
                launch_requests[name] = {
                    'request_id': request_id,
                    'cluster_name': cluster_name,
                    'model': model,
                    'model_path': actual_model_id
                }
        except Exception as e:
            print(f"  ‚ùå Failed to launch: {e}")

    # Combine existing models with newly launched ones
    launched_models = existing_models.copy()

    if launch_requests:
        # Wait for all launches to complete
        print(f"\n{'‚îÄ'*60}")
        print("‚è≥ WAITING FOR CLUSTERS TO PROVISION")
        print(f"{'‚îÄ'*60}\n")

        for name, info in launch_requests.items():
            try:
                # Get the launch result
                result = sky.get(info['request_id'])
                job_id = result[0]
                print(f"  ‚úÖ {name}: cluster provisioned")

                # Get cluster status to find endpoint
                endpoint = utils.get_cluster_endpoint(info['cluster_name'])

                if endpoint:
                    print(f"Endpoint: {endpoint}")
                    launched_models.append({
                        'name': name,
                        'cluster_name': info['cluster_name'],
                        'endpoint': endpoint,
                        'source': info['model'].get('source', 'unknown'),
                        'job_id': str(job_id),
                        'model_path': info.get('model_path')
                    })
                else:
                    print(f"  ‚ö†Ô∏è  {name}: endpoint not found")
            except Exception as e:
                print(f"  ‚ùå {name}: provisioning failed - {e}")

    # Wait for all models to be ready
    if launched_models:
        print(f"\n{'‚îÄ'*60}")
        print("üîÑ WAITING FOR MODEL SERVERS TO START")
        print(f"{'‚îÄ'*60}\n")

        ready_models = []
        for model in launched_models:
            # Skip waiting for existing models that are already verified
            if model['job_id'] == 'existing':
                print(f"  ‚úÖ {model['name']}: already serving (skipping wait)")
                ready_models.append(model)
            elif utils.wait_for_model_ready(model['cluster_name'],
                                            job_id=model['job_id']):
                # Verify endpoint is accessible
                if utils.verify_endpoint(model['endpoint'], utils.API_TOKEN):
                    print(
                        f"  üåê {model['name']}: endpoint verified at http://{model['endpoint']}"
                    )
                    if model.get('model_path'):
                        print(f"      Using model: {model['model_path']}")
                    ready_models.append(model)
                else:
                    print(f"  ‚ö†Ô∏è  {model['name']}: endpoint not accessible")
            else:
                print(f"  ‚ö†Ô∏è  {model['name']}: server didn't start in time")

        return ready_models

    return launched_models


def create_evaluation_config(models: List[Dict], config: Dict,
                             output_path: str):
    """Create Promptfoo configuration by merging deployed models with eval config."""
    # Build provider list from deployed models
    providers = []
    for model in models:
        if model:
            # Get model ID - use the actual model path from server if available
            model_id = model.get('model_path', 'auto')

            # Use openai provider with custom endpoint
            providers.append({
                'id': f'openai:chat:{model_id}',
                'label': model["name"],
                'config': {
                    'apiBaseUrl': f"http://{model['endpoint']}/v1",
                    'apiKey': utils.API_TOKEN,
                    'temperature': 0.7,
                    'max_tokens': 512
                }
            })

    # Start with the promptfoo config from the YAML (or use defaults)
    promptfoo_config = config.get('promptfoo', {})

    # If no promptfoo config provided, use sensible defaults
    if not promptfoo_config:
        promptfoo_config = {
            'description': 'Multi-model evaluation',
            'prompts': ["You are a helpful AI assistant. {{message}}"],
            'tests': [{
                'vars': {
                    'message': 'What is quantum computing?'
                },
                'assert': [{
                    'type': 'contains',
                    'value': 'quantum'
                }]
            }, {
                'vars': {
                    'message': 'Write a hello world in Python'
                },
                'assert': [{
                    'type': 'contains',
                    'value': 'print'
                }]
            }],
            'outputPath': './results.json'
        }

    # Merge the providers into the promptfoo config
    promptfoo_config['providers'] = providers

    # Ensure outputPath is set
    if 'outputPath' not in promptfoo_config:
        promptfoo_config['outputPath'] = './results.json'

    with open(output_path, 'w', encoding='utf-8') as f:
        yaml.dump(promptfoo_config, f, default_flow_style=False)

    num_tests = len(promptfoo_config.get('tests', []))
    print(
        f"\nüìù Created evaluation config for {len(providers)} models with {num_tests} tests"
    )


def run_evaluation(config_path: str):
    """Run Promptfoo evaluation."""
    print("\nüîç Running evaluation...")

    # Set environment variable to disable SSL verification for local endpoints
    env = os.environ.copy()
    env['NODE_TLS_REJECT_UNAUTHORIZED'] = '0'

    result = subprocess.run(
        ['promptfoo', 'eval', '-c', config_path, '--no-progress-bar'],
        capture_output=True,
        text=True,
        check=False,
        env=env)

    if result.returncode == 0:
        print("‚úÖ Evaluation complete!")
        print("\nView results: promptfoo view")
    else:
        print(f'‚ùå Evaluation failed: {result.returncode} {result.stdout}\n'
              f'{result.stderr}')


def get_existing_clusters(models: List[Dict],
                          cluster_prefix: str = "eval") -> List[Dict]:
    """Get endpoints for existing clusters without launching."""
    print(f"\n{'='*60}")
    print(f"üîç CHECKING EXISTING CLUSTERS")
    print(f"{'='*60}\n")

    existing_models = []
    for i, model in enumerate(models, 1):
        name = model['name']
        cluster_name = f"{cluster_prefix}-{name}"

        # Get the model path for all sources
        actual_model_path, _, _ = utils.get_model_path_and_mounts(
            model.get('source'))

        print(f"[{i}/{len(models)}] Checking {name}...")

        # Check if cluster exists and get endpoint
        exists, endpoint = utils.check_cluster_ready(cluster_name,
                                                     utils.API_TOKEN)

        if exists and endpoint:
            print(f"  ‚úÖ Found at {endpoint}")
            existing_models.append({
                'name': name,
                'cluster_name': cluster_name,
                'endpoint': endpoint,
                'source': model.get('source', 'unknown'),
                'model_path': actual_model_path
            })
        else:
            print(f"  ‚ùå Not found or not ready")

    return existing_models


def cleanup_clusters(models: List[Dict]):
    """Terminate all clusters."""
    print("\nüßπ Cleaning up...")

    for model in models:
        if model:
            try:
                request_id = sky.down(model['cluster_name'])
                result = sky.get(request_id)
                print(f"  ‚úì Terminated {model['cluster_name']}")
            except Exception as e:
                print(f"  ‚úó Failed to terminate {model['cluster_name']}: {e}")


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description='Evaluate multiple models using Promptfoo and SkyPilot',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Launch clusters and run evaluation (default)
  python evaluate_models.py
  
  # Use custom config file
  python evaluate_models.py --config models_config_test.yaml
  
  # To skip launch or cleanup, edit the config file:
  # eval_only: true      # Use existing clusters
  # cleanup_on_complete: false  # Keep clusters running
        """)

    parser.add_argument(
        '--config',
        default=utils.EVAL_CONFIG,
        help=f'Evaluation configuration file (default: {utils.EVAL_CONFIG})')

    return parser.parse_args()


def main():
    """Main workflow."""
    # Parse arguments
    args = parse_arguments()

    print("üéØ Multi-Model Evaluation with Parallel Launch")
    print("=" * 45)

    if args.config != utils.EVAL_CONFIG:
        print(f"üìã Using config: {args.config}")

    # Load configuration
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    models = config['models']
    eval_only = config.get('eval_only', False)
    cleanup_on_complete = config.get('cleanup_on_complete', True)
    cluster_prefix = config.get('cluster_prefix', 'eval')
    default_serve_template = config.get('default_serve_template', None)

    # Show configuration settings
    print(f"üóíÔ∏è Using cluster prefix: '{cluster_prefix}'")
    if default_serve_template:
        print(
            f"üìÑ Using custom default serve template: {default_serve_template}")
    if eval_only:
        print("üìä Evaluation-only mode (using existing clusters)")
    if not cleanup_on_complete:
        print("üîí Clusters will not be terminated after evaluation")

    # Either get existing clusters or launch new ones
    if eval_only:
        launched_models = get_existing_clusters(models, cluster_prefix)
        if not launched_models:
            print(
                "\n‚ùå No existing clusters found. Please launch clusters first or set eval_only: false"
            )
            return
        print(
            f"\nüéâ Found {len(launched_models)}/{len(models)} existing clusters")
    else:
        # Launch all models in parallel
        launched_models = launch_models_parallel(models, cluster_prefix,
                                                 default_serve_template)
        if not launched_models:
            print("\n‚ùå No models launched successfully")
            return
        print(
            f"\nüéâ Successfully launched {len(launched_models)}/{len(models)} models"
        )

    try:
        # Create and run evaluation
        create_evaluation_config(launched_models, config,
                                 'promptfoo_config.yaml')
        run_evaluation('promptfoo_config.yaml')

    finally:
        # Cleanup based on config settings
        if cleanup_on_complete and not eval_only:
            cleanup_clusters(launched_models)


if __name__ == '__main__':
    main()
