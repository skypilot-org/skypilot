# Default configuration for serving models with vLLM
# The Python script will override these values as needed

envs:
  MODEL_PATH: meta-llama/Llama-2-7b-hf  # Default model path
  API_TOKEN: default-token

resources:
  # infra: aws  # Optional: specify cloud provider (aws, gcp, azure, etc.)
  accelerators: L4:1
  ports: 8000

setup: |
  uv venv --seed --python=3.10
  source .venv/bin/activate
  uv pip install vllm==0.10.0
  uv pip install flashinfer-python==0.2.10

run: |
  source .venv/bin/activate
  echo "Starting vLLM server for model: $MODEL_PATH"
  vllm serve $MODEL_PATH \
    --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \
    --host 0.0.0.0 \
    --port 8000 \
    --max-model-len 2048 \
    --api-key "$API_TOKEN"
