# Configuration for serving models with Ollama
# Ollama provides easy model management and serving

envs:
  MODEL_PATH: llama2:7b  # Default Ollama model (will be overridden by evaluate_models.py)
  API_TOKEN: default-token

resources:
  # infra: aws  # Optional: specify cloud provider (aws, gcp, azure, etc.)
  accelerators: L4:1
  ports: 8000

setup: |
  # Install Ollama
  curl -fsSL https://ollama.ai/install.sh | sh
  
  # Install proxy to add authentication to Ollama API
  pip install fastapi uvicorn httpx

  # Create a simple proxy script to add authentication
  cat > ollama_proxy.py << 'EOF'
  from fastapi import FastAPI, HTTPException, Request, Header
  from fastapi.responses import StreamingResponse
  import httpx
  import os
  import json

  app = FastAPI()
  OLLAMA_URL = "http://localhost:11434"
  API_TOKEN = os.environ.get("API_TOKEN", "default-token")

  @app.api_route("/{path:path}", methods=["GET", "POST", "PUT", "DELETE"])
  async def proxy(path: str, request: Request, authorization: str = Header(None)):
      # Check API token
      if authorization != f"Bearer {API_TOKEN}":
          raise HTTPException(status_code=401, detail="Unauthorized")
      
      # Forward request to Ollama
      async with httpx.AsyncClient() as client:
          url = f"{OLLAMA_URL}/{path}"
          headers = dict(request.headers)
          headers.pop("authorization", None)
          headers.pop("host", None)
          
          response = await client.request(
              method=request.method,
              url=url,
              headers=headers,
              content=await request.body(),
              follow_redirects=True
          )
          
          return StreamingResponse(
              response.iter_bytes(),
              status_code=response.status_code,
              headers=dict(response.headers)
          )
  EOF

run: |
  echo "Starting Ollama server for model: $MODEL_PATH"
  
  # Start Ollama service
  ollama serve &
  sleep 5
  
  # MODEL_PATH contains the Ollama model ID directly (e.g., tinyllama:1.1b)
  echo "Pulling Ollama model: $MODEL_PATH"
  ollama pull $MODEL_PATH
  
  # Verify model was pulled
  if ! ollama list | grep -q "$MODEL_PATH"; then
    echo "Warning: Model $MODEL_PATH may not have been pulled successfully"
    echo "Available models:"
    ollama list
  fi
  
  # Start the authentication proxy on port 8000
  uvicorn ollama_proxy:app --host 0.0.0.0 --port 8000
