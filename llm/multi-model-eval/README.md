# Multi-Model Evaluation Example

Compare multiple trained models side-by-side using Promptfoo and SkyPilot.

## Architecture Overview

```
                    models_config.yaml
                           â”‚
                           â–¼
                  python evaluate_models.py
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      SkyPilot        â”‚
                â”‚  Parallel Launch     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                  â–¼                  â–¼
   HuggingFace          S3 Bucket      SkyPilot Volume
   mistral-7b         agent-qwen       agent-deepseek
        â”‚                  â”‚                  â”‚
        â–¼                  â–¼                  â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Cluster â”‚        â”‚ Cluster â”‚       â”‚ Cluster â”‚
   â”‚ â€¢ L4 GPUâ”‚        â”‚ â€¢ L4 GPUâ”‚       â”‚ â€¢ L4 GPUâ”‚
   â”‚ â€¢ vLLM  â”‚        â”‚ â€¢ vLLM  â”‚       â”‚ â€¢ vLLM  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚                  â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                   OpenAI-compatible APIs
                           â”‚
                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Promptfoo   â”‚
                   â”‚  Evaluation  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components:

1. **SkyPilot SDK**: Orchestrates the entire deployment
   - Loads serving configuration from YAML template
   - Launches multiple clusters in parallel
   - Handles cloud resources and GPU allocation

2. **SkyPilot Clusters**: Each model runs on its own cluster
   - Automatic GPU provisioning based on model size
   - vLLM for high-performance inference
   - OpenAI-compatible API endpoints

3. **Flexible Model Sources**: 
   - HuggingFace Hub for public models
   - Cloud buckets (S3/GCS) for custom checkpoints
   - SkyPilot Volumes for fast repeated access

## Quick Start

```bash
# 1. Install dependencies
pip install skypilot[all] pyyaml
npm install -g promptfoo

# 2. Configure models (edit models_config.yaml)
# 3. Run evaluation
python evaluate_models.py
```

## Project Structure

```
multi-model-eval/
â”œâ”€â”€ evaluate_models.py      # Main script
â”œâ”€â”€ models_config.yaml      # Your model configurations
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ serve-model.yaml    # vLLM serving template
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ test_setup.sh       # Check dependencies
â”œâ”€â”€ finetuning/             # Fine-tuning examples
â”‚   â”œâ”€â”€ finetune.py         # Fine-tuning script
â”‚   â”œâ”€â”€ finetune-to-s3.yaml
â”‚   â”œâ”€â”€ finetune-to-volume.yaml
â”‚   â””â”€â”€ README.md
â””â”€â”€ README.md
```

## Model Configuration

Edit `models_config.yaml` to specify your models:

```yaml
models:
  # Public model from HuggingFace
  - name: "mistral-7b"
    source: "huggingface"
    model_id: "mistralai/Mistral-7B-Instruct-v0.3"
    accelerators: "L4:1"
    
  # Custom model from S3 bucket
  - name: "agent-qwen"
    source: "s3://my-models/qwen-7b-agent"
    accelerators: "L4:1"
    
  # Model from SkyPilot volume
  - name: "agent-deepseek"
    source: "volume://model-checkpoints/deepseek-7b-agent"
    accelerators: "L4:1"

cleanup_on_complete: true
```

## How It Works

1. **Configure Models**: Edit `models_config.yaml` with your model sources
2. **Launch with SkyPilot**: The script uses SkyPilot SDK to deploy each model on its own GPU cluster
3. **Evaluate with Promptfoo**: All models receive the same test prompts
4. **Compare Results**: View outputs side-by-side in the Promptfoo UI

## Model Sources

- **HuggingFace**: Any public model from the Hub
- **S3/GCS**: Your trained models in cloud storage
- **Volumes**: Models stored in SkyPilot volumes for fast loading

### Using SkyPilot Volumes (Kubernetes)

For Kubernetes deployments, SkyPilot volumes provide persistent storage for models:

1. **Create a volume** (if not already exists):
   ```bash
   # For GKE clusters:
   sky volumes apply finetuning/create-volume.yaml
   
   # Or create custom volume:
   cat > my-volume.yaml <<EOF
   name: model-checkpoints
   type: k8s-pvc
   infra: kubernetes
   size: 50Gi
   config:
     namespace: default
     storage_class_name: standard  # GKE default storage class
     access_mode: ReadWriteOnce   # Standard GKE persistent disks
   EOF
   
   sky volumes apply my-volume.yaml
   ```

2. **Reference in model config**:
   ```yaml
   models:
     - name: "mistral-custom"
       source: "volume://model-checkpoints/mistral-7b"
       accelerators: "A10:1"
   ```

   The path format is `volume://<volume-name>/<path-within-volume>`

3. **List existing volumes**:
   ```bash
   sky volumes ls
   ```

### Multiple Volumes and Buckets

The evaluation tool automatically handles multiple volumes and buckets by creating unique mount points:

```yaml
models:
  # Different S3 buckets get unique mount points
  - name: "model-1"
    source: "s3://bucket-a/models/llama"  # Mounts at /buckets/bucket-a/
    
  - name: "model-2"  
    source: "s3://bucket-b/checkpoints/qwen"  # Mounts at /buckets/bucket-b/
    
  # Different volumes also get unique mount points
  - name: "model-3"
    source: "volume://volume-1/agent-model"  # Mounts at /volumes/volume-1/
    
  - name: "model-4"
    source: "volume://volume-2/base-model"  # Mounts at /volumes/volume-2/
```

This prevents conflicts when using models from multiple sources.

## GPU Selection

Common configurations:
- `"L4:1"` - Good for 7B models
- `"A10:1"` - Good for 7-13B models  
- `"A100:1"` - For larger models
- `"A100-80GB:1"` - For 70B+ models

## Example Output

```
ğŸ¯ Multi-Model Evaluation
=========================

ğŸ“‹ Launching 3 models...

ğŸš€ Launching mistral-7b...
âœ… Launched eval-mistral-7b
ğŸ“¡ Endpoint: http://34.125.23.45:8000/v1

ğŸš€ Launching agent-qwen...
âœ… Launched eval-agent-qwen
ğŸ“¡ Endpoint: http://35.223.12.89:8000/v1

ğŸš€ Launching agent-deepseek...
âœ… Launched eval-agent-deepseek
ğŸ“¡ Endpoint: http://35.198.76.12:8000/v1

âœ… Successfully launched 3 models

ğŸ“ Created evaluation config for 3 models

ğŸ” Running evaluation...
âœ… Evaluation complete!

View results: promptfoo view
```

## Tips

- Models run on port 8000 (no configuration needed)
- Launch happens in parallel for speed
- Results saved to `results.json`
- View detailed comparison with `promptfoo view`

## Fine-tuning Workflow

See `finetuning/` for complete examples of:
1. Fine-tuning models and saving to S3 or volumes
2. Using fine-tuned models in evaluation

Quick example:
```bash
# Fine-tune and save to S3
cd finetuning
sky launch finetune-to-s3.yaml -c my-finetune

# Or save to volume (Kubernetes)
sky volumes apply create-volume.yaml  # One-time setup
sky launch finetune-to-volume.yaml -c my-finetune

# Then use in evaluation by updating models_config.yaml
```

## Troubleshooting

```bash
# Check dependencies
./scripts/test_setup.sh

# View model logs
sky logs eval-<model-name>

# List running clusters
sky status

# Manually cleanup
sky down -a
```
