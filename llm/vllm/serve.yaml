envs:
  MODEL_NAME: decapoda-research/llama-65b-hf

resources:
  accelerators: A100-80GB:8

setup: |
  uv venv ~/vllm --python 3.10 --seed
  source ~/vllm/bin/activate

  # Install fschat and accelerate for chat completion
  git clone https://github.com/vllm-project/vllm.git || true
  uv pip install transformers==4.38.0
  uv pip install vllm==0.3.2

  uv pip install gradio


run: |
  source ~/vllm/bin/activate
  echo 'Starting vllm api server...'
  python -u -m vllm.entrypoints.api_server \
                   --model $MODEL_NAME \
                   --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \
                   --tokenizer hf-internal-testing/llama-tokenizer 2>&1 | tee api_server.log &

  echo 'Waiting for vllm api server to start...'
  while ! `cat api_server.log | grep -q 'Uvicorn running on'`; do sleep 1; done

  echo 'Starting gradio server...'
  python vllm/examples/gradio_webserver.py
