# Full finetuning of Llama-4 Scout 17B MoE model with 16 experts.
#
# Usage:
#
# HF_TOKEN=xxx sky launch llama-4-scout-sft.yaml -c scout --env HF_TOKEN
#
# This config requires at least 2 nodes with 8x H200 GPUs each.

envs:
  HF_TOKEN:
  WANDB_NAME: ""

# Required if using W&B for experiment tracking
# secrets:
#   WANDB_API_KEY:

resources:
  cpus: 100+
  memory: 1000+
  accelerators: H100:8
  disk_size: 1024
  disk_tier: best

num_nodes: 2

workdir: .

# Optional: configure buckets for dataset and checkpoints. You can then use the
# /checkpoints directory to write checkpoints, which writes to local disk first
# and asynchronously uploads to the cloud bucket. Pass /checkpoints to the main
# training script.
# file_mounts:
#  /dataset:
#    source: s3://my-dataset-bucket
#    mode: COPY  # COPY mode will prefetch the dataset to the node for faster access
#  /checkpoints:
#    source: s3://my-checkpoint-bucket
#    mode: MOUNT_CACHED  # MOUNT_CACHED mode will intelligently cache the checkpoint for faster writes

setup: |
  uv venv .venv --python 3.10
  source .venv/bin/activate
  uv pip install torch==2.9.0 torchvision==0.24.0 torchao==0.14.1
  uv pip install git+https://github.com/meta-pytorch/torchtune.git@67ab86b94de9e7ac7dd9850113ebe69e2bbd307c

  # Download the model (~200 GB, may take time to download)
  tune download meta-llama/Llama-4-Scout-17B-16E-Instruct \
    --hf-token $HF_TOKEN

run: |
  source .venv/bin/activate
  # Configure W&B if API key is set
  if [ -n "$WANDB_API_KEY" ]; then
    export WANDB_NAME=${WANDB_NAME:-llama4-scout-sft-run}
    export WANDB_RUN_ID=$SKYPILOT_TASK_ID
    echo "W&B tracking enabled"
    uv pip install wandb
  fi
  export MASTER_ADDR=$(echo "$SKYPILOT_NODE_IPS" | head -n1)
  echo "Starting distributed finetuning, head node: $MASTER_ADDR"

  tune run \
  --nnodes $SKYPILOT_NUM_NODES \
  --nproc_per_node $SKYPILOT_NUM_GPUS_PER_NODE \
  --rdzv_id $SKYPILOT_TASK_ID \
  --rdzv_backend c10d \
  --rdzv_endpoint=$MASTER_ADDR:29500 \
  full_finetune_distributed \
  --config configs/scout_17B_16E_full.yaml \
  model_dir=/tmp/Llama-4-Scout-17B-16E-Instruct \
  max_steps_per_epoch=10 \
  epochs=1
