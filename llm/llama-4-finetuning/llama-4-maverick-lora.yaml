# Full finetuning of Llama-4 Maverick 17B MoE model with 128 experts.
#
# Usage:
#
# HF_TOKEN=xxx sky launch llama-4-maverick-lora.yaml -c maverick --env HF_TOKEN
#
# This config requires at least 2 nodes with 8x H100 GPUs each.

envs:
  HF_TOKEN:

# Required if `report_to: wandb` in `configs/llama4_lora_sft.yaml`
# secrets:
#   WANDB_API_KEY: 

resources:
  infra: k8s
  cpus: 100+
  memory: 1000+
  accelerators: H100:8
  disk_tier: best
  network_tier: best

num_nodes: 2

# Optional: configure buckets for dataset and checkpoints. You can then use the
# /checkpoints directory to write checkpoints, which writes to local disk first
# and asynchronously uploads to the cloud bucket. Pass /checkpoints to the main
# training script.
#  /dataset:
#    source: s3://my-dataset-bucket
#    mode: COPY  # COPY mode will prefetch the dataset to the node for faster access
#  /checkpoints:
#    source: s3://my-checkpoint-bucket
#    mode: MOUNT_CACHED  # MOUNT_CACHED mode will intelligently cache the checkpoint for faster writes

file_mounts:
  /configs: ./configs

setup: |
  conda create -n training python=3.10 -y
  conda activate training

  # Install CUDA toolkit
  wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
  sudo dpkg -i cuda-keyring_1.1-1_all.deb
  sudo apt-get update
  sudo apt-get install cuda-toolkit-12-6 -y

  # Install LLaMA Factory and dependencies
  git clone https://github.com/hiyouga/LLaMA-Factory.git
  cd LLaMA-Factory
  git checkout 767b344
  pip install -e ".[torch,metrics,deepspeed]" --no-build-isolation
  pip install "transformers>=4.51.1" "huggingface_hub>=0.34.0,<1.0"
  hf download meta-llama/Llama-4-Maverick-17B-128E-Instruct

run: |
  conda activate training
  # Configure W&B if API key is set
  if [ -n "$WANDB_API_KEY" ]; then
    export WANDB_PROJECT=llama4-finetuning
    export WANDB_NAME=llama4-run
    export WANDB_RUN_ID=$SKYPILOT_TASK_ID
    echo "W&B tracking enabled"
    pip install wandb
  fi
  export FORCE_TORCHRUN=1
  export MASTER_ADDR=$(echo "$SKYPILOT_NODE_IPS" | head -n1)
  export MASTER_PORT=29500
  export NNODES=$SKYPILOT_NUM_NODES
  export NODE_RANK=$SKYPILOT_NODE_RANK
  echo "Starting distributed finetuning, head node: $MASTER_ADDR"

  cd LLaMA-Factory
  llamafactory-cli train /configs/llama4_lora_sft.yaml