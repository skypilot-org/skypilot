# LoRA finetuning Meta Llama-3.1 on any of your own infra.
#
# Usage:
#
#  HF_TOKEN=xxx sky launch lora.yaml -c llama31 --secret HF_TOKEN
#
# To finetune a 70B model:  
#
#  HF_TOKEN=xxx sky launch lora.yaml -c llama31-70 --secret HF_TOKEN --env MODEL_SIZE=70B

resources:
  accelerators: A100:8

envs:
  MODEL_SIZE: 8B
  DATASET: yahma/alpaca-cleaned

secrets:
  HF_TOKEN:  # Passed via --secret HF_TOKEN

file_mounts:
  /output:
    source: s3://my-skypilot-bucket # Change to your object storage bucket
    mode: MOUNT

setup: |
  uv venv
  source .venv/bin/activate
  uv pip install torch==2.9.1 torchvision torchao torchtune
  tune download meta-llama/Meta-Llama-3.1-${MODEL_SIZE}-Instruct \
  --hf-token $HF_TOKEN \
  --output-dir /tmp/Meta-Llama-3.1-${MODEL_SIZE}-Instruct \
  --ignore-patterns "original/consolidated*"

run: |
  source .venv/bin/activate
  # Generated a platform-specific config
  tune cp llama3_1/${MODEL_SIZE}_lora /tmp/${MODEL_SIZE}-lora.yaml

  # Run distributed LoRA fine-tuning across all GPUs
  tune run --nproc_per_node $SKYPILOT_NUM_GPUS_PER_NODE \
  lora_finetune_distributed \
  --config /tmp/${MODEL_SIZE}-lora.yaml \
  dataset.source=$DATASET

  # Remove the checkpoint files to save space, LoRA serving only needs the
  # adapter files.
  rm /tmp/Meta-Llama-3.1-${MODEL_SIZE}-Instruct/*.pt
  rm /tmp/Meta-Llama-3.1-${MODEL_SIZE}-Instruct/*.safetensors
  
  mkdir -p /output/$MODEL_SIZE-lora
  rsync -Pavz /tmp/Meta-Llama-3.1-${MODEL_SIZE}-Instruct /output/$MODEL_SIZE-lora
  cp -r /tmp/lora_finetune_output /output/$MODEL_SIZE-lora/
