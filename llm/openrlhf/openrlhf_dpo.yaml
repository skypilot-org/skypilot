# Adapted from https://openrlhf.readthedocs.io/en/latest/non_rl.html#direct-preference-optimization-dpo
# sky launch -c open-rlhf-dpo open-rlhf-dpo.yaml
resources:
  infra: gcp
  accelerators: A100:8
  use_spot: True
  autostop:
    idle_minutes: 2 # Auto-stop after 2 minutes of idleness.

envs:
  WANDB_TOKEN: null # Pass with `--secret WANDB_TOKEN` in CLI

file_mounts:
  /checkpoints:
    name: openrlhf-dpo-checkpoints
    store: gcs

setup: |
  sudo apt update
  docker pull nvcr.io/nvidia/pytorch:25.02-py3
  docker run --name openrlhf_tmp --runtime=nvidia -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:25.02-py3 bash -c "
          pip uninstall xgboost transformer_engine flash_attn pynvml opencv-python-headless -y

          pip install git+https://github.com/OpenRLHF/OpenRLHF.git@1bfcc334692f4d1e0ec0817465d3d9d495fb162f
  "
  docker commit openrlhf_tmp openrlhf:custom
  docker rm openrlhf_tmp

run: |
  docker run --runtime=nvidia -v $PWD:/openrlhf -v /checkpoints:/checkpoints openrlhf:custom bash -c "

    deepspeed --module openrlhf.cli.train_dpo \
      --save_path /checkpoints/llama3-8b-dpo \
      --save_steps -1 \
      --logging_steps 1 \
      --eval_steps -1 \
      --train_batch_size 256 \
      --micro_train_batch_size 1 \
      --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
      --bf16 \
      --max_samples 50000 \
      --max_epochs 1 \
      --max_len 8192 \
      --zero_stage 3 \
      --learning_rate 5e-7 \
      --beta 0.1 \
      --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \
      --apply_chat_template \
      --chosen_key chosen \
      --rejected_key rejected \
      --attn_implementation flash_attention_2 \
      --gradient_checkpointing \
      --use_wandb ${WANDB_TOKEN} \
      --wandb_project openrlhf \
      --wandb_run_name dpo-training
  "