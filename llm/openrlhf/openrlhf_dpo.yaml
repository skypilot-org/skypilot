# Adapted from https://openrlhf.readthedocs.io/en/latest/non_rl.html#direct-preference-optimization-dpo
# To run simply run the following command in a SkyPilot environment (with GCP configured):
# WANDB_TOKEN=xxx sky launch -c open-rlhf-dpo openrlhf_dpo.yaml --secret WANDB_TOKEN

resources:
  infra: gcp
  accelerators: A100:8
  use_spot: True
  autostop:
    idle_minutes: 2 # Auto-stop after 2 minutes of idleness.
  image_id: docker:nvcr.io/nvidia/pytorch:25.02-py3

config:
  docker:
    run_options:
      - -v $PWD:/openrlhf
      - -v /checkpoints:/checkpoints

envs:
  WANDB_TOKEN: null # Pass with `--secret WANDB_TOKEN` in CLI

file_mounts:
  /checkpoints:
    name: openrlhf-dpo-checkpoints
    store: gcs

setup: |
  sudo apt update
  pip uninstall xgboost transformer_engine flash_attn pynvml opencv-python-headless -y
  pip install git+https://github.com/OpenRLHF/OpenRLHF.git@1bfcc334692f4d1e0ec0817465d3d9d495fb162f

run: |
  deepspeed --module openrlhf.cli.train_dpo \
    --save_path /checkpoints/llama3-8b-dpo \
    --save_steps -1 \
    --logging_steps 1 \
    --eval_steps -1 \
    --train_batch_size 256 \
    --micro_train_batch_size 1 \
    --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
    --bf16 \
    --max_samples 50000 \
    --max_epochs 1 \
    --max_len 8192 \
    --zero_stage 3 \
    --learning_rate 5e-7 \
    --beta 0.1 \
    --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \
    --apply_chat_template \
    --chosen_key chosen \
    --rejected_key rejected \
    --attn_implementation flash_attention_2 \
    --gradient_checkpointing \
    --use_wandb ${WANDB_TOKEN} \
    --wandb_project openrlhf \
    --wandb_run_name dpo-training
  "