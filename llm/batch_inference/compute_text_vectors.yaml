name: batch-inference-compute-text-vectors

workdir: .

resources:
  cpus: 4
  accelerators: 
    L4: 1
  infra: aws
  any_of:
    - use_spot: true
    - use_spot: false

envs:
  START_IDX: 0  # Will be overridden by batch launcher script
  END_IDX: 10000  # Will be overridden by batch launcher script
  MODEL_NAME: "Alibaba-NLP/gte-Qwen2-7B-instruct"
  DATASET_NAME: "McAuley-Lab/Amazon-Reviews-2023"
  DATASET_CONFIG: "raw_review_Books"
  EMBEDDINGS_BUCKET_NAME: sky-text-embeddings
  WORKER_ID: ''

file_mounts:
  /output:
    name: ${EMBEDDINGS_BUCKET_NAME}
    mode: MOUNT

setup: |
  # Install dependencies for vLLM
  pip install transformers==4.48.1 vllm==0.6.6.post1
  
  # Install dependencies for embedding computation
  pip install numpy pandas requests tqdm datasets==3.6.0 nltk
  pip install torch torchvision aiohttp
  pip install hf_transfer pyarrow

run: |
  # Initialize and download the model
  HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download --local-dir /tmp/model $MODEL_NAME
  
  # Create metrics directory for monitoring service
  mkdir -p /output/metrics
  
  # Set worker ID for metrics tracking
  if [ -z "$WORKER_ID" ]; then
    export WORKER_ID="worker_$(date +%s)_$(hostname)"
    echo "Generated worker ID: $WORKER_ID"
  fi
  
  # Start vLLM service in background with token counting enabled
  python -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --model /tmp/model \
    --max-model-len 3072 \
    --task embed > /dev/null 2>&1 &

  # Wait for vLLM service to be ready by checking the health endpoint
  echo "Waiting for vLLM service to be ready..."
  while ! curl -s http://localhost:8000/health > /dev/null; do
    sleep 5
    echo "Still waiting for vLLM service..."
  done
  echo "vLLM service is ready!"
  
  # Process the assigned range of documents
  echo "Processing documents from $START_IDX to $END_IDX"
  
  # Process text documents and track token metrics
  python scripts/text_vector_processor.py \
    --output-path "/output/embeddings_${START_IDX}_${END_IDX}.parquet" \
    --start-idx $START_IDX \
    --end-idx $END_IDX \
    --chunk-size 512 \
    --chunk-overlap 50 \
    --vllm-endpoint http://localhost:8000 \
    --batch-size 32 \
    --model-name /tmp/model \
    --dataset-name $DATASET_NAME \
    --dataset-config $DATASET_CONFIG

  # Print tokens statistics summary from metrics
  echo "Embedding generation complete. Token statistics saved to metrics."
  
  # Clean up vLLM service
  pkill -f "python -m vllm.entrypoints.openai.api_server"
  echo "vLLM service has been stopped" 
