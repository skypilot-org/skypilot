"""The 'sky' command line tool.

Example usage:

  # See available commands.
  >> sky

  # Run a task, described in a yaml file.
  # Provisioning, setup, file syncing are handled.
  >> sky run task.yaml
  >> sky run [-c cluster_name] task.yaml

  # Show the list of tasks and running clusters.
  >> sky status

  # Tear down a specific cluster.
  >> sky down -c cluster_name

  # Tear down all existing clusters.
  >> sky down -a

TODO:
- Add support for local Docker backend.  Currently this module is very coupled
  with CloudVmRayBackend, as seen by the many use of ray commands.
"""
import os
import time
import yaml

import click
import pendulum
import prettytable

import sky
from sky import clouds
from sky import global_user_state
from sky.backends import backend_utils
from sky.backends import cloud_vm_ray_backend

_CLUSTER_FLAG_HELP = """
A cluster name. If provided, either reuse an existing cluster with that name or
provision a new cluster with that name. Otherwise provision a new cluster with
an autogenerated name.
""".strip()


def _get_region_zones_from_handle(handle):
    """Gets region and zones from a Ray YAML file."""

    with open(handle, 'r') as f:
        yaml_dict = yaml.safe_load(f)

    provider_config = yaml_dict['provider']
    region = provider_config['region']
    zones = provider_config['availability_zone']

    region = clouds.Region(name=region)
    if zones is not None:
        zones = [clouds.Zone(name=zone) for zone in zones.split(',')]
        region.set_zones(zones)

    return region, zones


def _create_interactive_node(name,
                             resources,
                             cluster_handle=None,
                             backend=cloud_vm_ray_backend.CloudVmRayBackend):
    """Creates an interactive session.

    Args:
        name: Name of the interactivve session.
        resources: Resources to attach to VM.
        cluster_handle: Cluster YAML file.
    """

    with sky.Dag() as dag:
        # TODO: Add conda environment replication
        # should be setup =
        # 'conda env export | grep -v "^prefix: " > environment.yml'
        # && conda env create -f environment.yml
        task = sky.Task(
            name,
            workdir=os.getcwd(),
            setup=None,
            run='',
        )
        task.set_resources(resources)

    backend = backend()
    backend.register_info(dag=dag, optimize_target=sky.OptimizeTarget.COST)

    dag = sky.optimize(dag, minimize=sky.OptimizeTarget.COST)
    task = dag.tasks[0]

    handle = cluster_handle
    if handle is None:
        handle = backend.provision(task,
                                   task.best_resources,
                                   dryrun=False,
                                   stream_logs=True)

    task_id = global_user_state.add_task(task)

    # TODO: cd into workdir immediately on the VM
    # TODO: Delete the temporary cluster config yml (or figure out a way to
    # re-use it)
    backend_utils.run(f'ray attach {handle}')

    if cluster_handle is None:  # if this is a secondary
        backend_utils.run(f'ray down -y {handle}')
        cluster_name = global_user_state.get_cluster_name_from_handle(handle)
        global_user_state.remove_cluster(cluster_name)

    global_user_state.remove_task(task_id)


def _reuse_or_provision_cluster(dag,
                                cluster_name,
                                backend=cloud_vm_ray_backend.CloudVmRayBackend):
    """Reuses or provisions a cluster. Updates existing cluster if required."""

    handle = global_user_state.get_handle_from_cluster_name(cluster_name)

    task = dag.tasks[0]

    # Provision new cluster with cluster_name if it doesn't already exist
    if handle is None:
        backend = backend()
        backend.register_info(dag=dag, optimize_target=sky.OptimizeTarget.COST)

        handle = backend.provision(task,
                                   task.best_resources,
                                   dryrun=False,
                                   stream_logs=True,
                                   cluster_name=cluster_name)
        return handle

    # Ensure changes to workdir, setup, etc. are reflected in the existing
    # cluster
    region, zones = _get_region_zones_from_handle(handle)

    with open(handle, 'r') as f:
        existing_cluster_handle_content = f.read()

    config_dict = backend_utils.write_cluster_config(
        None,
        task,
        # FIXME: we don't want to expose that func, and this module shouldn't
        # need that func either.
        # pylint: disable=protected-access
        cloud_vm_ray_backend._get_cluster_config_template(task),
        region=region,
        zones=zones,
        dryrun=False,
        cluster_name=cluster_name)

    new_handle = str(config_dict['ray'])
    assert new_handle == handle, 'Cluster handle changed'

    with open(new_handle, 'r') as f:
        new_cluster_handle_content = f.read()
        cluster_config_changed = \
            existing_cluster_handle_content != new_cluster_handle_content

    # If cluster configs are identical, then no need to re-run this step.
    if cluster_config_changed:
        backend_utils.run(f'ray up -y {handle} --no-config-cache')

    return new_handle


@click.group()
def cli():
    pass


@cli.command()
@click.argument('entry_point', required=True, type=str)
@click.option('--cluster',
              '-c',
              default=None,
              type=str,
              help=_CLUSTER_FLAG_HELP)
@click.option('--dryrun',
              '-n',
              default=False,
              type=bool,
              help='If True, do not actually run the job.')
def run(entry_point, cluster, dryrun):
    """Launch a job from a YAML config or Python script."""
    with sky.Dag() as dag:
        sky.Task.from_yaml(entry_point)

    dag = sky.optimize(dag, minimize=sky.OptimizeTarget.COST)

    handle = None
    if cluster is not None:
        handle = _reuse_or_provision_cluster(dag, cluster)

    # TODO: This is sketchy. What if we're reusing a cluster and the optimized
    # plan is different?
    sky.execute(dag,
                dryrun=dryrun,
                handle=handle,
                stream_logs=True,
                optimize_target=sky.OptimizeTarget.COST)


@cli.command()
@click.argument('task_id', required=False, type=str)
@click.option('--all',
              '-a',
              default=None,
              is_flag=True,
              help='Cancel all tasks.')
def cancel(task_id, all):  # pylint: disable=redefined-builtin
    """Cancel a task.

    TASK_ID is the id of the task to cancel.  If both TASK_ID and --all are
    supplied, the latter takes precedence.

    Examples:

      \b
      sky cancel task_id
      sky cancel -a
    """
    downall = all
    if task_id is None and downall is None:
        raise click.UsageError(
            'sky cancel requires either a task id (see `sky status`) '
            'or --all.')
    to_down = []
    if task_id is not None:
        to_down = [task_id]
    if downall:
        records = global_user_state.get_tasks()
        to_down = [r['id'] for r in records]
        if task_id is not None:
            print('Both --all and TASK_ID specified for sky cancel. '
                  'Letting --all take effect.')
            task_id = None
    if not to_down:
        if task_id is not None:
            print(f'Task {task_id} is not found (see `sky status`).')
        else:
            print('No existing tasks found (see `sky status`).')
        return
    # TODO: Current implementation is blocking and will wait for the task to
    # complete.  If this is changed to non-blocking, then we will need a way to
    # kill async tasks with ray exec.
    for tid in to_down:
        global_user_state.remove_task(tid)
    click.secho('Done.', fg='green')


@cli.command()
@click.argument('cluster', required=False)
@click.option('--all',
              '-a',
              default=None,
              is_flag=True,
              help='Tear down all existing clusters.')
def down(cluster, all):  # pylint: disable=redefined-builtin
    """Tears down the cluster CLUSTER.

    CLUSTER is the name of the cluster to tear down.  If both CLUSTER and --all
    are supplied, the latter takes precedence.

    Examples:

      \b
      # Tear down a specific cluster.
      sky down cluster_name

      \b
      # Tear down all existing clusters.
      sky down -a
    """
    # FIXME: make TPU part of handles; so that this kills TPUs too.
    name = cluster
    downall = all
    if name is None and downall is None:
        raise click.UsageError(
            'sky down requires either a cluster name (see `sky status`) '
            'or --all.')

    to_down = []
    if name is not None:
        handle = global_user_state.get_handle_from_cluster_name(name)
        if handle is not None:
            to_down = [{'name': name, 'handle': handle}]
    if downall:
        to_down = global_user_state.get_clusters()
        if name is not None:
            print('Both --all and --cluster specified for sky down. '
                  'Letting --all take effect.')
            name = None
    if not to_down:
        if name is not None:
            print(f'Cluster {name} is not found (see `sky status`).')
        else:
            print('No existing clusters found (see `sky status`).')

    # FIXME: Assumes a specific backend.
    backend = cloud_vm_ray_backend.CloudVmRayBackend()
    for record in to_down:
        name = record['name']
        handle = record['handle']
        backend.teardown(handle)
        global_user_state.remove_cluster(name)
        click.secho(f'Tearing down cluster {name}...done.', fg='green')


@cli.command()
def status():
    """Show the status of all tasks and clusters."""

    tasks_status = global_user_state.get_tasks()
    clusters_status = global_user_state.get_clusters()

    task_table = prettytable.PrettyTable()
    task_table.field_names = ['TASK ID', 'TASK NAME', 'LAUNCHED']
    for task_status in tasks_status:
        launched_at = task_status['launched_at']
        duration = pendulum.now().subtract(seconds=time.time() - launched_at)
        task_table.add_row([
            task_status['id'],
            task_status['name'],
            duration.diff_for_humans(),
        ])

    cluster_table = prettytable.PrettyTable()
    cluster_table.field_names = ['CLUSTER NAME', 'LAUNCHED']
    for cluster_status in clusters_status:
        launched_at = cluster_status['launched_at']
        duration = pendulum.now().subtract(seconds=time.time() - launched_at)
        cluster_table.add_row([
            cluster_status['name'],
            duration.diff_for_humans(),
        ])

    click.echo(f'Tasks\n{task_table}')
    click.echo()
    click.echo(f'Clusters\n{cluster_table}')


@cli.command()
@click.option('--cluster',
              '-c',
              default=None,
              type=str,
              help=_CLUSTER_FLAG_HELP)
def gpunode(cluster):
    """Launches an interactive GPU node.

    Automatically syncs current working directory.
    """
    # TODO: Sync code files between local and interactive node (watch rsync?)
    # TODO: Add port forwarding to allow access to localhost:PORT for jupyter
    handle = None
    if cluster is not None:
        handle = global_user_state.get_handle_from_cluster_name(cluster)

    _create_interactive_node('gpunode',
                             {sky.Resources(sky.AWS(), accelerators='V100')},
                             cluster_handle=handle)


@cli.command()
@click.option('--cluster',
              '-c',
              default=None,
              type=str,
              help=_CLUSTER_FLAG_HELP)
def cpunode(cluster):
    """Launches an interactive CPU node.

    Automatically syncs current working directory.
    """
    handle = None
    if cluster is not None:
        handle = global_user_state.get_handle_from_cluster_name(cluster)

    _create_interactive_node('cpunode', {sky.Resources(sky.AWS())},
                             cluster_handle=handle)


def main():
    return cli()


if __name__ == '__main__':
    main()
