"""The 'sky' command line tool.

Example usage:

  # See available commands.
  >> sky

  # Run a task, described in a yaml file.
  # Provisioning, setup, file syncing are handled.
  >> sky run task.yaml
  >> sky run [-c cluster_name] task.yaml

  # Show the list of tasks and running clusters.
  >> sky status

  # Tear down a specific cluster.
  >> sky down -c cluster_name

  # Tear down all existing clusters.
  >> sky down -a

TODO:
- Add support for local Docker backend.  Currently this module is very coupled
  with CloudVmRayBackend, as seen by the many use of ray commands.
"""
import os
import time
import yaml

import click
import pendulum
import prettytable

import sky
from sky import clouds
from sky import global_user_state
from sky.backends import backend_utils
from sky.backends import cloud_vm_ray_backend

_CLUSTER_FLAG_HELP = """
A cluster name. If provided, either reuse an existing cluster with that name or
provision a new cluster with that name. Otherwise provision a new cluster with
an autogenerated name.
""".strip()


def _get_region_zones_from_handle(handle):
    """Gets region and zones from a Ray YAML file."""

    with open(handle, 'r') as f:
        yaml_dict = yaml.safe_load(f)

    provider_config = yaml_dict['provider']
    region = provider_config['region']
    zones = provider_config['availability_zone']

    region = clouds.Region(name=region)
    if zones is not None:
        zones = [clouds.Zone(name=zone) for zone in zones.split(',')]
        region.set_zones(zones)

    return region, zones


def _create_interactive_node(name,
                             resources,
                             cluster_handle=None,
                             backend=cloud_vm_ray_backend.CloudVmRayBackend):
    """Creates an interactive session.

    Args:
        name: Name of the interactivve session.
        resources: Resources to attach to VM.
        cluster_handle: Cluster YAML file.
    """

    with sky.Dag() as dag:
        # TODO: Add conda environment replication
        # should be setup =
        # 'conda env export | grep -v "^prefix: " > environment.yml'
        # && conda env create -f environment.yml
        task = sky.Task(
            name,
            workdir=os.getcwd(),
            setup=None,
            run='',
        )
        task.set_resources(resources)

    backend = backend()
    backend.register_info(dag=dag, optimize_target=sky.OptimizeTarget.COST)

    dag = sky.optimize(dag, minimize=sky.OptimizeTarget.COST)
    task = dag.tasks[0]

    handle = cluster_handle
    if handle is None:
        handle = backend.provision(task,
                                   task.best_resources,
                                   dryrun=False,
                                   stream_logs=True)

    task_id = global_user_state.add_task(task)

    # TODO: cd into workdir immediately on the VM
    # TODO: Delete the temporary cluster config yml (or figure out a way to
    # re-use it)
    backend_utils.run(f'ray attach {handle}')

    if cluster_handle is None:  # if this is a secondary
        backend_utils.run(f'ray down -y {handle}')
        cluster_name = global_user_state.get_cluster_name_from_handle(handle)
        global_user_state.remove_cluster(cluster_name)

    global_user_state.remove_task(task_id)


@click.group()
def cli():
    pass


@cli.command()
@click.argument('entry_point', required=True, type=str)
@click.option('--cluster',
              '-c',
              default=None,
              type=str,
              help=_CLUSTER_FLAG_HELP)
@click.option('--dryrun',
              '-n',
              default=False,
              type=bool,
              help='If True, do not actually run the job.')
def run(entry_point, cluster, dryrun):
    """Launch a task from a YAML config."""
    with sky.Dag() as dag:
        sky.Task.from_yaml(entry_point)
    # FIXME: --cluster flag semantics has the following bug.  'sky run -c name
    # x.yml' requiring GCP.  Then change x.yml to requiring AWS.  'sky run -c
    # name x.yml' again.  The GCP cluster is not down'd but should be.  The
    # root cause is due to 'ray up' not dealing with this cross-cloud case (but
    # does correctly deal with in-cloud config changes).
    #
    # This bug also means that the old GCP cluster with the same name is
    # orphaned.  `sky down` would not have an entry pointing to that handle, so
    # would only down the NEW cluster.
    #
    # To fix all of the above, fix/circumvent the bug that 'ray up' not downing
    # old cloud's cluster with the same name.
    sky.execute(dag,
                dryrun=dryrun,
                stream_logs=True,
                optimize_target=sky.OptimizeTarget.COST,
                cluster_name=cluster)


@cli.command()
@click.argument('entry_point', required=True, type=str)
@click.option('--cluster',
              '-c',
              required=True,
              type=str,
              help='Name of the existing cluster to execute a task on.')
def exec(entry_point, cluster):  # pylint: disable=redefined-builtin
    """Execute a task from a YAML config on an existing cluster.

    \b
    Actions performed by this command only include:
      - workdir syncing
      - executing the task's run command
    `sky exec` is thus typically faster than `sky run`, provided a cluster
    already exists.

    All setup steps (provisioning, setup commands, file mounts syncing) are
    skipped.  If any of those specifications changed, this command will not
    reflect those changes.  To ensure a cluster's setup is up to date, use `sky
    run` instead.

    Typical workflow:

      # First command: set up the cluster once.

      >> sky run -c name app.yaml

    \b
      # Starting iterative development...
      # For example, modify local workdir code.
      # Future commands: simply execute the task on the launched cluster.

      >> sky exec -c name app.yaml

      # Simply do "sky run" again if anything other than Task.run is modified:

      >> sky run -c name app.yaml

    """
    handle = global_user_state.get_handle_from_cluster_name(cluster)
    if handle is None:
        raise click.BadParameter(f'Cluster \'{cluster}\' not found.  '
                                 'Use `sky run` to provision first.')
    with sky.Dag() as dag:
        sky.Task.from_yaml(entry_point)
    sky.execute(dag,
                handle=handle,
                stages=[
                    sky.execution.Stage.SYNC_WORKDIR,
                    sky.execution.Stage.EXEC,
                ])


@cli.command()
@click.argument('task_id', required=False, type=str)
@click.option('--all',
              '-a',
              default=None,
              is_flag=True,
              help='Cancel all tasks.')
def cancel(task_id, all):  # pylint: disable=redefined-builtin
    """Cancel task(s).

    TASK_ID is the id of the task to cancel.  If both TASK_ID and --all are
    supplied, the latter takes precedence.

    Examples:

      \b
      sky cancel task_id
      sky cancel -a
    """
    downall = all
    if task_id is None and downall is None:
        raise click.UsageError(
            'sky cancel requires either a task id (see `sky status`) '
            'or --all.')
    to_down = []
    if task_id is not None:
        to_down = [task_id]
    if downall:
        records = global_user_state.get_tasks()
        to_down = [r['id'] for r in records]
        if task_id is not None:
            print('Both --all and TASK_ID specified for sky cancel. '
                  'Letting --all take effect.')
            task_id = None
    if not to_down:
        if task_id is not None:
            print(f'Task {task_id} is not found (see `sky status`).')
        else:
            print('No existing tasks found (see `sky status`).')
        return
    # TODO: Current implementation is blocking and will wait for the task to
    # complete.  If this is changed to non-blocking, then we will need a way to
    # kill async tasks with ray exec.
    for tid in to_down:
        global_user_state.remove_task(tid)
    click.secho('Done.', fg='green')


@cli.command()
@click.argument('cluster', required=False)
@click.option('--all',
              '-a',
              default=None,
              is_flag=True,
              help='Tear down all existing clusters.')
def down(cluster, all):  # pylint: disable=redefined-builtin
    """Tear down cluster(s).

    CLUSTER is the name of the cluster to tear down.  If both CLUSTER and --all
    are supplied, the latter takes precedence.

    Examples:

      \b
      # Tear down a specific cluster.
      sky down cluster_name

      \b
      # Tear down all existing clusters.
      sky down -a
    """
    # FIXME: make TPU part of handles; so that this kills TPUs too.
    name = cluster
    downall = all
    if name is None and downall is None:
        raise click.UsageError(
            'sky down requires either a cluster name (see `sky status`) '
            'or --all.')

    to_down = []
    if name is not None:
        handle = global_user_state.get_handle_from_cluster_name(name)
        if handle is not None:
            to_down = [{'name': name, 'handle': handle}]
    if downall:
        to_down = global_user_state.get_clusters()
        if name is not None:
            print('Both --all and --cluster specified for sky down. '
                  'Letting --all take effect.')
            name = None
    if not to_down:
        if name is not None:
            print(f'Cluster {name} is not found (see `sky status`).')
        else:
            print('No existing clusters found (see `sky status`).')

    # FIXME: Assumes a specific backend.
    backend = cloud_vm_ray_backend.CloudVmRayBackend()
    for record in to_down:
        name = record['name']
        handle = record['handle']
        backend.teardown(handle)
        global_user_state.remove_cluster(name)
        click.secho(f'Tearing down cluster {name}...done.', fg='green')


@cli.command()
def status():
    """Show tasks and clusters."""

    tasks_status = global_user_state.get_tasks()
    clusters_status = global_user_state.get_clusters()

    task_table = prettytable.PrettyTable()
    task_table.field_names = ['TASK ID', 'TASK NAME', 'LAUNCHED']
    for task_status in tasks_status:
        launched_at = task_status['launched_at']
        duration = pendulum.now().subtract(seconds=time.time() - launched_at)
        task_table.add_row([
            task_status['id'],
            task_status['name'],
            duration.diff_for_humans(),
        ])

    cluster_table = prettytable.PrettyTable()
    cluster_table.field_names = ['CLUSTER NAME', 'LAUNCHED']
    for cluster_status in clusters_status:
        launched_at = cluster_status['launched_at']
        duration = pendulum.now().subtract(seconds=time.time() - launched_at)
        cluster_table.add_row([
            cluster_status['name'],
            duration.diff_for_humans(),
        ])

    click.echo(f'Tasks\n{task_table}')
    click.echo()
    click.echo(f'Clusters\n{cluster_table}')


@cli.command()
@click.option('--cluster',
              '-c',
              default=None,
              type=str,
              help=_CLUSTER_FLAG_HELP)
def gpunode(cluster):
    """Launch an interactive GPU node.

    Automatically syncs current working directory.
    """
    # TODO: Sync code files between local and interactive node (watch rsync?)
    # TODO: Add port forwarding to allow access to localhost:PORT for jupyter
    handle = None
    if cluster is not None:
        handle = global_user_state.get_handle_from_cluster_name(cluster)

    _create_interactive_node('gpunode',
                             {sky.Resources(sky.AWS(), accelerators='V100')},
                             cluster_handle=handle)


@cli.command()
@click.option('--cluster',
              '-c',
              default=None,
              type=str,
              help=_CLUSTER_FLAG_HELP)
def cpunode(cluster):
    """Launch an interactive CPU node.

    Automatically syncs current working directory.
    """
    handle = None
    if cluster is not None:
        handle = global_user_state.get_handle_from_cluster_name(cluster)

    _create_interactive_node('cpunode', {sky.Resources(sky.AWS())},
                             cluster_handle=handle)


def main():
    return cli()


if __name__ == '__main__':
    main()
