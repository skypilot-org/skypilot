# Ref: https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/gcp/example-gpu-docker.yaml
# FIXME: use variables for project_id, zone.

cluster_name: {{cluster_name}}

# The maximum number of workers nodes to launch in addition to the head node.
max_workers: {{num_nodes - 1}}
idle_timeout_minutes: 60

provider:
  type: gcp
  region: {{region}}
  availability_zone: {{zones}}
  project_id: intercloud-320520  # Globally unique project id

auth:
  ssh_user: ubuntu

available_node_types:
  ray_head_default:
    resources: {}
    node_config:
      machineType: {{instance_type}}
      disks:
        - boot: true
          autoDelete: true
          type: PERSISTENT
          initializeParams:
            diskSizeGb: 256
            # See https://cloud.google.com/deep-learning-vm/docs/images
{%- if gpu is none %}
            sourceImage: projects/deeplearning-platform-release/global/images/family/common-cpu
{%- else %}
            sourceImage: projects/deeplearning-platform-release/global/images/family/common-cu110
      guestAccelerators:
        - acceleratorType: projects/intercloud-320520/zones/{{zones}}/acceleratorTypes/{{gpu}}
          acceleratorCount: {{gpu_count}}
      metadata:
        items:
          - key: install-nvidia-driver
            value: "True"
      scheduling:
        - onHostMaintenance: TERMINATE  # Required for GPU-attached VMs.
{%- endif %}
{% if num_nodes > 1 %}
  ray_worker_default:
    min_workers: {{num_nodes - 1}}
    max_workers: {{num_nodes - 1}}
    resources: {}
    node_config:
      machineType: {{instance_type}}
      disks:
        - boot: true
          autoDelete: true
          type: PERSISTENT
          initializeParams:
            diskSizeGb: 256
            # See https://cloud.google.com/deep-learning-vm/docs/images
{%- if gpu is none %}
            sourceImage: projects/deeplearning-platform-release/global/images/family/common-cpu
{%- else %}
            sourceImage: projects/deeplearning-platform-release/global/images/family/common-cu110
      guestAccelerators:
        - acceleratorType: projects/intercloud-320520/zones/{{zones}}/acceleratorTypes/{{gpu}}
          acceleratorCount: {{gpu_count}}
      metadata:
        items:
          - key: install-nvidia-driver
            value: "True"
      scheduling:
        - onHostMaintenance: TERMINATE  # Required for GPU-attached VMs.
{%- endif %}
{%- endif %}

head_node_type: ray_head_default

# Format: `REMOTE_PATH : LOCAL_PATH`
file_mounts: {
{%- if workdir is not none %}
  "/tmp/workdir": "{{workdir}}",
{%- endif %}
{%- if setup_sh_path is not none %}
  "/tmp/setup.sh": "{{setup_sh_path}}",
{%- endif %}
{%- for remote_path, local_path in file_mounts.items() %}
  "{{remote_path}}": "{{local_path}}",
{%- endfor %}
}

# Patterns for files to exclude when running rsync up or rsync down
# rsync_exclude:
#   - "**/.git"
#   - "**/.git/**"

rsync_filter:
  - .gitignore

# List of shell commands to run to set up nodes.
setup_commands:
  # This AMI's system Python is version 2+.
  - pip3 install -U ray==1.7.0
{%- if setup_sh_path is not none %}
  - mkdir -p /tmp/workdir && cd /tmp/workdir && bash /tmp/setup.sh
{%- else %}
  - mkdir -p /tmp/workdir && cd /tmp/workdir
{%- endif %}

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
  - ray stop
  - ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml {{"--resources='%s'" % custom_resources if custom_resources}}

{%- if num_nodes > 1 %}
worker_start_ray_commands:
  - ray stop
  - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 {{"--resources='%s'" % custom_resources if custom_resources}}
{%- endif %}

head_node: {}
worker_nodes: {}
