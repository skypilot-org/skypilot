cluster_name: {{run_id}}

max_workers: 1

docker:
    image: "rayproject/ray-ml:latest-gpu"
    container_name: "resnet_docker"

# Cloud-provider specific configuration.
provider:
    type: azure
    location: southcentralus
    resource_group: ray-cluster
    # set subscription id otherwise the default from az cli will be used
    subscription_id: c721f523-3577-40bc-846a-e8bf4d139ed6 #aa86df77-e703-453e-b2f4-955c3b33e534

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu
    # you must specify paths to matching private and public key pair files
    # use `ssh-keygen -t rsa -b 4096` to generate a new ssh key pair
    ssh_private_key: ~/.ssh/id_rsa
    # changes to this should match what is specified in file_mounts
    ssh_public_key: ~/.ssh/id_rsa.pub

# Tell the autoscaler the allowed node types and the resources they provide.
# The key is the name of the node type, which is just for debugging purposes.
# The node config specifies the launch config and physical instance type.
available_node_types:
    ray.head.gpu:
        # The resources provided by this node type.
        resources: {"CPU": 6, "GPU": 1}
        # Provider-specific config, e.g. instance type.
        node_config:
            # To find right image:
            #  az vm image list --all --publisher microsoft-dsvm --offer ubuntu-1804
            azure_arm_parameters:
                vmSize: {{instance_type}}
                # List images https://docs.microsoft.com/en-us/azure/virtual-machines/linux/cli-ps-findimage
                imagePublisher: microsoft-dsvm
                imageOffer: ubuntu-1804
                imageSku: 1804-gen2
                imageVersion: 21.09.13

    ray.worker.gpu:
        # The minimum number of nodes of this type to launch.
        # This number should be >= 0.
        min_workers: 0
        # The maximum number of workers nodes of this type to launch.
        # This takes precedence over min_workers.
        max_workers: 2
        # The resources provided by this node type.
        resources: {"CPU": 6, "GPU": 1}
        # Provider-specific config, e.g. instance type.
        node_config:
            azure_arm_parameters:
                vmSize: {{instance_type}}
                # List images https://docs.microsoft.com/en-us/azure/virtual-machines/linux/cli-ps-findimage
                imagePublisher: microsoft-dsvm
                imageOffer: ubuntu-1804
                imageSku: 1804-gen2
                imageVersion: 21.09.13
                # optionally set priority to use Spot instances
                priority: Spot
                # set a maximum price for spot instances if desired
                # billingProfile:
                #     maxPrice: -1

# Specify the node type of the head node (as configured above).
head_node_type: ray.head.gpu

# Files or directories to copy to the head and worker nodes. The format is a
# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
file_mounts: {
#    "/path1/on/remote/machine": "/path1/on/local/machine",
#    "/path2/on/remote/machine": "/path2/on/local/machine",
     "~/.ssh/id_rsa.pub": "~/.ssh/id_rsa.pub"
}

# List of commands that will be run before `setup_commands`. If docker is
# enabled, these commands will run outside the container and before docker
# is setup.
initialization_commands:
    # enable docker setup
    - sudo usermod -aG docker $USER || true
    - sleep 10  # delay to avoid docker permission denied errors
    # get rid of annoying Ubuntu message
    - touch ~/.sudo_as_admin_successful

# List of shell commands to run to set up nodes.
# NOTE: rayproject/ray-ml:latest has ray latest bundled
setup_commands: []
#     - pip install -U "ray[default] @ https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-2.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl"

# Custom commands that will be run on the head node after common setup.
# NOTE: rayproject/ray-ml:latest has azure packages bundled
head_setup_commands:
    - conda create -n resnet python=3.7 -y; sudo apt-get update; sudo apt-get --assume-yes install vim; sudo apt-get --assume-yes install htop; echo 'export PYTHONPATH=$PYTHONPATH:~/tpu/models/' >> ~/.bashrc 
    - git clone https://github.com/concretevitamin/tpu.git; export PYTHONPATH="$PYTHONPATH:~/tpu/models/"; cd tpu/models; git checkout -f gpu_train; conda activate resnet; pip install -U azure-cli-core==2.22.0 azure-mgmt-compute==14.0.0 azure-mgmt-msi==1.0.0 azure-mgmt-network==10.2.0 azure-mgmt-resource==13.0.0; python -m pip install awscli; python -m pip install -r official/resnet/requirements.txt; aws configure set default.region us-west-2
    # Move to ray exec
    #- conda activate resnet; cd tpu/models; python official/resnet/resnet_main.py --use_tpu=False --mode=train --model_dir=~/resnet_model --data_dir=gs://cloud-tpu-test-datasets/fake_imagenet --train_batch_size=256 --train_steps=112590 --amp --xla --loss_scale=128 --iterations_per_loop=1251 2>&1 | tee ~/run.log

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
    - ray stop
    - ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    - ray stop
    - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
