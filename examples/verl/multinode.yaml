# Multi-node distributed training with Verl (Volcano Engine Reinforcement Learning) framework.
#
# Verl is a flexible and efficient reinforcement learning framework designed for
# training large language models with RLHF (Reinforcement Learning from Human Feedback).
# This example demonstrates multi-node training using PPO on the GSM8K dataset.
#
# Prerequisites:
#   - GPU nodes with at least 40GB memory (e.g., A100)
#   - Access to Hugging Face models (Qwen/Qwen2.5-0.5B-Instruct in this example)
#
# Usage:
#   # Launch a 2-node training cluster:
#   $ sky launch -c verl-cluster examples/verl/multinode.yaml
#
#   # Monitor the Ray dashboard (optional):
#   $ sky status --endpoint 8280 verl-cluster
#
#   # Stream logs:
#   $ sky logs verl-cluster
#
#   # Cleanup:
#   $ sky down verl-cluster

name: verl-multinode-training

resources:
  accelerators: A100:1  # Can use A100:1, A100-80GB:1, or other high-memory GPUs
  # cloud: lambda  # Optional: specify cloud provider
  use_spot: false  # Set to true for cost savings (may be preempted)
  ports:
    - 8280  # Ray dashboard port

num_nodes: 2  # Number of nodes for distributed training

setup: |
  # Install uv for fast Python package management
  curl -LsSf https://astral.sh/uv/install.sh | sh
  export PATH="$HOME/.local/bin:$PATH"
  
  # Clone and setup Verl
  rm -rf verl
  git clone https://github.com/volcengine/verl.git
  cd verl
  
  # Create virtual environment and install dependencies
  uv venv --seed
  source .venv/bin/activate
  
  # Install Verl and its dependencies (skip Megatron for this example)
  USE_MEGATRON=0 bash scripts/install_vllm_sglang_mcore.sh
  uv pip install --no-deps -e .
  uv pip install "ray[default]"  # For Ray dashboard

run: |
  # Set up distributed training environment
  head_ip=`echo "$SKYPILOT_NODE_IPS" | head -n1`
  num_nodes=`echo "$SKYPILOT_NODE_IPS" | wc -l`
  
  cd verl
  source .venv/bin/activate
  
  # Create custom runtime environment configuration
  cat > runtime_env_custom.yaml <<EOF
  working_dir: ./
  excludes: ["/.git/", "*.whl", "**/*.whl"]
  env_vars:
    TORCH_NCCL_AVOID_RECORD_STREAMS: "1"
    CUDA_DEVICE_MAX_CONNECTIONS: "1"
  EOF
  
  # Ray cluster configuration
  HEAD_PORT=6385
  DASH_PORT=8280
  
  # Function to check if Ray is already running
  is_ray_alive () {
    ray status --address="$1:$HEAD_PORT" >/dev/null 2>&1
  }
  
  if [ "$SKYPILOT_NODE_RANK" == "0" ]; then
    # Head node: prepare data, download model, start Ray head, and submit training job
    echo "Setting up head node..."
    
    # Install additional dependencies for data processing
    uv pip install datasets transformers
    
    # Prepare GSM8K dataset
    python3 examples/data_preprocess/gsm8k.py --local_dir ~/data/gsm8k
    
    # Download model to cache
    python3 -c "import transformers; transformers.pipeline('text-generation', model='Qwen/Qwen2.5-0.5B-Instruct')"
    
    # Start Ray head node if not already running
    if ! is_ray_alive "$head_ip"; then
      echo "Starting Ray head node..."
      ray start --head --node-ip-address="$head_ip" \
                --port $HEAD_PORT --dashboard-port $DASH_PORT \
                --dashboard-host=0.0.0.0 \
                --dashboard-agent-listen-port=52366 \
                --disable-usage-stats
      sleep 10
    else
      echo "Ray is already running at $head_ip:$HEAD_PORT, reusing existing instance"
    fi
    
    # Submit the training job to Ray
    export RAY_ADDRESS="http://localhost:$DASH_PORT"
    echo "Submitting training job to Ray cluster..."
    
    ray job submit --address="$RAY_ADDRESS" --working-dir=. \
      --runtime-env=runtime_env_custom.yaml \
      -- python3 -m verl.trainer.main_ppo \
      data.train_files=$HOME/data/gsm8k/train.parquet \
      data.val_files=$HOME/data/gsm8k/test.parquet \
      data.train_batch_size=256 \
      data.max_prompt_length=512 \
      data.max_response_length=256 \
      actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \
      actor_rollout_ref.actor.optim.lr=1e-6 \
      actor_rollout_ref.actor.ppo_mini_batch_size=64 \
      actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \
      actor_rollout_ref.rollout.name=vllm \
      actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \
      actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
      actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
      actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \
      critic.optim.lr=1e-5 \
      critic.model.path=Qwen/Qwen2.5-0.5B-Instruct \
      critic.ppo_micro_batch_size_per_gpu=4 \
      critic.ppo_mini_batch_size=64 \
      algorithm.kl_ctrl.kl_coef=0.001 \
      trainer.project_name=ppo_training \
      trainer.experiment_name=qwen-2.5-0.5B \
      trainer.val_before_train=False \
      trainer.n_gpus_per_node=$SKYPILOT_NUM_GPUS_PER_NODE \
      trainer.nnodes=$num_nodes \
      trainer.default_local_dir=/checkpoints \
      trainer.save_freq=10 \
      trainer.test_freq=10 \
      trainer.total_epochs=15 \
      trainer.logger=['console'] \
      trainer.resume_mode=disable 2>&1 | tee verl_training.log
  else
    # Worker nodes: connect to Ray head
    echo "Setting up worker node..."
    sleep 10  # Wait for head node to start
    
    # Check if this worker is already connected to Ray
    echo "Connecting worker to Ray head at $head_ip:$HEAD_PORT"
    ray start --address $head_ip:$HEAD_PORT --disable-usage-stats
  fi