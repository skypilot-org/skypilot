# sky launch e2e_disk.yaml -c e2e_disk --env HF_TOKEN="YOUR TOKEN"

envs:
  S3_BUCKET: nebius://henry-test  # S3 bucket for checkpoints
  S3_BUCKET2: nebius://henry-test-et  # S3 bucket for checkpoints
  HF_TOKEN:  # Add your HuggingFace token if needed
  MODEL_ID: openai/gpt-oss-120b  # Default model ID

resources:
  cpus: 32+
  memory: 256+
  disk_tier: best
  disk_size: 2000
  accelerators: H100:8
  network_tier: best

num_nodes: 1

# Configure buckets for dataset and checkpoints with both S3 and local NVMe
file_mounts:
  /checkpoints_s3:
    source: ${S3_BUCKET}
    mode: MOUNT
  /checkpoints_s3_mount_cached:
    source: ${S3_BUCKET2}
    mode: MOUNT_CACHED
  /e2e: ./e2e

volumes:
  # Mount the Nebius shared filesystem to /mnt/data across all nodes
  /mnt/data: nebius-pvc

setup: |
  conda install cuda -c nvidia
  uv venv ~/training --seed --python 3.10
  source ~/training/bin/activate
  sudo DEBIAN_FRONTEND=noninteractive apt install -y vmtouch
  sudo DEBIAN_FRONTEND=noninteractive apt install -y python3-dev python3.10-dev build-essential

  uv pip install torch --index-url https://download.pytorch.org/whl/cu128
  uv pip install "trl>=0.20.0" "peft>=0.17.0" "transformers>=4.55.0"
  uv pip install deepspeed
  uv pip install git+https://github.com/huggingface/accelerate.git@c0a3aefea8aa5008a0fbf55b049bd3f0efa9cbf2
  # Add requirements for MXFP4 quantization
  uv pip install "triton>=3.4.0" kernels
  uv pip install git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels

  # Other tools
  sudo DEBIAN_FRONTEND=noninteractive apt install -y htop sysstat iproute2 net-tools infiniband-diags
  uv pip install nvitop

  mkdir /tmp/checkpoint

run: |
  source ~/training/bin/activate

  MASTER_ADDR=$(echo "$SKYPILOT_NODE_IPS" | head -n1)
  NP=$(($SKYPILOT_NUM_GPUS_PER_NODE * $SKYPILOT_NUM_NODES))

  python /e2e/download.py \
    --model_id ${MODEL_ID} \
    --dataset_name "HuggingFaceH4/Multilingual-Thinking" \
    --num_proc -1 \
    --dirs /tmp/checkpoint /mnt/data /checkpoints_s3 /checkpoints_s3_mount_cached

  accelerate launch --config_file /e2e/fsdp2.yaml \
    --num_machines $SKYPILOT_NUM_NODES \
    --num_processes $NP \
    --machine_rank $SKYPILOT_NODE_RANK \
    --main_process_ip $MASTER_ADDR \
    --main_process_port 29500 \
    /e2e/e2e_disk.py \
    --num_proc 32 \
    --model_id ${MODEL_ID} \
    --per_device_train_batch_size 4 \
    --skip_training \
    --dirs /tmp/checkpoint

  accelerate launch --config_file /e2e/fsdp2.yaml \
    --num_machines $SKYPILOT_NUM_NODES \
    --num_processes $NP \
    --machine_rank $SKYPILOT_NODE_RANK \
    --main_process_ip $MASTER_ADDR \
    --main_process_port 29500 \
    /e2e/e2e_disk.py \
    --num_proc 32 \
    --model_id ${MODEL_ID} \
    --per_device_train_batch_size 4 \
    --skip_training \
    --dirs /mnt/data/

  accelerate launch --config_file /e2e/fsdp2.yaml \
    --num_machines $SKYPILOT_NUM_NODES \
    --num_processes $NP \
    --machine_rank $SKYPILOT_NODE_RANK \
    --main_process_ip $MASTER_ADDR \
    --main_process_port 29500 \
    /e2e/e2e_disk.py \
    --num_proc 32 \
    --model_id ${MODEL_ID} \
    --per_device_train_batch_size 4 \
    --skip_training \
    --dirs /checkpoints_s3

  accelerate launch --config_file /e2e/fsdp2.yaml \
    --num_machines $SKYPILOT_NUM_NODES \
    --num_processes $NP \
    --machine_rank $SKYPILOT_NODE_RANK \
    --main_process_ip $MASTER_ADDR \
    --main_process_port 29500 \
    /e2e/e2e_disk.py \
    --num_proc 32 \
    --model_id ${MODEL_ID} \
    --per_device_train_batch_size 4 \
    --skip_training \
    --dirs /checkpoints_s3_mount_cached
