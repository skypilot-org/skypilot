name: nccl-gpu-direct-tcpx

resources:
  cloud: gcp
  instance_type: a3-highgpu-8g
  image_id: docker:us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx/nccl-plugin-gpudirecttcpx

num_nodes: 2

envs:
  USE_GPU_DIRECT: "true"

setup: |
  # Check if the /usr/local/tcpx/lib64/libnccl.so.2 is
  # present to ensure the user-data script has completed
  while [ ! -f /usr/local/tcpx/lib64/libnccl.so.2 ]; do
    echo "Waiting for user-data script to complete"
    sleep 10
  done
  # Remount the directories with exec permissions
  sudo mount -o remount,exec /usr/local/tcpx/lib64
  sudo mount -o remount,exec /usr/local/nvidia/lib64
  sudo mount -o remount,exec /usr/local/nvidia/bin


run: |
  if [ "${SKYPILOT_NODE_RANK}" == "0" ]; then
    echo "Head node"

    # Total number of processes, NP should be the total number of GPUs in the cluster
    NP=$(($SKYPILOT_NUM_GPUS_PER_NODE * $SKYPILOT_NUM_NODES))

    # Append :${SKYPILOT_NUM_GPUS_PER_NODE} to each IP as slots
    nodes=""
    for ip in $SKYPILOT_NODE_IPS; do
      nodes="${nodes}${ip}:${SKYPILOT_NUM_GPUS_PER_NODE},"
    done
    nodes=${nodes::-1}
    echo "All nodes: ${nodes}"

    # Set environment variables
    export PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:$PATH
    export NCCL_SOCKET_IFNAME=eth0
    export NCCL_CROSS_NIC=0
    export NCCL_ALGO=Ring
    export NCCL_PROTO=Simple
    export NCCL_NSOCKS_PERTHREAD=4
    export NCCL_SOCKET_NTHREADS=1
    export NCCL_NET_GDR_LEVEL=PIX
    export NCCL_DYNAMIC_CHUNK_SIZE=524288
    export NCCL_P2P_PXN_LEVEL=0    
    export NCCL_P2P_NET_CHUNKSIZE=524288
    export NCCL_P2P_PCI_CHUNKSIZE=524288
    export NCCL_P2P_NVL_CHUNKSIZE=1048576
    export NCCL_BUFFSIZE=8388608
    export NCCL_MAX_NCHANNELS=8
    export NCCL_MIN_NCHANNELS=8
    export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    export NCCL_GPUDIRECTTCPX_SOCKET_IFNAME=eth1,eth2,eth3,eth4
    export NCCL_GPUDIRECTTCPX_CTRL_DEV=eth0
    export NCCL_GPUDIRECTTCPX_TX_BINDINGS="eth1:8-21,112-125;eth2:8-21,112-125;eth3:60-73,164-177;eth4:60-73,164-177"
    export NCCL_GPUDIRECTTCPX_RX_BINDINGS="eth1:22-35,126-139;eth2:22-35,126-139;eth3:74-87,178-191;eth4:74-87,178-191"
    export NCCL_GPUDIRECTTCPX_PROGRAM_FLOW_STEERING_WAIT_MICROS=50000
    export NCCL_GPUDIRECTTCPX_UNIX_CLIENT_PREFIX="/run/tcpx"
    export NCCL_GPUDIRECTTCPX_FORCE_ACK=0

    if [ "${USE_GPU_DIRECT}" == "true" ]; then
      export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:/usr/local/tcpx/lib64
    else
      # Use the default NCCL library
      export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib64
    fi
    
    mpirun \
      --allow-run-as-root \
      --tag-output \
      -H $nodes \
      -np $NP \
      -N $SKYPILOT_NUM_GPUS_PER_NODE \
      -x PATH \
      -x LD_LIBRARY_PATH \
      -x NCCL_SOCKET_IFNAME \
      -x NCCL_CROSS_NIC \
      -x NCCL_ALGO \
      -x NCCL_PROTO \
      -x NCCL_NSOCKS_PERTHREAD \
      -x NCCL_SOCKET_NTHREADS \
      -x NCCL_MAX_NCHANNELS \
      -x NCCL_MIN_NCHANNELS \
      -x NCCL_DYNAMIC_CHUNK_SIZE \
      -x NCCL_P2P_NET_CHUNKSIZE \
      -x NCCL_P2P_PCI_CHUNKSIZE \
      -x NCCL_P2P_NVL_CHUNKSIZE \
      -x NCCL_BUFFSIZE \
      -x CUDA_VISIBLE_DEVICES \
      -x NCCL_GPUDIRECTTCPX_SOCKET_IFNAME \
      -x NCCL_GPUDIRECTTCPX_CTRL_DEV \
      -x NCCL_GPUDIRECTTCPX_TX_BINDINGS \
      -x NCCL_GPUDIRECTTCPX_RX_BINDINGS \
      -x NCCL_GPUDIRECTTCPX_PROGRAM_FLOW_STEERING_WAIT_MICROS \
      -x NCCL_GPUDIRECTTCPX_UNIX_CLIENT_PREFIX \
      -x NCCL_GPUDIRECTTCPX_FORCE_ACK \
      -x NCCL_NET_GDR_LEVEL \
      -x NCCL_P2P_PXN_LEVEL \
      -x NCCL_DEBUG=INFO -x NCCL_DEBUG_SUBSYS=ENV \
      --mca btl tcp,self \
      --mca btl_tcp_if_include eth0 \
      --mca plm_rsh_args "-p 10022" \
      /third_party/nccl-tests-mpi/build/all_reduce_perf \
      -b 8 \
      -e 2G \
      -f 2 \
      -g 1 \
      -c 1 \
      -w 5 \
      -n 20
  else
    echo "Worker nodes"
  fi

config:
  gcp:
    enable_gpu_direct: true
    managed_instance_group:
      run_duration: 36000
      provision_timeout: 900
