[33mTailing logs of job 1 on cluster 'cc'...[0m
[2mâ”œâ”€â”€ [0m[2mWaiting for task resources on 1 node.[0m
[2mâ””â”€â”€ [0mJob started. Streaming logs... [2m(Ctrl-C to exit log streaming; job will not be killed)[0m
[36m(setup pid=2574)[0m Channels:
[36m(setup pid=2574)[0m  - nvidia
[36m(setup pid=2574)[0m  - defaults
[36m(setup pid=2574)[0m Platform: linux-64
[36m(setup pid=2574)[0m Collecting package metadata (repodata.json): ...working... done
[36m(setup pid=2574)[0m Solving environment: ...working... done
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m ## Package Plan ##
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m   environment location: /home/sky/miniconda3
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m   added / updated specs:
[36m(setup pid=2574)[0m     - cuda
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m The following packages will be downloaded:
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m     package                    |            build
[36m(setup pid=2574)[0m     ---------------------------|-----------------
[36m(setup pid=2574)[0m     _sysroot_linux-64_curr_repodata_hack-3|      haa98f57_10          12 KB
[36m(setup pid=2574)[0m     archspec-0.2.3             |     pyhd3eb1b0_0          47 KB
[36m(setup pid=2574)[0m     binutils_impl_linux-64-2.38|       h2a08ee3_1         5.2 MB
[36m(setup pid=2574)[0m     binutils_linux-64-2.38.0   |       hc2dff05_0          24 KB
[36m(setup pid=2574)[0m     ca-certificates-2025.2.25  |       h06a4308_0         129 KB
[36m(setup pid=2574)[0m     certifi-2025.7.14          |  py310h06a4308_0         160 KB
[36m(setup pid=2574)[0m     conda-24.11.3              |  py310h06a4308_0         930 KB
[36m(setup pid=2574)[0m     cpp-expected-1.1.0         |       hdb19cb5_0         130 KB
[36m(setup pid=2574)[0m     cuda-12.9.1                |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-cccl_linux-64-12.9.27 |                0         1.1 MB  nvidia
[36m(setup pid=2574)[0m     cuda-command-line-tools-12.9.1|                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-compiler-12.9.1       |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-crt-dev_linux-64-12.9.86|                0          84 KB  nvidia
[36m(setup pid=2574)[0m     cuda-crt-tools-12.9.86     |                0          20 KB  nvidia
[36m(setup pid=2574)[0m     cuda-cudart-12.9.79        |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-cudart-dev-12.9.79    |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-cudart-dev_linux-64-12.9.79|                0         374 KB  nvidia
[36m(setup pid=2574)[0m     cuda-cudart-static-12.9.79 |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-cudart-static_linux-64-12.9.79|                0         1.1 MB  nvidia
[36m(setup pid=2574)[0m     cuda-cudart_linux-64-12.9.79|                0         189 KB  nvidia
[36m(setup pid=2574)[0m     cuda-cuobjdump-12.9.82     |                1         241 KB  nvidia
[36m(setup pid=2574)[0m     cuda-cupti-12.9.79         |                0         1.8 MB  nvidia
[36m(setup pid=2574)[0m     cuda-cupti-dev-12.9.79     |                0         4.1 MB  nvidia
[36m(setup pid=2574)[0m     cuda-cuxxfilt-12.9.82      |                1         209 KB  nvidia
[36m(setup pid=2574)[0m     cuda-driver-dev-12.9.79    |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-driver-dev_linux-64-12.9.79|                0          31 KB  nvidia
[36m(setup pid=2574)[0m     cuda-gdb-12.9.79           |                1         374 KB  nvidia
[36m(setup pid=2574)[0m     cuda-libraries-12.9.1      |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-libraries-dev-12.9.1  |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-nsight-12.9.79        |                0       113.2 MB  nvidia
[36m(setup pid=2574)[0m     cuda-nvcc-12.9.86          |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-nvcc-dev_linux-64-12.9.86|                0        13.8 MB  nvidia
[36m(setup pid=2574)[0m     cuda-nvcc-impl-12.9.86     |                0          19 KB  nvidia
[36m(setup pid=2574)[0m     cuda-nvcc-tools-12.9.86    |                0        26.1 MB  nvidia
[36m(setup pid=2574)[0m     cuda-nvcc_linux-64-12.9.86 |                0          20 KB  nvidia
[36m(setup pid=2574)[0m     cuda-nvdisasm-12.9.88      |                1         5.3 MB  nvidia
[36m(setup pid=2574)[0m     cuda-nvml-dev-12.9.79      |                1         136 KB  nvidia
[36m(setup pid=2574)[0m     cuda-nvprof-12.9.79        |                0         2.5 MB  nvidia
[36m(setup pid=2574)[0m     cuda-nvprune-12.9.82       |                1          65 KB  nvidia
[36m(setup pid=2574)[0m     cuda-nvrtc-12.9.86         |                0        64.1 MB  nvidia
[36m(setup pid=2574)[0m     cuda-nvrtc-dev-12.9.86     |                0          31 KB  nvidia
[36m(setup pid=2574)[0m     cuda-nvtx-12.9.79          |                0          24 KB  nvidia
[36m(setup pid=2574)[0m     cuda-nvvm-dev_linux-64-12.9.86|                0          18 KB  nvidia
[36m(setup pid=2574)[0m     cuda-nvvm-impl-12.9.86     |                0        20.5 MB  nvidia
[36m(setup pid=2574)[0m     cuda-nvvm-tools-12.9.86    |                0        23.2 MB  nvidia
[36m(setup pid=2574)[0m     cuda-nvvp-12.9.79          |                1       112.4 MB  nvidia
[36m(setup pid=2574)[0m     cuda-opencl-12.9.19        |                0          25 KB  nvidia
[36m(setup pid=2574)[0m     cuda-opencl-dev-12.9.19    |                0          91 KB  nvidia
[36m(setup pid=2574)[0m     cuda-profiler-api-12.9.79  |                0          19 KB  nvidia
[36m(setup pid=2574)[0m     cuda-runtime-12.9.1        |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-sanitizer-api-12.9.79 |                1         8.8 MB  nvidia
[36m(setup pid=2574)[0m     cuda-toolkit-12.9.1        |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-tools-12.9.1          |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-version-12.9          |                3          17 KB  nvidia
[36m(setup pid=2574)[0m     cuda-visual-tools-12.9.1   |                0          17 KB  nvidia
[36m(setup pid=2574)[0m     dbus-1.13.18               |       hb2f20db_0         504 KB
[36m(setup pid=2574)[0m     expat-2.7.1                |       h6a678d5_0         182 KB
[36m(setup pid=2574)[0m     fontconfig-2.14.1          |       h55d465d_3         281 KB
[36m(setup pid=2574)[0m     freetype-2.13.3            |       h4a9f257_0         686 KB
[36m(setup pid=2574)[0m     frozendict-2.4.2           |  py310h5eee18b_0          55 KB
[36m(setup pid=2574)[0m     gcc_impl_linux-64-11.2.0   |       h1234567_1        22.2 MB
[36m(setup pid=2574)[0m     gcc_linux-64-11.2.0        |       h5c386dc_0          25 KB
[36m(setup pid=2574)[0m     gds-tools-1.14.1.1         |                4        37.7 MB  nvidia
[36m(setup pid=2574)[0m     glib-2.84.2                |       h6a678d5_0         526 KB
[36m(setup pid=2574)[0m     glib-tools-2.84.2          |       h6a678d5_0         119 KB
[36m(setup pid=2574)[0m     gmp-6.3.0                  |       h6a678d5_0         608 KB
[36m(setup pid=2574)[0m     gxx_impl_linux-64-11.2.0   |       h1234567_1        10.6 MB
[36m(setup pid=2574)[0m     gxx_linux-64-11.2.0        |       hc2dff05_0          24 KB
[36m(setup pid=2574)[0m     kernel-headers_linux-64-3.10.0|      h57e8cba_10         952 KB
[36m(setup pid=2574)[0m     libarchive-3.7.7           |       hfab0078_0         936 KB
[36m(setup pid=2574)[0m     libcublas-12.9.1.4         |                0       446.3 MB  nvidia
[36m(setup pid=2574)[0m     libcublas-dev-12.9.1.4     |                0          86 KB  nvidia
[36m(setup pid=2574)[0m     libcufft-11.4.1.4          |                0       154.8 MB  nvidia
[36m(setup pid=2574)[0m     libcufft-dev-11.4.1.4      |                0          29 KB  nvidia
[36m(setup pid=2574)[0m     libcufile-1.14.1.1         |                4         946 KB  nvidia
[36m(setup pid=2574)[0m     libcufile-dev-1.14.1.1     |                4          30 KB  nvidia
[36m(setup pid=2574)[0m     libcurand-10.3.10.19       |                0        44.0 MB  nvidia
[36m(setup pid=2574)[0m     libcurand-dev-10.3.10.19   |                0         238 KB  nvidia
[36m(setup pid=2574)[0m     libcusolver-11.7.5.82      |                0       195.7 MB  nvidia
[36m(setup pid=2574)[0m     libcusolver-dev-11.7.5.82  |                0          57 KB  nvidia
[36m(setup pid=2574)[0m     libcusparse-12.5.10.65     |                0       199.3 MB  nvidia
[36m(setup pid=2574)[0m     libcusparse-dev-12.5.10.65 |                0          46 KB  nvidia
[36m(setup pid=2574)[0m     libgcc-7.2.0               |       h69d50b8_2         269 KB
[36m(setup pid=2574)[0m     libgcc-devel_linux-64-11.2.0|       h1234567_1         2.5 MB
[36m(setup pid=2574)[0m     libglib-2.84.2             |       h37c7471_0         1.7 MB
[36m(setup pid=2574)[0m     libiconv-1.16              |       h5eee18b_3         759 KB
[36m(setup pid=2574)[0m     libmamba-2.0.5             |       haf1ee3a_1         2.2 MB
[36m(setup pid=2574)[0m     libmambapy-2.0.5           |  py310hdb19cb5_1         671 KB
[36m(setup pid=2574)[0m     libnpp-12.4.1.87           |                0       167.6 MB  nvidia
[36m(setup pid=2574)[0m     libnpp-dev-12.4.1.87       |                0         442 KB  nvidia
[36m(setup pid=2574)[0m     libnvfatbin-12.9.82        |                0         799 KB  nvidia
[36m(setup pid=2574)[0m     libnvfatbin-dev-12.9.82    |                0          22 KB  nvidia
[36m(setup pid=2574)[0m     libnvjitlink-12.9.86       |                0        29.2 MB  nvidia
[36m(setup pid=2574)[0m     libnvjitlink-dev-12.9.86   |                0          22 KB  nvidia
[36m(setup pid=2574)[0m     libnvjpeg-12.4.0.76        |                0         3.4 MB  nvidia
[36m(setup pid=2574)[0m     libnvjpeg-dev-12.4.0.76    |                0          27 KB  nvidia
[36m(setup pid=2574)[0m     libpng-1.6.39              |       h5eee18b_0         304 KB
[36m(setup pid=2574)[0m     libsolv-0.7.30             |       he621ea3_1         492 KB
[36m(setup pid=2574)[0m     libstdcxx-devel_linux-64-11.2.0|       h1234567_1        14.6 MB
[36m(setup pid=2574)[0m     libxcb-1.17.0              |       h9b100fa_0         430 KB
[36m(setup pid=2574)[0m     libxkbcommon-1.9.1         |       h69220b7_0         732 KB
[36m(setup pid=2574)[0m     libxml2-2.13.8             |       hfdd30dd_0         739 KB
[36m(setup pid=2574)[0m     nlohmann_json-3.11.2       |       h6a678d5_0         124 KB
[36m(setup pid=2574)[0m     nsight-compute-2025.2.1.3  |                0       319.3 MB  nvidia
[36m(setup pid=2574)[0m     nspr-4.35                  |       h6a678d5_0         244 KB
[36m(setup pid=2574)[0m     nss-3.89.1                 |       h6a678d5_0         2.1 MB
[36m(setup pid=2574)[0m     ocl-icd-2.3.2              |       h5eee18b_1         136 KB
[36m(setup pid=2574)[0m     openssl-3.0.17             |       h5eee18b_0         5.2 MB
[36m(setup pid=2574)[0m     pthread-stubs-0.3          |       h0ce48e5_1           5 KB
[36m(setup pid=2574)[0m     simdjson-3.10.1            |       hdb19cb5_0         258 KB
[36m(setup pid=2574)[0m     spdlog-1.11.0              |       hdb19cb5_0         234 KB
[36m(setup pid=2574)[0m     sysroot_linux-64-2.17      |      h57e8cba_10        32.6 MB
[36m(setup pid=2574)[0m     xkeyboard-config-2.44      |       h5eee18b_0         411 KB
[36m(setup pid=2574)[0m     xorg-libx11-1.8.12         |       h9b100fa_1         895 KB
[36m(setup pid=2574)[0m     xorg-libxau-1.0.12         |       h9b100fa_0          13 KB
[36m(setup pid=2574)[0m     xorg-libxdmcp-1.1.5        |       h9b100fa_0          19 KB
[36m(setup pid=2574)[0m     xorg-xorgproto-2024.1      |       h5eee18b_1         580 KB
[36m(setup pid=2574)[0m     xz-5.6.4                   |       h5eee18b_1         567 KB
[36m(setup pid=2574)[0m     ------------------------------------------------------------
[36m(setup pid=2574)[0m                                            Total:        2.06 GB
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m The following NEW packages will be INSTALLED:
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m   _sysroot_linux-64~ pkgs/main/noarch::_sysroot_linux-64_curr_repodata_hack-3-haa98f57_10 
[36m(setup pid=2574)[0m   binutils_impl_lin~ pkgs/main/linux-64::binutils_impl_linux-64-2.38-h2a08ee3_1 
[36m(setup pid=2574)[0m   binutils_linux-64  pkgs/main/linux-64::binutils_linux-64-2.38.0-hc2dff05_0 
[36m(setup pid=2574)[0m   cpp-expected       pkgs/main/linux-64::cpp-expected-1.1.0-hdb19cb5_0 
[36m(setup pid=2574)[0m   cuda               nvidia/linux-64::cuda-12.9.1-0 
[36m(setup pid=2574)[0m   cuda-cccl_linux-64 nvidia/linux-64::cuda-cccl_linux-64-12.9.27-0 
[36m(setup pid=2574)[0m   cuda-command-line~ nvidia/linux-64::cuda-command-line-tools-12.9.1-0 
[36m(setup pid=2574)[0m   cuda-compiler      nvidia/linux-64::cuda-compiler-12.9.1-0 
[36m(setup pid=2574)[0m   cuda-crt-dev_linu~ nvidia/noarch::cuda-crt-dev_linux-64-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-crt-tools     nvidia/linux-64::cuda-crt-tools-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-cudart        nvidia/linux-64::cuda-cudart-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-cudart-dev    nvidia/linux-64::cuda-cudart-dev-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-cudart-dev_l~ nvidia/noarch::cuda-cudart-dev_linux-64-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-cudart-static nvidia/linux-64::cuda-cudart-static-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-cudart-stati~ nvidia/noarch::cuda-cudart-static_linux-64-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-cudart_linux~ nvidia/noarch::cuda-cudart_linux-64-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-cuobjdump     nvidia/linux-64::cuda-cuobjdump-12.9.82-1 
[36m(setup pid=2574)[0m   cuda-cupti         nvidia/linux-64::cuda-cupti-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-cupti-dev     nvidia/linux-64::cuda-cupti-dev-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-cuxxfilt      nvidia/linux-64::cuda-cuxxfilt-12.9.82-1 
[36m(setup pid=2574)[0m   cuda-driver-dev    nvidia/linux-64::cuda-driver-dev-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-driver-dev_l~ nvidia/noarch::cuda-driver-dev_linux-64-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-gdb           nvidia/linux-64::cuda-gdb-12.9.79-1 
[36m(setup pid=2574)[0m   cuda-libraries     nvidia/linux-64::cuda-libraries-12.9.1-0 
[36m(setup pid=2574)[0m   cuda-libraries-dev nvidia/linux-64::cuda-libraries-dev-12.9.1-0 
[36m(setup pid=2574)[0m   cuda-nsight        nvidia/linux-64::cuda-nsight-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-nvcc          nvidia/linux-64::cuda-nvcc-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-nvcc-dev_lin~ nvidia/noarch::cuda-nvcc-dev_linux-64-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-nvcc-impl     nvidia/linux-64::cuda-nvcc-impl-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-nvcc-tools    nvidia/linux-64::cuda-nvcc-tools-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-nvcc_linux-64 nvidia/linux-64::cuda-nvcc_linux-64-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-nvdisasm      nvidia/linux-64::cuda-nvdisasm-12.9.88-1 
[36m(setup pid=2574)[0m   cuda-nvml-dev      nvidia/linux-64::cuda-nvml-dev-12.9.79-1 
[36m(setup pid=2574)[0m   cuda-nvprof        nvidia/linux-64::cuda-nvprof-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-nvprune       nvidia/linux-64::cuda-nvprune-12.9.82-1 
[36m(setup pid=2574)[0m   cuda-nvrtc         nvidia/linux-64::cuda-nvrtc-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-nvrtc-dev     nvidia/linux-64::cuda-nvrtc-dev-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-nvtx          nvidia/linux-64::cuda-nvtx-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-nvvm-dev_lin~ nvidia/noarch::cuda-nvvm-dev_linux-64-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-nvvm-impl     nvidia/linux-64::cuda-nvvm-impl-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-nvvm-tools    nvidia/linux-64::cuda-nvvm-tools-12.9.86-0 
[36m(setup pid=2574)[0m   cuda-nvvp          nvidia/linux-64::cuda-nvvp-12.9.79-1 
[36m(setup pid=2574)[0m   cuda-opencl        nvidia/linux-64::cuda-opencl-12.9.19-0 
[36m(setup pid=2574)[0m   cuda-opencl-dev    nvidia/linux-64::cuda-opencl-dev-12.9.19-0 
[36m(setup pid=2574)[0m   cuda-profiler-api  nvidia/linux-64::cuda-profiler-api-12.9.79-0 
[36m(setup pid=2574)[0m   cuda-runtime       nvidia/linux-64::cuda-runtime-12.9.1-0 
[36m(setup pid=2574)[0m   cuda-sanitizer-api nvidia/linux-64::cuda-sanitizer-api-12.9.79-1 
[36m(setup pid=2574)[0m   cuda-toolkit       nvidia/linux-64::cuda-toolkit-12.9.1-0 
[36m(setup pid=2574)[0m   cuda-tools         nvidia/linux-64::cuda-tools-12.9.1-0 
[36m(setup pid=2574)[0m   cuda-version       nvidia/noarch::cuda-version-12.9-3 
[36m(setup pid=2574)[0m   cuda-visual-tools  nvidia/linux-64::cuda-visual-tools-12.9.1-0 
[36m(setup pid=2574)[0m   dbus               pkgs/main/linux-64::dbus-1.13.18-hb2f20db_0 
[36m(setup pid=2574)[0m   expat              pkgs/main/linux-64::expat-2.7.1-h6a678d5_0 
[36m(setup pid=2574)[0m   fontconfig         pkgs/main/linux-64::fontconfig-2.14.1-h55d465d_3 
[36m(setup pid=2574)[0m   freetype           pkgs/main/linux-64::freetype-2.13.3-h4a9f257_0 
[36m(setup pid=2574)[0m   frozendict         pkgs/main/linux-64::frozendict-2.4.2-py310h5eee18b_0 
[36m(setup pid=2574)[0m   gcc_impl_linux-64  pkgs/main/linux-64::gcc_impl_linux-64-11.2.0-h1234567_1 
[36m(setup pid=2574)[0m   gcc_linux-64       pkgs/main/linux-64::gcc_linux-64-11.2.0-h5c386dc_0 
[36m(setup pid=2574)[0m   gds-tools          nvidia/linux-64::gds-tools-1.14.1.1-4 
[36m(setup pid=2574)[0m   glib               pkgs/main/linux-64::glib-2.84.2-h6a678d5_0 
[36m(setup pid=2574)[0m   glib-tools         pkgs/main/linux-64::glib-tools-2.84.2-h6a678d5_0 
[36m(setup pid=2574)[0m   gmp                pkgs/main/linux-64::gmp-6.3.0-h6a678d5_0 
[36m(setup pid=2574)[0m   gxx_impl_linux-64  pkgs/main/linux-64::gxx_impl_linux-64-11.2.0-h1234567_1 
[36m(setup pid=2574)[0m   gxx_linux-64       pkgs/main/linux-64::gxx_linux-64-11.2.0-hc2dff05_0 
[36m(setup pid=2574)[0m   kernel-headers_li~ pkgs/main/noarch::kernel-headers_linux-64-3.10.0-h57e8cba_10 
[36m(setup pid=2574)[0m   libcublas          nvidia/linux-64::libcublas-12.9.1.4-0 
[36m(setup pid=2574)[0m   libcublas-dev      nvidia/linux-64::libcublas-dev-12.9.1.4-0 
[36m(setup pid=2574)[0m   libcufft           nvidia/linux-64::libcufft-11.4.1.4-0 
[36m(setup pid=2574)[0m   libcufft-dev       nvidia/linux-64::libcufft-dev-11.4.1.4-0 
[36m(setup pid=2574)[0m   libcufile          nvidia/linux-64::libcufile-1.14.1.1-4 
[36m(setup pid=2574)[0m   libcufile-dev      nvidia/linux-64::libcufile-dev-1.14.1.1-4 
[36m(setup pid=2574)[0m   libcurand          nvidia/linux-64::libcurand-10.3.10.19-0 
[36m(setup pid=2574)[0m   libcurand-dev      nvidia/linux-64::libcurand-dev-10.3.10.19-0 
[36m(setup pid=2574)[0m   libcusolver        nvidia/linux-64::libcusolver-11.7.5.82-0 
[36m(setup pid=2574)[0m   libcusolver-dev    nvidia/linux-64::libcusolver-dev-11.7.5.82-0 
[36m(setup pid=2574)[0m   libcusparse        nvidia/linux-64::libcusparse-12.5.10.65-0 
[36m(setup pid=2574)[0m   libcusparse-dev    nvidia/linux-64::libcusparse-dev-12.5.10.65-0 
[36m(setup pid=2574)[0m   libgcc             pkgs/main/linux-64::libgcc-7.2.0-h69d50b8_2 
[36m(setup pid=2574)[0m   libgcc-devel_linu~ pkgs/main/linux-64::libgcc-devel_linux-64-11.2.0-h1234567_1 
[36m(setup pid=2574)[0m   libglib            pkgs/main/linux-64::libglib-2.84.2-h37c7471_0 
[36m(setup pid=2574)[0m   libiconv           pkgs/main/linux-64::libiconv-1.16-h5eee18b_3 
[36m(setup pid=2574)[0m   libnpp             nvidia/linux-64::libnpp-12.4.1.87-0 
[36m(setup pid=2574)[0m   libnpp-dev         nvidia/linux-64::libnpp-dev-12.4.1.87-0 
[36m(setup pid=2574)[0m   libnvfatbin        nvidia/linux-64::libnvfatbin-12.9.82-0 
[36m(setup pid=2574)[0m   libnvfatbin-dev    nvidia/linux-64::libnvfatbin-dev-12.9.82-0 
[36m(setup pid=2574)[0m   libnvjitlink       nvidia/linux-64::libnvjitlink-12.9.86-0 
[36m(setup pid=2574)[0m   libnvjitlink-dev   nvidia/linux-64::libnvjitlink-dev-12.9.86-0 
[36m(setup pid=2574)[0m   libnvjpeg          nvidia/linux-64::libnvjpeg-12.4.0.76-0 
[36m(setup pid=2574)[0m   libnvjpeg-dev      nvidia/linux-64::libnvjpeg-dev-12.4.0.76-0 
[36m(setup pid=2574)[0m   libpng             pkgs/main/linux-64::libpng-1.6.39-h5eee18b_0 
[36m(setup pid=2574)[0m   libstdcxx-devel_l~ pkgs/main/linux-64::libstdcxx-devel_linux-64-11.2.0-h1234567_1 
[36m(setup pid=2574)[0m   libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0 
[36m(setup pid=2574)[0m   libxkbcommon       pkgs/main/linux-64::libxkbcommon-1.9.1-h69220b7_0 
[36m(setup pid=2574)[0m   nlohmann_json      pkgs/main/linux-64::nlohmann_json-3.11.2-h6a678d5_0 
[36m(setup pid=2574)[0m   nsight-compute     nvidia/linux-64::nsight-compute-2025.2.1.3-0 
[36m(setup pid=2574)[0m   nspr               pkgs/main/linux-64::nspr-4.35-h6a678d5_0 
[36m(setup pid=2574)[0m   nss                pkgs/main/linux-64::nss-3.89.1-h6a678d5_0 
[36m(setup pid=2574)[0m   ocl-icd            pkgs/main/linux-64::ocl-icd-2.3.2-h5eee18b_1 
[36m(setup pid=2574)[0m   pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1 
[36m(setup pid=2574)[0m   simdjson           pkgs/main/linux-64::simdjson-3.10.1-hdb19cb5_0 
[36m(setup pid=2574)[0m   spdlog             pkgs/main/linux-64::spdlog-1.11.0-hdb19cb5_0 
[36m(setup pid=2574)[0m   sysroot_linux-64   pkgs/main/noarch::sysroot_linux-64-2.17-h57e8cba_10 
[36m(setup pid=2574)[0m   xkeyboard-config   pkgs/main/linux-64::xkeyboard-config-2.44-h5eee18b_0 
[36m(setup pid=2574)[0m   xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1 
[36m(setup pid=2574)[0m   xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0 
[36m(setup pid=2574)[0m   xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0 
[36m(setup pid=2574)[0m   xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1 
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m The following packages will be UPDATED:
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m   archspec                               0.2.1-pyhd3eb1b0_0 --> 0.2.3-pyhd3eb1b0_0 
[36m(setup pid=2574)[0m   ca-certificates                     2023.12.12-h06a4308_0 --> 2025.2.25-h06a4308_0 
[36m(setup pid=2574)[0m   certifi                        2023.11.17-py310h06a4308_0 --> 2025.7.14-py310h06a4308_0 
[36m(setup pid=2574)[0m   conda                             23.11.0-py310h06a4308_0 --> 24.11.3-py310h06a4308_0 
[36m(setup pid=2574)[0m   libarchive                               3.6.2-h6ac8c49_2 --> 3.7.7-hfab0078_0 
[36m(setup pid=2574)[0m   libmamba                                 1.5.3-haf1ee3a_0 --> 2.0.5-haf1ee3a_1 
[36m(setup pid=2574)[0m   libmambapy                          1.5.3-py310h2dafd23_0 --> 2.0.5-py310hdb19cb5_1 
[36m(setup pid=2574)[0m   libsolv                                 0.7.24-he621ea3_0 --> 0.7.30-he621ea3_1 
[36m(setup pid=2574)[0m   libxml2                                 2.10.4-hf1b16e4_1 --> 2.13.8-hfdd30dd_0 
[36m(setup pid=2574)[0m   openssl                                 3.0.12-h7f8727e_0 --> 3.0.17-h5eee18b_0 
[36m(setup pid=2574)[0m   xz                                       5.4.5-h5eee18b_0 --> 5.6.4-h5eee18b_1 
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m Proceed ([y]/n)? 
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m Downloading and Extracting Packages: ...working... done
[36m(setup pid=2574)[0m Preparing transaction: ...working... done
[36m(setup pid=2574)[0m Verifying transaction: ...working... done
[36m(setup pid=2574)[0m Executing transaction: ...working... done
[36m(setup pid=2574)[0m Error while loading conda entry point: conda-libmamba-solver (module 'libmambapy' has no attribute 'QueryFormat')
[36m(setup pid=2574)[0m Using CPython 3.10.13 interpreter at: /home/sky/miniconda3/bin/python3.10
[36m(setup pid=2574)[0m Creating virtual environment with seed packages at: /home/sky/training
[36m(setup pid=2574)[0m  + pip==25.2
[36m(setup pid=2574)[0m  + setuptools==80.9.0
[36m(setup pid=2574)[0m  + wheel==0.45.1
[36m(setup pid=2574)[0m Using Python 3.10.13 environment at: /home/sky/training
[36m(setup pid=2574)[0m Resolved 29 packages in 95ms
[36m(setup pid=2574)[0m Downloading nvidia-cufile-cu12 (1.1MiB)
[36m(setup pid=2574)[0m Downloading nvidia-curand-cu12 (53.7MiB)
[36m(setup pid=2574)[0m Downloading nvidia-cublas-cu12 (374.9MiB)
[36m(setup pid=2574)[0m Downloading nvidia-cusolver-cu12 (150.9MiB)
[36m(setup pid=2574)[0m Downloading torchaudio (3.3MiB)
[36m(setup pid=2574)[0m Downloading nvidia-cuda-nvrtc-cu12 (22.6MiB)
[36m(setup pid=2574)[0m Downloading nvidia-nccl-cu12 (192.0MiB)
[36m(setup pid=2574)[0m Downloading nvidia-cusparse-cu12 (206.5MiB)
[36m(setup pid=2574)[0m Downloading nvidia-cudnn-cu12 (544.5MiB)
[36m(setup pid=2574)[0m Downloading triton (148.4MiB)
[36m(setup pid=2574)[0m Downloading torch (783.1MiB)
[36m(setup pid=2574)[0m Downloading nvidia-nvjitlink-cu12 (18.8MiB)
[36m(setup pid=2574)[0m Downloading nvidia-cufft-cu12 (190.9MiB)
[36m(setup pid=2574)[0m Downloading numpy (16.0MiB)
[36m(setup pid=2574)[0m Downloading nvidia-cuda-cupti-cu12 (8.5MiB)
[36m(setup pid=2574)[0m Downloading sympy (6.0MiB)
[36m(setup pid=2574)[0m Downloading pillow (6.3MiB)
[36m(setup pid=2574)[0m Downloading torchvision (7.1MiB)
[36m(setup pid=2574)[0m Downloading nvidia-cusparselt-cu12 (149.5MiB)
[36m(setup pid=2574)[0m  Downloaded nvidia-cufile-cu12
[36m(setup pid=2574)[0m  Downloaded torchaudio
[36m(setup pid=2574)[0m  Downloaded torchvision
[36m(setup pid=2574)[0m  Downloaded pillow
[36m(setup pid=2574)[0m  Downloaded nvidia-cuda-cupti-cu12
[36m(setup pid=2574)[0m  Downloaded numpy
[36m(setup pid=2574)[0m  Downloaded nvidia-nvjitlink-cu12
[36m(setup pid=2574)[0m  Downloaded nvidia-cuda-nvrtc-cu12
[36m(setup pid=2574)[0m  Downloaded nvidia-curand-cu12
[36m(setup pid=2574)[0m  Downloaded sympy
[36m(setup pid=2574)[0m  Downloaded nvidia-cusparselt-cu12
[36m(setup pid=2574)[0m  Downloaded nvidia-cusolver-cu12
[36m(setup pid=2574)[0m  Downloaded triton
[36m(setup pid=2574)[0m  Downloaded nvidia-nccl-cu12
[36m(setup pid=2574)[0m  Downloaded nvidia-cufft-cu12
[36m(setup pid=2574)[0m  Downloaded nvidia-cusparse-cu12
[36m(setup pid=2574)[0m  Downloaded nvidia-cublas-cu12
[36m(setup pid=2574)[0m  Downloaded nvidia-cudnn-cu12
[36m(setup pid=2574)[0m  Downloaded torch
[36m(setup pid=2574)[0m Prepared 26 packages in 26.41s
[36m(setup pid=2574)[0m Installed 28 packages in 582ms
[36m(setup pid=2574)[0m  + filelock==3.18.0
[36m(setup pid=2574)[0m  + fsspec==2025.7.0
[36m(setup pid=2574)[0m  + jinja2==3.1.6
[36m(setup pid=2574)[0m  + markupsafe==3.0.2
[36m(setup pid=2574)[0m  + mpmath==1.3.0
[36m(setup pid=2574)[0m  + networkx==3.4.2
[36m(setup pid=2574)[0m  + numpy==2.2.6
[36m(setup pid=2574)[0m  + nvidia-cublas-cu12==12.6.4.1
[36m(setup pid=2574)[0m  + nvidia-cuda-cupti-cu12==12.6.80
[36m(setup pid=2574)[0m  + nvidia-cuda-nvrtc-cu12==12.6.77
[36m(setup pid=2574)[0m  + nvidia-cuda-runtime-cu12==12.6.77
[36m(setup pid=2574)[0m  + nvidia-cudnn-cu12==9.5.1.17
[36m(setup pid=2574)[0m  + nvidia-cufft-cu12==11.3.0.4
[36m(setup pid=2574)[0m  + nvidia-cufile-cu12==1.11.1.6
[36m(setup pid=2574)[0m  + nvidia-curand-cu12==10.3.7.77
[36m(setup pid=2574)[0m  + nvidia-cusolver-cu12==11.7.1.2
[36m(setup pid=2574)[0m  + nvidia-cusparse-cu12==12.5.4.2
[36m(setup pid=2574)[0m  + nvidia-cusparselt-cu12==0.6.3
[36m(setup pid=2574)[0m  + nvidia-nccl-cu12==2.26.2
[36m(setup pid=2574)[0m  + nvidia-nvjitlink-cu12==12.6.85
[36m(setup pid=2574)[0m  + nvidia-nvtx-cu12==12.6.77
[36m(setup pid=2574)[0m  + pillow==11.3.0
[36m(setup pid=2574)[0m  + sympy==1.14.0
[36m(setup pid=2574)[0m  + torch==2.7.1
[36m(setup pid=2574)[0m  + torchaudio==2.7.1
[36m(setup pid=2574)[0m  + torchvision==0.22.1
[36m(setup pid=2574)[0m  + triton==3.3.1
[36m(setup pid=2574)[0m  + typing-extensions==4.14.1
[36m(setup pid=2574)[0m Using Python 3.10.13 environment at: /home/sky/training
[36m(setup pid=2574)[0m Resolved 73 packages in 369ms
[36m(setup pid=2574)[0m    Building deepspeed==0.17.4
[36m(setup pid=2574)[0m Downloading pandas (11.8MiB)
[36m(setup pid=2574)[0m Downloading transformers (10.7MiB)
[36m(setup pid=2574)[0m Downloading aiohttp (1.6MiB)
[36m(setup pid=2574)[0m Downloading pydantic-core (1.9MiB)
[36m(setup pid=2574)[0m Downloading tokenizers (3.0MiB)
[36m(setup pid=2574)[0m Downloading hf-xet (3.0MiB)
[36m(setup pid=2574)[0m Downloading pyarrow (40.8MiB)
[36m(setup pid=2574)[0m  Downloaded pydantic-core
[36m(setup pid=2574)[0m  Downloaded tokenizers
[36m(setup pid=2574)[0m  Downloaded hf-xet
[36m(setup pid=2574)[0m  Downloaded aiohttp
[36m(setup pid=2574)[0m  Downloaded pyarrow
[36m(setup pid=2574)[0m  Downloaded pandas
[36m(setup pid=2574)[0m  Downloaded transformers
[36m(setup pid=2574)[0m       Built deepspeed==0.17.4
[36m(setup pid=2574)[0m Prepared 41 packages in 1.43s
[36m(setup pid=2574)[0m Uninstalled 1 package in 1ms
[36m(setup pid=2574)[0m Installed 48 packages in 145ms
[36m(setup pid=2574)[0m  + accelerate==1.9.0
[36m(setup pid=2574)[0m  + aiohappyeyeballs==2.6.1
[36m(setup pid=2574)[0m  + aiohttp==3.12.15
[36m(setup pid=2574)[0m  + aiosignal==1.4.0
[36m(setup pid=2574)[0m  + annotated-types==0.7.0
[36m(setup pid=2574)[0m  + async-timeout==5.0.1
[36m(setup pid=2574)[0m  + attrs==25.3.0
[36m(setup pid=2574)[0m  + certifi==2025.7.14
[36m(setup pid=2574)[0m  + charset-normalizer==3.4.2
[36m(setup pid=2574)[0m  + datasets==4.0.0
[36m(setup pid=2574)[0m  + deepspeed==0.17.4
[36m(setup pid=2574)[0m  + dill==0.3.8
[36m(setup pid=2574)[0m  + einops==0.8.1
[36m(setup pid=2574)[0m  + frozenlist==1.7.0
[36m(setup pid=2574)[0m  - fsspec==2025.7.0
[36m(setup pid=2574)[0m  + fsspec==2025.3.0
[36m(setup pid=2574)[0m  + hf-xet==1.1.5
[36m(setup pid=2574)[0m  + hjson==3.1.0
[36m(setup pid=2574)[0m  + huggingface-hub==0.34.3
[36m(setup pid=2574)[0m  + idna==3.10
[36m(setup pid=2574)[0m  + liger-kernel==0.6.1
[36m(setup pid=2574)[0m  + msgpack==1.1.1
[36m(setup pid=2574)[0m  + multidict==6.6.3
[36m(setup pid=2574)[0m  + multiprocess==0.70.16
[36m(setup pid=2574)[0m  + ninja==1.11.1.4
[36m(setup pid=2574)[0m  + packaging==25.0
[36m(setup pid=2574)[0m  + pandas==2.3.1
[36m(setup pid=2574)[0m  + propcache==0.3.2
[36m(setup pid=2574)[0m  + psutil==7.0.0
[36m(setup pid=2574)[0m  + py-cpuinfo==9.0.0
[36m(setup pid=2574)[0m  + pyarrow==21.0.0
[36m(setup pid=2574)[0m  + pydantic==2.11.7
[36m(setup pid=2574)[0m  + pydantic-core==2.33.2
[36m(setup pid=2574)[0m  + python-dateutil==2.9.0.post0
[36m(setup pid=2574)[0m  + pytz==2025.2
[36m(setup pid=2574)[0m  + pyyaml==6.0.2
[36m(setup pid=2574)[0m  + regex==2025.7.34
[36m(setup pid=2574)[0m  + requests==2.32.4
[36m(setup pid=2574)[0m  + safetensors==0.5.3
[36m(setup pid=2574)[0m  + six==1.17.0
[36m(setup pid=2574)[0m  + tokenizers==0.21.4
[36m(setup pid=2574)[0m  + tqdm==4.67.1
[36m(setup pid=2574)[0m  + transformers==4.54.1
[36m(setup pid=2574)[0m  + trl==0.20.0
[36m(setup pid=2574)[0m  + typing-inspection==0.4.1
[36m(setup pid=2574)[0m  + tzdata==2025.2
[36m(setup pid=2574)[0m  + urllib3==2.5.0
[36m(setup pid=2574)[0m  + xxhash==3.5.0
[36m(setup pid=2574)[0m  + yarl==1.20.1
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m Reading package lists...
[36m(setup pid=2574)[0m Building dependency tree...
[36m(setup pid=2574)[0m Reading state information...
[36m(setup pid=2574)[0m The following package was automatically installed and is no longer required:
[36m(setup pid=2574)[0m   libfuse2
[36m(setup pid=2574)[0m Use 'sudo apt autoremove' to remove it.
[36m(setup pid=2574)[0m The following additional packages will be installed:
[36m(setup pid=2574)[0m   alsa-topology-conf alsa-ucm-conf libasound2 libasound2-data libcanberra0
[36m(setup pid=2574)[0m   libgpm2 libltdl7 libogg0 libpython3.8 libpython3.8-minimal
[36m(setup pid=2574)[0m   libpython3.8-stdlib libtdb1 libvorbis0a libvorbisfile3 python3.8
[36m(setup pid=2574)[0m   python3.8-minimal sound-theme-freedesktop vim-common vim-runtime xxd
[36m(setup pid=2574)[0m Suggested packages:
[36m(setup pid=2574)[0m   libasound2-plugins alsa-utils libcanberra-gtk0 libcanberra-pulse gpm
[36m(setup pid=2574)[0m   python3.8-venv python3.8-doc binfmt-support ctags vim-doc vim-scripts
[36m(setup pid=2574)[0m The following NEW packages will be installed:
[36m(setup pid=2574)[0m   alsa-topology-conf alsa-ucm-conf libasound2 libasound2-data libcanberra0
[36m(setup pid=2574)[0m   libgpm2 libltdl7 libogg0 libpython3.8 libtdb1 libvorbis0a libvorbisfile3
[36m(setup pid=2574)[0m   sound-theme-freedesktop vim vim-common vim-runtime vmtouch xxd
[36m(setup pid=2574)[0m The following packages will be upgraded:
[36m(setup pid=2574)[0m   libpython3.8-minimal libpython3.8-stdlib python3.8 python3.8-minimal
[36m(setup pid=2574)[0m 4 upgraded, 18 newly installed, 0 to remove and 73 not upgraded.
[36m(setup pid=2574)[0m Need to get 14.6 MB of archives.
[36m(setup pid=2574)[0m After this operation, 44.4 MB of additional disk space will be used.
[36m(setup pid=2574)[0m Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3.8 amd64 3.8.10-0ubuntu1~20.04.18 [387 kB]
[36m(setup pid=2574)[0m Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpython3.8-stdlib amd64 3.8.10-0ubuntu1~20.04.18 [1676 kB]
[36m(setup pid=2574)[0m Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3.8-minimal amd64 3.8.10-0ubuntu1~20.04.18 [1900 kB]
[36m(setup pid=2574)[0m Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpython3.8-minimal amd64 3.8.10-0ubuntu1~20.04.18 [721 kB]
[36m(setup pid=2574)[0m Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 vmtouch amd64 1.3.1-1 [21.8 kB]
[36m(setup pid=2574)[0m Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 xxd amd64 2:8.1.2269-1ubuntu5.32 [50.0 kB]
[36m(setup pid=2574)[0m Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 vim-common all 2:8.1.2269-1ubuntu5.32 [84.9 kB]
[36m(setup pid=2574)[0m Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 alsa-topology-conf all 1.2.2-1 [7364 B]
[36m(setup pid=2574)[0m Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 alsa-ucm-conf all 1.2.2-1ubuntu0.13 [27.0 kB]
[36m(setup pid=2574)[0m Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libasound2-data all 1.2.2-2.1ubuntu2.5 [20.1 kB]
[36m(setup pid=2574)[0m Get:11 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libasound2 amd64 1.2.2-2.1ubuntu2.5 [335 kB]
[36m(setup pid=2574)[0m Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libltdl7 amd64 2.4.6-14 [38.5 kB]
[36m(setup pid=2574)[0m Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libtdb1 amd64 1.4.5-0ubuntu0.20.04.1 [44.2 kB]
[36m(setup pid=2574)[0m Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 libogg0 amd64 1.3.4-0ubuntu1 [24.0 kB]
[36m(setup pid=2574)[0m Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 libvorbis0a amd64 1.3.6-2ubuntu1 [87.0 kB]
[36m(setup pid=2574)[0m Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 libvorbisfile3 amd64 1.3.6-2ubuntu1 [16.1 kB]
[36m(setup pid=2574)[0m Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 sound-theme-freedesktop all 0.8-2ubuntu1 [384 kB]
[36m(setup pid=2574)[0m Get:18 http://archive.ubuntu.com/ubuntu focal/main amd64 libcanberra0 amd64 0.30-7ubuntu1 [38.1 kB]
[36m(setup pid=2574)[0m Get:19 http://archive.ubuntu.com/ubuntu focal/main amd64 libgpm2 amd64 1.20.7-5 [15.1 kB]
[36m(setup pid=2574)[0m Get:20 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpython3.8 amd64 3.8.10-0ubuntu1~20.04.18 [1625 kB]
[36m(setup pid=2574)[0m Get:21 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 vim-runtime all 2:8.1.2269-1ubuntu5.32 [5876 kB]
[36m(setup pid=2574)[0m Get:22 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 vim amd64 2:8.1.2269-1ubuntu5.32 [1241 kB]
[36m(setup pid=2574)[0m debconf: delaying package configuration, since apt-utils is not installed
[36m(setup pid=2574)[0m Fetched 14.6 MB in 1s (21.0 MB/s)
[36m(setup pid=2574)[0m (Reading database ... 
(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%
(Reading database ... 70%
(Reading database ... 75%
(Reading database ... 80%
(Reading database ... 85%
(Reading database ... 90%
(Reading database ... 95%
(Reading database ... 100%
(Reading database ... 20384 files and directories currently installed.)
[36m(setup pid=2574)[0m Preparing to unpack .../00-python3.8_3.8.10-0ubuntu1~20.04.18_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking python3.8 (3.8.10-0ubuntu1~20.04.18) over (3.8.10-0ubuntu1~20.04.15) ...
[36m(setup pid=2574)[0m Preparing to unpack .../01-libpython3.8-stdlib_3.8.10-0ubuntu1~20.04.18_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libpython3.8-stdlib:amd64 (3.8.10-0ubuntu1~20.04.18) over (3.8.10-0ubuntu1~20.04.15) ...
[36m(setup pid=2574)[0m Preparing to unpack .../02-python3.8-minimal_3.8.10-0ubuntu1~20.04.18_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking python3.8-minimal (3.8.10-0ubuntu1~20.04.18) over (3.8.10-0ubuntu1~20.04.15) ...
[36m(setup pid=2574)[0m Preparing to unpack .../03-libpython3.8-minimal_3.8.10-0ubuntu1~20.04.18_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libpython3.8-minimal:amd64 (3.8.10-0ubuntu1~20.04.18) over (3.8.10-0ubuntu1~20.04.15) ...
[36m(setup pid=2574)[0m Selecting previously unselected package vmtouch.
[36m(setup pid=2574)[0m Preparing to unpack .../04-vmtouch_1.3.1-1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking vmtouch (1.3.1-1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package xxd.
[36m(setup pid=2574)[0m Preparing to unpack .../05-xxd_2%3a8.1.2269-1ubuntu5.32_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking xxd (2:8.1.2269-1ubuntu5.32) ...
[36m(setup pid=2574)[0m Selecting previously unselected package vim-common.
[36m(setup pid=2574)[0m Preparing to unpack .../06-vim-common_2%3a8.1.2269-1ubuntu5.32_all.deb ...
[36m(setup pid=2574)[0m Unpacking vim-common (2:8.1.2269-1ubuntu5.32) ...
[36m(setup pid=2574)[0m Selecting previously unselected package alsa-topology-conf.
[36m(setup pid=2574)[0m Preparing to unpack .../07-alsa-topology-conf_1.2.2-1_all.deb ...
[36m(setup pid=2574)[0m Unpacking alsa-topology-conf (1.2.2-1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package alsa-ucm-conf.
[36m(setup pid=2574)[0m Preparing to unpack .../08-alsa-ucm-conf_1.2.2-1ubuntu0.13_all.deb ...
[36m(setup pid=2574)[0m Unpacking alsa-ucm-conf (1.2.2-1ubuntu0.13) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libasound2-data.
[36m(setup pid=2574)[0m Preparing to unpack .../09-libasound2-data_1.2.2-2.1ubuntu2.5_all.deb ...
[36m(setup pid=2574)[0m Unpacking libasound2-data (1.2.2-2.1ubuntu2.5) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libasound2:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../10-libasound2_1.2.2-2.1ubuntu2.5_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libasound2:amd64 (1.2.2-2.1ubuntu2.5) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libltdl7:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../11-libltdl7_2.4.6-14_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libltdl7:amd64 (2.4.6-14) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libtdb1:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../12-libtdb1_1.4.5-0ubuntu0.20.04.1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libtdb1:amd64 (1.4.5-0ubuntu0.20.04.1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libogg0:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../13-libogg0_1.3.4-0ubuntu1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libogg0:amd64 (1.3.4-0ubuntu1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libvorbis0a:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../14-libvorbis0a_1.3.6-2ubuntu1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libvorbis0a:amd64 (1.3.6-2ubuntu1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libvorbisfile3:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../15-libvorbisfile3_1.3.6-2ubuntu1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libvorbisfile3:amd64 (1.3.6-2ubuntu1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package sound-theme-freedesktop.
[36m(setup pid=2574)[0m Preparing to unpack .../16-sound-theme-freedesktop_0.8-2ubuntu1_all.deb ...
[36m(setup pid=2574)[0m Unpacking sound-theme-freedesktop (0.8-2ubuntu1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libcanberra0:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../17-libcanberra0_0.30-7ubuntu1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libcanberra0:amd64 (0.30-7ubuntu1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libgpm2:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../18-libgpm2_1.20.7-5_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libgpm2:amd64 (1.20.7-5) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libpython3.8:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../19-libpython3.8_3.8.10-0ubuntu1~20.04.18_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libpython3.8:amd64 (3.8.10-0ubuntu1~20.04.18) ...
[36m(setup pid=2574)[0m Selecting previously unselected package vim-runtime.
[36m(setup pid=2574)[0m Preparing to unpack .../20-vim-runtime_2%3a8.1.2269-1ubuntu5.32_all.deb ...
[36m(setup pid=2574)[0m Adding 'diversion of /usr/share/vim/vim81/doc/help.txt to /usr/share/vim/vim81/doc/help.txt.vim-tiny by vim-runtime'
[36m(setup pid=2574)[0m Adding 'diversion of /usr/share/vim/vim81/doc/tags to /usr/share/vim/vim81/doc/tags.vim-tiny by vim-runtime'
[36m(setup pid=2574)[0m Unpacking vim-runtime (2:8.1.2269-1ubuntu5.32) ...
[36m(setup pid=2574)[0m Selecting previously unselected package vim.
[36m(setup pid=2574)[0m Preparing to unpack .../21-vim_2%3a8.1.2269-1ubuntu5.32_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking vim (2:8.1.2269-1ubuntu5.32) ...
[36m(setup pid=2574)[0m Setting up libpython3.8-minimal:amd64 (3.8.10-0ubuntu1~20.04.18) ...
[36m(setup pid=2574)[0m Setting up libgpm2:amd64 (1.20.7-5) ...
[36m(setup pid=2574)[0m Setting up libogg0:amd64 (1.3.4-0ubuntu1) ...
[36m(setup pid=2574)[0m Setting up alsa-ucm-conf (1.2.2-1ubuntu0.13) ...
[36m(setup pid=2574)[0m Setting up libtdb1:amd64 (1.4.5-0ubuntu0.20.04.1) ...
[36m(setup pid=2574)[0m Setting up vmtouch (1.3.1-1) ...
[36m(setup pid=2574)[0m invoke-rc.d: could not determine current runlevel
[36m(setup pid=2574)[0m invoke-rc.d: policy-rc.d denied execution of start.
[36m(setup pid=2574)[0m Setting up xxd (2:8.1.2269-1ubuntu5.32) ...
[36m(setup pid=2574)[0m Setting up libasound2-data (1.2.2-2.1ubuntu2.5) ...
[36m(setup pid=2574)[0m Setting up vim-common (2:8.1.2269-1ubuntu5.32) ...
[36m(setup pid=2574)[0m Setting up libvorbis0a:amd64 (1.3.6-2ubuntu1) ...
[36m(setup pid=2574)[0m Setting up libltdl7:amd64 (2.4.6-14) ...
[36m(setup pid=2574)[0m Setting up alsa-topology-conf (1.2.2-1) ...
[36m(setup pid=2574)[0m Setting up python3.8-minimal (3.8.10-0ubuntu1~20.04.18) ...
[36m(setup pid=2574)[0m Setting up sound-theme-freedesktop (0.8-2ubuntu1) ...
[36m(setup pid=2574)[0m Setting up libasound2:amd64 (1.2.2-2.1ubuntu2.5) ...
[36m(setup pid=2574)[0m Setting up vim-runtime (2:8.1.2269-1ubuntu5.32) ...
[36m(setup pid=2574)[0m Setting up libpython3.8-stdlib:amd64 (3.8.10-0ubuntu1~20.04.18) ...
[36m(setup pid=2574)[0m Setting up python3.8 (3.8.10-0ubuntu1~20.04.18) ...
[36m(setup pid=2574)[0m Setting up libvorbisfile3:amd64 (1.3.6-2ubuntu1) ...
[36m(setup pid=2574)[0m Setting up libpython3.8:amd64 (3.8.10-0ubuntu1~20.04.18) ...
[36m(setup pid=2574)[0m Setting up libcanberra0:amd64 (0.30-7ubuntu1) ...
[36m(setup pid=2574)[0m Setting up vim (2:8.1.2269-1ubuntu5.32) ...
[36m(setup pid=2574)[0m update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vim (vim) in auto mode
[36m(setup pid=2574)[0m update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vimdiff (vimdiff) in auto mode
[36m(setup pid=2574)[0m update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rvim (rvim) in auto mode
[36m(setup pid=2574)[0m update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rview (rview) in auto mode
[36m(setup pid=2574)[0m update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vi (vi) in auto mode
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/da/man1/vi.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group vi) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/de/man1/vi.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group vi) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/fr/man1/vi.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group vi) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/it/man1/vi.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group vi) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/ja/man1/vi.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group vi) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/pl/man1/vi.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group vi) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/ru/man1/vi.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group vi) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/man1/vi.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group vi) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/view (view) in auto mode
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/da/man1/view.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group view) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/de/man1/view.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group view) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/fr/man1/view.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group view) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/it/man1/view.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group view) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/ja/man1/view.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group view) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/pl/man1/view.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group view) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/ru/man1/view.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group view) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/man1/view.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group view) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/ex (ex) in auto mode
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/da/man1/ex.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group ex) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/de/man1/ex.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group ex) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/fr/man1/ex.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group ex) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/it/man1/ex.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group ex) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/ja/man1/ex.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group ex) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/pl/man1/ex.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group ex) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/ru/man1/ex.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group ex) doesn't exist
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/man1/ex.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group ex) doesn't exist
[36m(setup pid=2574)[0m Processing triggers for libc-bin (2.31-0ubuntu9.12) ...
[36m(setup pid=2574)[0m Processing triggers for systemd (245.4-4ubuntu3.24) ...
[36m(setup pid=2574)[0m Processing triggers for mime-support (3.64ubuntu1) ...
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m Reading package lists...
[36m(setup pid=2574)[0m Building dependency tree...
[36m(setup pid=2574)[0m Reading state information...
[36m(setup pid=2574)[0m The following package was automatically installed and is no longer required:
[36m(setup pid=2574)[0m   libfuse2
[36m(setup pid=2574)[0m Use 'sudo apt autoremove' to remove it.
[36m(setup pid=2574)[0m The following additional packages will be installed:
[36m(setup pid=2574)[0m   cron libatm1 libcap2-bin libelf1 libibmad5 libibnetdisc5 libibumad3 libmnl0
[36m(setup pid=2574)[0m   libpam-cap libsensors-config libsensors5 libxtables12
[36m(setup pid=2574)[0m Suggested packages:
[36m(setup pid=2574)[0m   anacron logrotate checksecurity default-mta | mail-transport-agent lsof
[36m(setup pid=2574)[0m   strace iproute2-doc lm-sensors isag
[36m(setup pid=2574)[0m The following NEW packages will be installed:
[36m(setup pid=2574)[0m   cron htop infiniband-diags iproute2 libatm1 libcap2-bin libelf1 libibmad5
[36m(setup pid=2574)[0m   libibnetdisc5 libibumad3 libmnl0 libpam-cap libsensors-config libsensors5
[36m(setup pid=2574)[0m   libxtables12 net-tools sysstat
[36m(setup pid=2574)[0m 0 upgraded, 17 newly installed, 0 to remove and 73 not upgraded.
[36m(setup pid=2574)[0m Need to get 2145 kB of archives.
[36m(setup pid=2574)[0m After this operation, 7876 kB of additional disk space will be used.
[36m(setup pid=2574)[0m Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 cron amd64 3.0pl1-136ubuntu1 [71.5 kB]
[36m(setup pid=2574)[0m Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libelf1 amd64 0.176-1.1ubuntu0.1 [44.2 kB]
[36m(setup pid=2574)[0m Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libmnl0 amd64 1.0.4-2 [12.3 kB]
[36m(setup pid=2574)[0m Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libxtables12 amd64 1.8.4-3ubuntu2.1 [28.7 kB]
[36m(setup pid=2574)[0m Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libcap2-bin amd64 1:2.32-1ubuntu0.2 [26.2 kB]
[36m(setup pid=2574)[0m Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 iproute2 amd64 5.5.0-1ubuntu1 [858 kB]
[36m(setup pid=2574)[0m Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 libatm1 amd64 1:2.5.1-4 [21.8 kB]
[36m(setup pid=2574)[0m Get:8 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpam-cap amd64 1:2.32-1ubuntu0.2 [8376 B]
[36m(setup pid=2574)[0m Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 htop amd64 2.2.0-2build1 [80.5 kB]
[36m(setup pid=2574)[0m Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsensors-config all 1:3.6.0-2ubuntu1.1 [6052 B]
[36m(setup pid=2574)[0m Get:11 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsensors5 amd64 1:3.6.0-2ubuntu1.1 [27.2 kB]
[36m(setup pid=2574)[0m Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 net-tools amd64 1.60+git20180626.aebd88e-1ubuntu1.3 [192 kB]
[36m(setup pid=2574)[0m Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 sysstat amd64 12.2.0-2ubuntu0.3 [448 kB]
[36m(setup pid=2574)[0m Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 libibumad3 amd64 28.0-1ubuntu1 [25.8 kB]
[36m(setup pid=2574)[0m Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 libibmad5 amd64 28.0-1ubuntu1 [39.7 kB]
[36m(setup pid=2574)[0m Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 libibnetdisc5 amd64 28.0-1ubuntu1 [30.5 kB]
[36m(setup pid=2574)[0m Get:17 http://archive.ubuntu.com/ubuntu focal/universe amd64 infiniband-diags amd64 28.0-1ubuntu1 [224 kB]
[36m(setup pid=2574)[0m debconf: delaying package configuration, since apt-utils is not installed
[36m(setup pid=2574)[0m Fetched 2145 kB in 1s (1781 kB/s)
[36m(setup pid=2574)[0m Selecting previously unselected package cron.
[36m(setup pid=2574)[0m (Reading database ... 
(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%
(Reading database ... 70%
(Reading database ... 75%
(Reading database ... 80%
(Reading database ... 85%
(Reading database ... 90%
(Reading database ... 95%
(Reading database ... 100%
(Reading database ... 22757 files and directories currently installed.)
[36m(setup pid=2574)[0m Preparing to unpack .../00-cron_3.0pl1-136ubuntu1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking cron (3.0pl1-136ubuntu1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libelf1:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../01-libelf1_0.176-1.1ubuntu0.1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libelf1:amd64 (0.176-1.1ubuntu0.1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libmnl0:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../02-libmnl0_1.0.4-2_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libmnl0:amd64 (1.0.4-2) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libxtables12:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../03-libxtables12_1.8.4-3ubuntu2.1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libxtables12:amd64 (1.8.4-3ubuntu2.1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libcap2-bin.
[36m(setup pid=2574)[0m Preparing to unpack .../04-libcap2-bin_1%3a2.32-1ubuntu0.2_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libcap2-bin (1:2.32-1ubuntu0.2) ...
[36m(setup pid=2574)[0m Selecting previously unselected package iproute2.
[36m(setup pid=2574)[0m Preparing to unpack .../05-iproute2_5.5.0-1ubuntu1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking iproute2 (5.5.0-1ubuntu1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libatm1:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../06-libatm1_1%3a2.5.1-4_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libatm1:amd64 (1:2.5.1-4) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libpam-cap:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../07-libpam-cap_1%3a2.32-1ubuntu0.2_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libpam-cap:amd64 (1:2.32-1ubuntu0.2) ...
[36m(setup pid=2574)[0m Selecting previously unselected package htop.
[36m(setup pid=2574)[0m Preparing to unpack .../08-htop_2.2.0-2build1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking htop (2.2.0-2build1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libsensors-config.
[36m(setup pid=2574)[0m Preparing to unpack .../09-libsensors-config_1%3a3.6.0-2ubuntu1.1_all.deb ...
[36m(setup pid=2574)[0m Unpacking libsensors-config (1:3.6.0-2ubuntu1.1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libsensors5:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../10-libsensors5_1%3a3.6.0-2ubuntu1.1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libsensors5:amd64 (1:3.6.0-2ubuntu1.1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package net-tools.
[36m(setup pid=2574)[0m Preparing to unpack .../11-net-tools_1.60+git20180626.aebd88e-1ubuntu1.3_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking net-tools (1.60+git20180626.aebd88e-1ubuntu1.3) ...
[36m(setup pid=2574)[0m Selecting previously unselected package sysstat.
[36m(setup pid=2574)[0m Preparing to unpack .../12-sysstat_12.2.0-2ubuntu0.3_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking sysstat (12.2.0-2ubuntu0.3) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libibumad3:amd64.
[36m(setup pid=2574)[0m Preparing to unpack .../13-libibumad3_28.0-1ubuntu1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libibumad3:amd64 (28.0-1ubuntu1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libibmad5.
[36m(setup pid=2574)[0m Preparing to unpack .../14-libibmad5_28.0-1ubuntu1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libibmad5 (28.0-1ubuntu1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package libibnetdisc5.
[36m(setup pid=2574)[0m Preparing to unpack .../15-libibnetdisc5_28.0-1ubuntu1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking libibnetdisc5 (28.0-1ubuntu1) ...
[36m(setup pid=2574)[0m Selecting previously unselected package infiniband-diags.
[36m(setup pid=2574)[0m Preparing to unpack .../16-infiniband-diags_28.0-1ubuntu1_amd64.deb ...
[36m(setup pid=2574)[0m Unpacking infiniband-diags (28.0-1ubuntu1) ...
[36m(setup pid=2574)[0m Setting up net-tools (1.60+git20180626.aebd88e-1ubuntu1.3) ...
[36m(setup pid=2574)[0m Setting up htop (2.2.0-2build1) ...
[36m(setup pid=2574)[0m Setting up cron (3.0pl1-136ubuntu1) ...
[36m(setup pid=2574)[0m Adding group `crontab' (GID 107) ...
[36m(setup pid=2574)[0m Done.
[36m(setup pid=2574)[0m invoke-rc.d: could not determine current runlevel
[36m(setup pid=2574)[0m invoke-rc.d: policy-rc.d denied execution of start.
[36m(setup pid=2574)[0m Created symlink /etc/systemd/system/multi-user.target.wants/cron.service â†’ /lib/systemd/system/cron.service.
[36m(setup pid=2574)[0m Setting up libsensors-config (1:3.6.0-2ubuntu1.1) ...
[36m(setup pid=2574)[0m Setting up libibumad3:amd64 (28.0-1ubuntu1) ...
[36m(setup pid=2574)[0m Setting up libibmad5 (28.0-1ubuntu1) ...
[36m(setup pid=2574)[0m Setting up libatm1:amd64 (1:2.5.1-4) ...
[36m(setup pid=2574)[0m Setting up libcap2-bin (1:2.32-1ubuntu0.2) ...
[36m(setup pid=2574)[0m Setting up libmnl0:amd64 (1.0.4-2) ...
[36m(setup pid=2574)[0m Setting up libibnetdisc5 (28.0-1ubuntu1) ...
[36m(setup pid=2574)[0m Setting up libsensors5:amd64 (1:3.6.0-2ubuntu1.1) ...
[36m(setup pid=2574)[0m Setting up libxtables12:amd64 (1.8.4-3ubuntu2.1) ...
[36m(setup pid=2574)[0m Setting up libelf1:amd64 (0.176-1.1ubuntu0.1) ...
[36m(setup pid=2574)[0m Setting up libpam-cap:amd64 (1:2.32-1ubuntu0.2) ...
[36m(setup pid=2574)[0m debconf: unable to initialize frontend: Dialog
[36m(setup pid=2574)[0m debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)
[36m(setup pid=2574)[0m debconf: falling back to frontend: Readline
[36m(setup pid=2574)[0m Setting up sysstat (12.2.0-2ubuntu0.3) ...
[36m(setup pid=2574)[0m debconf: unable to initialize frontend: Dialog
[36m(setup pid=2574)[0m debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)
[36m(setup pid=2574)[0m debconf: falling back to frontend: Readline
[36m(setup pid=2574)[0m 
[36m(setup pid=2574)[0m Creating config file /etc/default/sysstat with new version
[36m(setup pid=2574)[0m update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode
[36m(setup pid=2574)[0m update-alternatives: warning: skip creation of /usr/share/man/man1/sar.1.gz because associated file /usr/share/man/man1/sar.sysstat.1.gz (of link group sar) doesn't exist
[36m(setup pid=2574)[0m Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service â†’ /lib/systemd/system/sysstat.service.
[36m(setup pid=2574)[0m Setting up infiniband-diags (28.0-1ubuntu1) ...
[36m(setup pid=2574)[0m Setting up iproute2 (5.5.0-1ubuntu1) ...
[36m(setup pid=2574)[0m debconf: unable to initialize frontend: Dialog
[36m(setup pid=2574)[0m debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)
[36m(setup pid=2574)[0m debconf: falling back to frontend: Readline
[36m(setup pid=2574)[0m Processing triggers for systemd (245.4-4ubuntu3.24) ...
[36m(setup pid=2574)[0m Processing triggers for mime-support (3.64ubuntu1) ...
[36m(setup pid=2574)[0m Processing triggers for libc-bin (2.31-0ubuntu9.12) ...
[36m(setup pid=2574)[0m Using Python 3.10.13 environment at: /home/sky/training
[36m(setup pid=2574)[0m Resolved 3 packages in 50ms
[36m(setup pid=2574)[0m Prepared 2 packages in 10ms
[36m(setup pid=2574)[0m Installed 2 packages in 16ms
[36m(setup pid=2574)[0m  + nvidia-ml-py==12.575.51
[36m(setup pid=2574)[0m  + nvitop==1.5.2
[36m(task, pid=2574)[0m Error while loading conda entry point: conda-libmamba-solver (module 'libmambapy' has no attribute 'QueryFormat')
[36m(task, pid=2574)[0m Error while loading conda entry point: conda-libmamba-solver (module 'libmambapy' has no attribute 'QueryFormat')
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Processing directory 1/2: /tmp/checkpoint ===
[36m(task, pid=2574)[0m Preserving existing cache directories...
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /tmp/checkpoint/dataset_cache
[36m(task, pid=2574)[0m Model cache: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Downloading and caching dataset...
[36m(task, pid=2574)[0m Setting num_proc from 128 to 10 for the train split as it only contains 10 shards.
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m Generating train split:   0%|          | 0/47780 [00:00<?, ? examples/s]
[36m(task, pid=2574)[0m Generating train split:   2%|â–         | 1000/47780 [00:00<00:24, 1944.21 examples/s]
[36m(task, pid=2574)[0m Generating train split:  15%|â–ˆâ–        | 7000/47780 [00:00<00:02, 14537.03 examples/s]
[36m(task, pid=2574)[0m Generating train split:  23%|â–ˆâ–ˆâ–Ž       | 11000/47780 [00:00<00:01, 20058.99 examples/s]
[36m(task, pid=2574)[0m Generating train split:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 18000/47780 [00:00<00:00, 32115.87 examples/s]
[36m(task, pid=2574)[0m Generating train split:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23000/47780 [00:00<00:00, 33225.94 examples/s]
[36m(task, pid=2574)[0m Generating train split:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 29000/47780 [00:01<00:00, 38151.05 examples/s]
[36m(task, pid=2574)[0m Generating train split:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40112/47780 [00:01<00:00, 55468.97 examples/s]
[36m(task, pid=2574)[0m Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47780/47780 [00:01<00:00, 59713.30 examples/s]
Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47780/47780 [00:01<00:00, 34877.15 examples/s]
[36m(task, pid=2574)[0m Dataset downloaded successfully. Size: 47780 examples
[36m(task, pid=2574)[0m Flushing filesystem cache for: /tmp/checkpoint/dataset_cache
[36m(task, pid=2574)[0m Successfully flushed filesystem cache
[36m(task, pid=2574)[0m vmtouch output: Files: 14
[36m(task, pid=2574)[0m      Directories: 5
[36m(task, pid=2574)[0m    Evicted Pages: 1212952 (4G)
[36m(task, pid=2574)[0m          Elapsed: 3.6521 seconds
[36m(task, pid=2574)[0m Downloading and caching model...
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]
[36m(task, pid=2574)[0m Fetching 5 files:  20%|â–ˆâ–ˆ        | 1/5 [00:25<01:42, 25.63s/it]
[36m(task, pid=2574)[0m Fetching 5 files:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:25<00:13,  6.69s/it]
Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:26<00:00,  3.39s/it]
Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:26<00:00,  5.26s/it]
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:04<00:17,  4.39s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:08<00:12,  4.31s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:12<00:08,  4.27s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:17<00:04,  4.29s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:21<00:00,  4.18s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:21<00:00,  4.23s/it]
[36m(task, pid=2574)[0m Model downloaded and cached at: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Flushing filesystem cache for: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Successfully flushed filesystem cache
[36m(task, pid=2574)[0m vmtouch output: Files: 18
[36m(task, pid=2574)[0m      Directories: 10
[36m(task, pid=2574)[0m    Evicted Pages: 5950909 (22G)
[36m(task, pid=2574)[0m          Elapsed: 2.4255 seconds
[36m(task, pid=2574)[0m Completed processing directory 1/2
[36m(task, pid=2574)[0m Flushing filesystem cache for: /tmp/checkpoint/dataset_cache, /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Successfully flushed filesystem cache
[36m(task, pid=2574)[0m vmtouch output: Files: 32
[36m(task, pid=2574)[0m      Directories: 15
[36m(task, pid=2574)[0m    Evicted Pages: 7163861 (27G)
[36m(task, pid=2574)[0m          Elapsed: 0.005782 seconds
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Processing directory 2/2: /mnt/data ===
[36m(task, pid=2574)[0m Preserving existing cache directories...
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /mnt/data/dataset_cache
[36m(task, pid=2574)[0m Model cache: /mnt/data/model_cache
[36m(task, pid=2574)[0m Downloading and caching dataset...
[36m(task, pid=2574)[0m Dataset downloaded successfully. Size: 47780 examples
[36m(task, pid=2574)[0m Flushing filesystem cache for: /mnt/data/dataset_cache
[36m(task, pid=2574)[0m Successfully flushed filesystem cache
[36m(task, pid=2574)[0m vmtouch output: Files: 271
[36m(task, pid=2574)[0m      Directories: 5
[36m(task, pid=2574)[0m    Evicted Pages: 4618162 (17G)
[36m(task, pid=2574)[0m          Elapsed: 0.51712 seconds
[36m(task, pid=2574)[0m Downloading and caching model...
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:23,  5.80s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:11<00:16,  5.51s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:16<00:10,  5.50s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:22<00:05,  5.76s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  5.76s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  5.70s/it]
[36m(task, pid=2574)[0m Model downloaded and cached at: /mnt/data/model_cache
[36m(task, pid=2574)[0m Flushing filesystem cache for: /mnt/data/model_cache
[36m(task, pid=2574)[0m Successfully flushed filesystem cache
[36m(task, pid=2574)[0m vmtouch output: Files: 18
[36m(task, pid=2574)[0m      Directories: 10
[36m(task, pid=2574)[0m    Evicted Pages: 5950909 (22G)
[36m(task, pid=2574)[0m          Elapsed: 1.182 seconds
[36m(task, pid=2574)[0m Completed processing directory 2/2
[36m(task, pid=2574)[0m Flushing filesystem cache for: /mnt/data/dataset_cache, /mnt/data/model_cache
[36m(task, pid=2574)[0m Successfully flushed filesystem cache
[36m(task, pid=2574)[0m vmtouch output: Files: 289
[36m(task, pid=2574)[0m      Directories: 15
[36m(task, pid=2574)[0m    Evicted Pages: 10569071 (40G)
[36m(task, pid=2574)[0m          Elapsed: 0.31926 seconds
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Copying cached data to S3 directories ===
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m Copying to S3 directory 1/2: /checkpoints_s3
[36m(task, pid=2574)[0m Copying dataset cache from /tmp/checkpoint/dataset_cache to /checkpoints_s3/dataset_cache
[36m(task, pid=2574)[0m Dataset cache copied successfully
[36m(task, pid=2574)[0m Copying model cache from /tmp/checkpoint/model_cache to /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Model cache copied successfully
[36m(task, pid=2574)[0m Copying checkpoints from /tmp/checkpoint/checkpoints to /checkpoints_s3/checkpoints
[36m(task, pid=2574)[0m Checkpoints copied successfully
[36m(task, pid=2574)[0m Completed copying to S3 directory 1/2
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m Copying to S3 directory 2/2: /checkpoints_s3_mount_cached
[36m(task, pid=2574)[0m Copying dataset cache from /tmp/checkpoint/dataset_cache to /checkpoints_s3_mount_cached/dataset_cache
[36m(task, pid=2574)[0m Dataset cache copied successfully
[36m(task, pid=2574)[0m Copying model cache from /tmp/checkpoint/model_cache to /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Model cache copied successfully
[36m(task, pid=2574)[0m Copying checkpoints from /tmp/checkpoint/checkpoints to /checkpoints_s3_mount_cached/checkpoints
[36m(task, pid=2574)[0m Checkpoints copied successfully
[36m(task, pid=2574)[0m Completed copying to S3 directory 2/2
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Download and caching completed ===
[36m(task, pid=2574)[0m ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
[36m(task, pid=2574)[0m [2025-07-31 22:04:19,222] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m df: /home/sky/.triton/autotune: No such file or directory
[36m(task, pid=2574)[0m [2025-07-31 22:04:21,631] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:04:31,377] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:04:31,828] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:04:32,567] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:04:32,739] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:04:32,749] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:04:32,780] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:04:32,786] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:04:32,828] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:04:33,247] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:04:33,266] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:04:33,322] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:04:33,341] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,104] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,123] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,256] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,275] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,276] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,295] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,333] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,343] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,352] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,352] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,362] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,378] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:04:34,396] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /tmp/checkpoint/dataset_cache
[36m(task, pid=2574)[0m Model cache: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /tmp/checkpoint/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /tmp/checkpoint/dataset_cache
[36m(task, pid=2574)[0m Model cache: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /tmp/checkpoint/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /tmp/checkpoint/dataset_cache
[36m(task, pid=2574)[0m Model cache: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /tmp/checkpoint/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /tmp/checkpoint/dataset_cache
[36m(task, pid=2574)[0m Model cache: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /tmp/checkpoint/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /tmp/checkpoint/dataset_cache
[36m(task, pid=2574)[0m Model cache: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /tmp/checkpoint/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /tmp/checkpoint/dataset_cache
[36m(task, pid=2574)[0m Model cache: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /tmp/checkpoint/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /tmp/checkpoint/dataset_cache
[36m(task, pid=2574)[0m Model cache: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /tmp/checkpoint/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /tmp/checkpoint/dataset_cache
[36m(task, pid=2574)[0m Model cache: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /tmp/checkpoint/checkpoints
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/tmp/checkpoint/checkpoints'Removing checkpoint exception [Errno 2] No such file or directory: '/tmp/checkpoint/checkpoints'Removing checkpoint exception [Errno 2] No such file or directory: '/tmp/checkpoint/checkpoints'
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/tmp/checkpoint/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/tmp/checkpoint/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Completed Load dataset in 2.03 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.04 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.05 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.05 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.07 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m [2025-07-31 22:04:47,240] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:04:47,255] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:04:47,257] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:04:47,266] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:04:47,280] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m Completed Load dataset in 2.19 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m [2025-07-31 22:04:47,407] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m Completed Load dataset in 2.31 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m [2025-07-31 22:04:47,528] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m Completed Load dataset in 2.44 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /tmp/checkpoint/model_cache
[36m(task, pid=2574)[0m [2025-07-31 22:04:47,651] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:04:48,351] [INFO] [partition_parameters.py:366:__exit__] finished initializing model - num_params = 1066, num_elems = 13.19B
[36m(task, pid=2574)[0m 
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.76it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.80it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.82it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.75it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.74it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.77it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.80it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:05,  1.31s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.15it/s]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.15it/s]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.15it/s]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.15it/s]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.15it/s]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.15it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.15it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  1.12s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.08it/s]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.08it/s]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.08it/s]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.08it/s]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.08it/s]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.08it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.08it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  1.05s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.07it/s]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.07it/s]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.07it/s]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.07it/s]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.07it/s]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.07it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.07it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  1.02s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.07it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.07it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.07it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.07it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.12it/s]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.12it/s]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.12it/s]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.07it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.12it/s]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.07it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.07it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.12it/s]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.12it/s]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.12it/s]
[36m(task, pid=2574)[0m Completed Load model in 5.81 seconds
[36m(task, pid=2574)[0m Completed Load model in 5.84 seconds
[36m(task, pid=2574)[0m Completed Load model in 5.82 seconds
[36m(task, pid=2574)[0m Completed Load model in 5.83 seconds
[36m(task, pid=2574)[0m Completed Load model in 5.57 seconds
[36m(task, pid=2574)[0m Completed Load model in 5.68 seconds
[36m(task, pid=2574)[0m Completed Load model in 5.85 seconds
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.01it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.03s/it]
[36m(task, pid=2574)[0m Completed Load model in 6.15 seconds
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   0%|          | 0/47780 [00:00<?, ? examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   0%|          | 6/47780 [00:03<7:18:55,  1.81 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   0%|          | 16/47780 [00:03<2:28:45,  5.35 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   0%|          | 41/47780 [00:03<50:08, 15.87 examples/s]  
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   0%|          | 80/47780 [00:04<23:46, 33.44 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   0%|          | 164/47780 [00:04<08:49, 89.88 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   0%|          | 201/47780 [00:04<07:07, 111.22 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   0%|          | 235/47780 [00:04<07:33, 104.95 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   1%|          | 325/47780 [00:05<05:22, 147.07 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   1%|          | 433/47780 [00:05<04:17, 183.77 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   1%|          | 541/47780 [00:06<03:38, 216.07 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   1%|â–         | 676/47780 [00:06<03:02, 258.77 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   2%|â–         | 839/47780 [00:06<02:34, 303.14 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   2%|â–         | 1038/47780 [00:07<02:08, 363.16 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   3%|â–Ž         | 1203/47780 [00:07<02:01, 383.86 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   3%|â–Ž         | 1371/47780 [00:08<01:58, 391.83 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   3%|â–Ž         | 1597/47780 [00:08<01:42, 449.00 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   4%|â–         | 1807/47780 [00:08<01:38, 467.24 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   4%|â–         | 2070/47780 [00:09<01:28, 518.13 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   5%|â–         | 2333/47780 [00:09<01:22, 551.14 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   5%|â–Œ         | 2617/47780 [00:10<01:17, 584.04 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   6%|â–Œ         | 2908/47780 [00:10<00:56, 797.39 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   6%|â–‹         | 3034/47780 [00:10<01:09, 644.23 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   7%|â–‹         | 3229/47780 [00:11<01:19, 561.39 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   7%|â–‹         | 3563/47780 [00:11<01:09, 632.32 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   8%|â–Š         | 3910/47780 [00:11<01:04, 679.83 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):   9%|â–‰         | 4254/47780 [00:12<01:00, 720.37 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  10%|â–‰         | 4591/47780 [00:12<00:58, 744.53 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  10%|â–ˆ         | 4933/47780 [00:13<00:57, 747.54 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  11%|â–ˆ         | 5245/47780 [00:13<00:57, 745.06 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  12%|â–ˆâ–        | 5568/47780 [00:14<00:55, 757.95 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  12%|â–ˆâ–        | 5930/47780 [00:14<00:53, 788.93 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  13%|â–ˆâ–Ž        | 6283/47780 [00:14<00:51, 806.21 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  14%|â–ˆâ–        | 6616/47780 [00:15<00:51, 796.19 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  15%|â–ˆâ–        | 6983/47780 [00:15<00:49, 824.35 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  15%|â–ˆâ–Œ        | 7344/47780 [00:16<00:48, 838.02 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  16%|â–ˆâ–Œ        | 7705/47780 [00:16<00:47, 847.38 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  17%|â–ˆâ–‹        | 8061/47780 [00:17<00:46, 855.07 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  17%|â–ˆâ–‹        | 8361/47780 [00:17<00:48, 820.93 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  18%|â–ˆâ–Š        | 8665/47780 [00:17<00:48, 805.12 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  19%|â–ˆâ–‰        | 8991/47780 [00:18<00:48, 805.54 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  19%|â–ˆâ–‰        | 9299/47780 [00:18<00:48, 792.32 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  20%|â–ˆâ–ˆ        | 9623/47780 [00:19<00:48, 793.30 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  21%|â–ˆâ–ˆ        | 9950/47780 [00:19<00:47, 798.43 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  21%|â–ˆâ–ˆâ–       | 10238/47780 [00:19<00:49, 753.31 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  22%|â–ˆâ–ˆâ–       | 10618/47780 [00:20<00:46, 805.62 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  23%|â–ˆâ–ˆâ–Ž       | 10971/47780 [00:20<00:45, 815.65 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  24%|â–ˆâ–ˆâ–       | 11364/47780 [00:21<00:42, 848.67 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  25%|â–ˆâ–ˆâ–       | 11724/47780 [00:21<00:43, 837.60 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  25%|â–ˆâ–ˆâ–Œ       | 12114/47780 [00:22<00:41, 853.77 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  26%|â–ˆâ–ˆâ–Œ       | 12518/47780 [00:22<00:40, 864.90 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  27%|â–ˆâ–ˆâ–‹       | 12921/47780 [00:22<00:39, 880.40 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  28%|â–ˆâ–ˆâ–Š       | 13368/47780 [00:23<00:37, 916.09 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  29%|â–ˆâ–ˆâ–‰       | 13772/47780 [00:23<00:37, 905.96 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  30%|â–ˆâ–ˆâ–‰       | 14232/47780 [00:24<00:35, 938.57 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  31%|â–ˆâ–ˆâ–ˆ       | 14657/47780 [00:24<00:35, 941.37 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  32%|â–ˆâ–ˆâ–ˆâ–      | 15152/47780 [00:25<00:32, 991.78 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 15623/47780 [00:25<00:32, 1000.88 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  34%|â–ˆâ–ˆâ–ˆâ–      | 16153/47780 [00:26<00:30, 1050.02 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  35%|â–ˆâ–ˆâ–ˆâ–      | 16590/47780 [00:26<00:30, 1020.53 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17107/47780 [00:27<00:29, 1043.04 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 17552/47780 [00:27<00:30, 1005.68 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 18011/47780 [00:27<00:30, 989.72 examples/s] 
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 18458/47780 [00:28<00:29, 981.14 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 18891/47780 [00:28<00:29, 966.48 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 19281/47780 [00:29<00:30, 922.55 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19740/47780 [00:29<00:29, 939.91 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20116/47780 [00:30<00:30, 900.51 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 20508/47780 [00:30<00:30, 883.52 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20929/47780 [00:31<00:30, 881.86 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21315/47780 [00:31<00:31, 847.50 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 21697/47780 [00:32<00:32, 802.04 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 22083/47780 [00:32<00:32, 793.12 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 22449/47780 [00:33<00:32, 783.41 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 22782/47780 [00:33<00:32, 764.09 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23133/47780 [00:34<00:32, 758.81 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 23472/47780 [00:34<00:32, 745.33 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 23792/47780 [00:35<00:32, 728.99 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 24089/47780 [00:35<00:34, 688.59 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 24448/47780 [00:36<00:32, 720.89 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 24766/47780 [00:36<00:32, 718.14 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 25097/47780 [00:37<00:31, 725.49 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 25443/47780 [00:37<00:30, 722.09 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25817/47780 [00:37<00:29, 739.50 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26186/47780 [00:38<00:28, 759.77 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 26501/47780 [00:38<00:29, 729.17 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 26829/47780 [00:39<00:28, 744.88 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27131/47780 [00:39<00:28, 737.39 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27423/47780 [00:40<00:27, 730.50 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 27766/47780 [00:40<00:26, 759.55 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 28057/47780 [00:41<00:27, 727.60 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 28422/47780 [00:41<00:25, 765.56 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 28755/47780 [00:41<00:24, 763.66 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 29123/47780 [00:42<00:24, 776.57 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 29496/47780 [00:42<00:22, 797.85 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 29828/47780 [00:43<00:22, 785.14 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 30214/47780 [00:43<00:21, 816.83 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 30608/47780 [00:44<00:20, 826.30 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31002/47780 [00:44<00:19, 842.22 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 31415/47780 [00:45<00:18, 869.65 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 31831/47780 [00:45<00:17, 892.53 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 32225/47780 [00:45<00:17, 888.43 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 32589/47780 [00:46<00:17, 859.49 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 32983/47780 [00:46<00:17, 866.26 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 33350/47780 [00:47<00:17, 843.33 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 33776/47780 [00:47<00:16, 872.95 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 34134/47780 [00:48<00:16, 839.63 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 34520/47780 [00:48<00:15, 842.32 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 34880/47780 [00:49<00:15, 829.53 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35244/47780 [00:49<00:15, 821.62 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35609/47780 [00:50<00:14, 811.45 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 36003/47780 [00:50<00:14, 813.96 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 36421/47780 [00:50<00:13, 840.14 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 36861/47780 [00:51<00:12, 873.14 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 37246/47780 [00:51<00:12, 856.15 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 37663/47780 [00:52<00:11, 863.33 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 38085/47780 [00:52<00:10, 924.21 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 38449/47780 [00:53<00:10, 877.65 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 38877/47780 [00:53<00:10, 881.09 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 39363/47780 [00:54<00:09, 923.29 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 39776/47780 [00:54<00:09, 872.45 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40219/47780 [00:55<00:08, 884.34 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 40649/47780 [00:55<00:07, 897.01 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 41080/47780 [00:56<00:07, 896.48 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 41457/47780 [00:56<00:07, 862.20 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 41815/47780 [00:57<00:06, 868.52 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 42148/47780 [00:57<00:06, 869.51 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 42469/47780 [00:57<00:06, 866.04 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 42828/47780 [00:57<00:04, 1117.81 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 42995/47780 [00:58<00:04, 1058.59 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 43140/47780 [00:58<00:04, 1007.62 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 43266/47780 [00:58<00:04, 927.34 examples/s] 
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 43373/47780 [00:58<00:05, 844.10 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 43466/47780 [00:58<00:05, 792.60 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 43552/47780 [00:58<00:05, 767.67 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 43632/47780 [00:59<00:05, 766.50 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 43711/47780 [00:59<00:05, 762.30 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 43788/47780 [00:59<00:05, 703.66 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 43861/47780 [00:59<00:05, 701.11 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 43948/47780 [00:59<00:05, 729.95 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44024/47780 [00:59<00:05, 734.79 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44102/47780 [00:59<00:04, 746.93 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 44198/47780 [00:59<00:04, 803.22 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 44281/47780 [00:59<00:04, 792.29 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 44362/47780 [00:59<00:04, 764.01 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 44449/47780 [01:00<00:04, 782.21 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 44535/47780 [01:00<00:04, 776.79 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 44626/47780 [01:00<00:03, 808.99 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 44709/47780 [01:00<00:04, 751.05 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 44786/47780 [01:00<00:04, 709.34 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44866/47780 [01:00<00:04, 722.04 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44942/47780 [01:00<00:03, 727.00 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45017/47780 [01:00<00:04, 684.76 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45087/47780 [01:01<00:04, 649.31 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45157/47780 [01:01<00:04, 652.99 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45224/47780 [01:01<00:04, 634.07 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45289/47780 [01:01<00:04, 597.21 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45369/47780 [01:01<00:03, 645.60 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 45440/47780 [01:01<00:03, 635.51 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 45507/47780 [01:01<00:03, 631.53 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 45574/47780 [01:01<00:03, 582.94 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 45635/47780 [01:01<00:03, 589.33 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 45701/47780 [01:02<00:03, 602.89 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 45763/47780 [01:02<00:03, 584.74 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 45828/47780 [01:02<00:03, 599.17 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 45892/47780 [01:02<00:03, 580.64 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 45957/47780 [01:02<00:03, 595.49 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46018/47780 [01:02<00:03, 540.17 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46080/47780 [01:02<00:03, 549.95 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46136/47780 [01:02<00:03, 518.42 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46190/47780 [01:02<00:03, 518.06 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46245/47780 [01:03<00:03, 509.82 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46297/47780 [01:03<00:03, 486.83 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46354/47780 [01:03<00:02, 500.45 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46411/47780 [01:03<00:02, 513.00 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46481/47780 [01:03<00:02, 543.49 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 46537/47780 [01:03<00:02, 516.62 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 46592/47780 [01:03<00:02, 506.50 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 46644/47780 [01:03<00:02, 424.52 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 46690/47780 [01:04<00:02, 415.59 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 46733/47780 [01:04<00:02, 409.97 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 46777/47780 [01:04<00:02, 409.54 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 46828/47780 [01:04<00:02, 427.55 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 46874/47780 [01:04<00:02, 394.03 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 46919/47780 [01:04<00:02, 402.80 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 46963/47780 [01:04<00:01, 411.53 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47006/47780 [01:04<00:02, 366.68 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47045/47780 [01:04<00:02, 340.44 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47082/47780 [01:05<00:02, 343.77 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47121/47780 [01:05<00:01, 353.24 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47162/47780 [01:05<00:01, 346.08 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47199/47780 [01:05<00:01, 315.52 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47234/47780 [01:05<00:01, 298.68 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47266/47780 [01:05<00:01, 269.51 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47295/47780 [01:05<00:01, 269.84 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47323/47780 [01:05<00:01, 258.18 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47351/47780 [01:06<00:01, 226.81 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47376/47780 [01:06<00:01, 225.28 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47401/47780 [01:06<00:01, 214.68 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47423/47780 [01:06<00:01, 202.71 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47445/47780 [01:06<00:01, 190.43 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47467/47780 [01:06<00:01, 191.30 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47492/47780 [01:06<00:01, 188.21 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47512/47780 [01:07<00:01, 179.50 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47531/47780 [01:07<00:01, 179.94 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47551/47780 [01:07<00:01, 164.26 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47573/47780 [01:07<00:01, 173.91 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47591/47780 [01:07<00:01, 170.09 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47609/47780 [01:07<00:01, 159.29 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47627/47780 [01:07<00:01, 146.67 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47644/47780 [01:07<00:00, 146.48 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47660/47780 [01:08<00:00, 137.94 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47674/47780 [01:08<00:00, 123.56 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47688/47780 [01:08<00:00, 105.77 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47700/47780 [01:08<00:00, 102.56 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47712/47780 [01:08<00:00, 104.81 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47724/47780 [01:08<00:00, 103.35 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47736/47780 [01:08<00:00, 107.48 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47748/47780 [01:08<00:00, 106.41 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47760/47780 [01:09<00:00, 101.18 examples/s]
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 47771/47780 [01:09<00:00, 72.14 examples/s] 
[36m(task, pid=2574)[0m Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47780/47780 [01:10<00:00, 21.93 examples/s]
Tokenizing train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47780/47780 [01:11<00:00, 667.47 examples/s]
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m Truncating train dataset (num_proc=128):   0%|          | 0/47780 [00:00<?, ? examples/s]
[36m(task, pid=2574)[0m Truncating train dataset (num_proc=128):   1%|          | 374/47780 [00:01<03:25, 231.05 examples/s]
[36m(task, pid=2574)[0m Truncating train dataset (num_proc=128):  12%|â–ˆâ–        | 5610/47780 [00:01<00:09, 4448.25 examples/s]
[36m(task, pid=2574)[0m Truncating train dataset (num_proc=128):  29%|â–ˆâ–ˆâ–‰       | 13837/47780 [00:01<00:02, 12491.03 examples/s]
[36m(task, pid=2574)[0m Truncating train dataset (num_proc=128):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 23908/47780 [00:01<00:01, 23839.10 examples/s]
[36m(task, pid=2574)[0m Truncating train dataset (num_proc=128):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 33606/47780 [00:02<00:00, 34984.52 examples/s]
[36m(task, pid=2574)[0m Truncating train dataset (num_proc=128):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45169/47780 [00:02<00:00, 46602.71 examples/s]
Truncating train dataset (num_proc=128): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47780/47780 [00:06<00:00, 6988.80 examples/s] 
[36m(task, pid=2574)[0m Parameter Offload - Persistent parameters statistics: param_count = 563, numel = 1166448
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...Starting Get batch samples...
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Completed Get batch samples in 0.07 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.07s (Total: 0.07s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.07 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.07s (Total: 0.07s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.08 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.08s (Total: 0.08s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.08 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.08s (Total: 0.08s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.08 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.08s (Total: 0.08s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.08 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.08s (Total: 0.08s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.10 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.10s (Total: 0.10s)
[36m(task, pid=2574)[0m 
  0%|          | 0/5 [00:00<?, ?it/s]Starting Get batch samples...
[36m(task, pid=2574)[0m Completed Get batch samples in 0.13 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.13s (Total: 0.13s)
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m Completed Training in 98.86 seconds
[36m(task, pid=2574)[0m [rank1]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank1]:     main()
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank1]:     trainer.train()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank1]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank1]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank1]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank1]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank1]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank1]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank1]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank1]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank1]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank1]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank1]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank1]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank1]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank1]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank1]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank1]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank1]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank1]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank1]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank1]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank1]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 1 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2166046 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 98.68 seconds
[36m(task, pid=2574)[0m Completed Training in 98.73 seconds
[36m(task, pid=2574)[0m [rank0]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank0]:     main()
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank0]:     trainer.train()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank0]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank0]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank0]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank0]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank0]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank0]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank0]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank0]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank0]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank0]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank0]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank0]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank0]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank0]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank0]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank0]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank0]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank0]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank0]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank0]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank0]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2166045 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank4]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank4]:     main()
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank4]:     trainer.train()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank4]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank4]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank4]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank4]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank4]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank4]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank4]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank4]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank4]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank4]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank4]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank4]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank4]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank4]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank4]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank4]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank4]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank4]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank4]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank4]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank4]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank4]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 4 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2166049 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 98.79 seconds
[36m(task, pid=2574)[0m Completed Training in 98.82 seconds
[36m(task, pid=2574)[0m [rank2]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank2]:     main()
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank2]:     trainer.train()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank2]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank2]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank2]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank2]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank2]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank2]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank2]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank2]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank2]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank2]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank2]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank2]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank2]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank2]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank2]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank2]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank2]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank2]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank2]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank2]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank2]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank2]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 2 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2166047 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank7]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank7]:     main()
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank7]:     trainer.train()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank7]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank7]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank7]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank7]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank7]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank7]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank7]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank7]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank7]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank7]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank7]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank7]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank7]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank7]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank7]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank7]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank7]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank7]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank7]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank7]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank7]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank7]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 7 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2166052 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 98.89 seconds
[36m(task, pid=2574)[0m Completed Training in 98.82 seconds
[36m(task, pid=2574)[0m Completed Training in 98.88 seconds
[36m(task, pid=2574)[0m [rank5]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank5]:     main()
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank5]:     trainer.train()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank5]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank5]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank5]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank5]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank5]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank5]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank5]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank5]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank5]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank5]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank5]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank5]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank5]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank5]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank5]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank5]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank5]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank5]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank5]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank5]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank5]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank5]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank5]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 5 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2166050 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank3]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank3]:     main()
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank3]:     trainer.train()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank3]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank3]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank3]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank3]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank3]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank3]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank3]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank3]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank3]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank3]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank3]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank3]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank3]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank3]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank3]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank3]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank3]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank3]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank3]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank3]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank3]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank3]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 3 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2166048 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank6]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank6]:     main()
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank6]:     trainer.train()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank6]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank6]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank6]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank6]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank6]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank6]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank6]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank6]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank6]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank6]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank6]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank6]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank6]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank6]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank6]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank6]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank6]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank6]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank6]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank6]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank6]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank6]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 6 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2166051 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m 
  0%|          | 0/5 [00:03<?, ?it/s]
[36m(task, pid=2574)[0m W0731 22:06:35.187000 198489 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 203794 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:06:35.187000 198489 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 203795 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:06:35.188000 198489 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 203796 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:06:35.188000 198489 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 203797 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:06:35.188000 198489 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 203798 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:06:35.188000 198489 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 203800 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:06:35.188000 198489 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 203801 closing signal SIGTERM
[36m(task, pid=2574)[0m E0731 22:06:36.775000 198489 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 5 (pid: 203799) of binary: /home/sky/training/bin/python3
[36m(task, pid=2574)[0m Traceback (most recent call last):
[36m(task, pid=2574)[0m   File "/home/sky/training/bin/accelerate", line 10, in <module>
[36m(task, pid=2574)[0m     sys.exit(main())
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
[36m(task, pid=2574)[0m     args.func(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1184, in launch_command
[36m(task, pid=2574)[0m     deepspeed_launcher(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/launch.py", line 868, in deepspeed_launcher
[36m(task, pid=2574)[0m     distrib_run.run(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
[36m(task, pid=2574)[0m     elastic_launch(
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
[36m(task, pid=2574)[0m     return launch_agent(self._config, self._entrypoint, list(args))
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
[36m(task, pid=2574)[0m     raise ChildFailedError(
[36m(task, pid=2574)[0m torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
[36m(task, pid=2574)[0m ============================================================
[36m(task, pid=2574)[0m /e2e/train.py FAILED
[36m(task, pid=2574)[0m ------------------------------------------------------------
[36m(task, pid=2574)[0m Failures:
[36m(task, pid=2574)[0m   <NO_OTHER_FAILURES>
[36m(task, pid=2574)[0m ------------------------------------------------------------
[36m(task, pid=2574)[0m Root Cause (first observed failure):
[36m(task, pid=2574)[0m [0]:
[36m(task, pid=2574)[0m   time      : 2025-07-31_22:06:35
[36m(task, pid=2574)[0m   host      : cc-d87e1263-head
[36m(task, pid=2574)[0m   rank      : 5 (local_rank: 5)
[36m(task, pid=2574)[0m   exitcode  : 1 (pid: 203799)
[36m(task, pid=2574)[0m   error_file: <N/A>
[36m(task, pid=2574)[0m   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[36m(task, pid=2574)[0m ============================================================
[36m(task, pid=2574)[0m ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
[36m(task, pid=2574)[0m [2025-07-31 22:06:45,902] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:06:47,012] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:06:57,308] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:06:57,425] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:06:57,514] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:06:58,377] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:06:58,643] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:06:58,717] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:06:58,739] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:06:58,754] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:06:59,372] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:06:59,390] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:06:59,436] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:06:59,455] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:06:59,633] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:06:59,651] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:06:59,936] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:06:59,957] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:07:00,166] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:07:00,184] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:07:00,260] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:07:00,283] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:07:00,329] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:07:00,332] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:07:00,349] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:07:00,351] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:07:00,351] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /mnt/data/dataset_cache
[36m(task, pid=2574)[0m Model cache: /mnt/data/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /mnt/data/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /mnt/data/dataset_cache
[36m(task, pid=2574)[0m Model cache: /mnt/data/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /mnt/data/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /mnt/data/dataset_cache
[36m(task, pid=2574)[0m Model cache: /mnt/data/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /mnt/data/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /mnt/data/dataset_cache
[36m(task, pid=2574)[0m Model cache: /mnt/data/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /mnt/data/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /mnt/data/dataset_cache
[36m(task, pid=2574)[0m Model cache: /mnt/data/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /mnt/data/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /mnt/data/dataset_cache
[36m(task, pid=2574)[0m Model cache: /mnt/data/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /mnt/data/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /mnt/data/dataset_cache
[36m(task, pid=2574)[0m Model cache: /mnt/data/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /mnt/data/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /mnt/data/dataset_cache
[36m(task, pid=2574)[0m Model cache: /mnt/data/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /mnt/data/checkpoints
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/mnt/data/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/mnt/data/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/mnt/data/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/mnt/data/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/mnt/data/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Completed Load dataset in 2.14 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /mnt/data/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.15 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /mnt/data/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.18 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /mnt/data/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.19 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /mnt/data/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.22 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /mnt/data/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.24 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /mnt/data/model_cache
[36m(task, pid=2574)[0m [2025-07-31 22:07:12,771] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:07:12,771] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:07:12,807] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:07:12,812] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m Completed Load dataset in 2.29 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /mnt/data/model_cache
[36m(task, pid=2574)[0m [2025-07-31 22:07:12,847] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:07:12,857] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:07:12,917] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m Completed Load dataset in 2.54 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /mnt/data/model_cache
[36m(task, pid=2574)[0m [2025-07-31 22:07:13,159] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:07:13,828] [INFO] [partition_parameters.py:366:__exit__] finished initializing model - num_params = 1066, num_elems = 13.19B
[36m(task, pid=2574)[0m 
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.12it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.11it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.11it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.13it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.12it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.11it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.11it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:02<00:11,  2.87s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  1.86s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  1.86s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  1.87s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  1.86s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  1.87s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  1.86s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  1.86s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:05<00:07,  2.50s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:05<00:04,  2.03s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:05<00:04,  2.03s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:05<00:04,  2.03s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:05<00:04,  2.03s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:05<00:04,  2.03s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:05<00:04,  2.03s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:05<00:04,  2.03s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:07<00:04,  2.35s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:07<00:02,  2.09s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:07<00:02,  2.09s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:07<00:02,  2.09s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:07<00:02,  2.09s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:07<00:02,  2.09s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:07<00:02,  2.09s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:07<00:02,  2.09s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:09<00:02,  2.27s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  2.05s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.97s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  2.05s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  2.05s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  2.05s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.96s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  2.05s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  2.05s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.97s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  2.05s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.97s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.97s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.97s/it]
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.97s/it]
[36m(task, pid=2574)[0m Completed Load model in 10.80 seconds
[36m(task, pid=2574)[0m Completed Load model in 11.10 seconds
[36m(task, pid=2574)[0m Completed Load model in 11.19 seconds
[36m(task, pid=2574)[0m Completed Load model in 11.20 seconds
[36m(task, pid=2574)[0m Completed Load model in 11.12 seconds
[36m(task, pid=2574)[0m Completed Load model in 11.06 seconds
[36m(task, pid=2574)[0m Completed Load model in 11.17 seconds
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.11s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.25s/it]
[36m(task, pid=2574)[0m Completed Load model in 12.57 seconds
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Parameter Offload - Persistent parameters statistics: param_count = 563, numel = 1166448
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Completed Get batch samples in 0.16 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.16s (Total: 0.16s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.16 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.16s (Total: 0.16s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.17 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.17s (Total: 0.17s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.18 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.18s (Total: 0.18s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.18 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.18s (Total: 0.18s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.19 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.19s (Total: 0.19s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.20 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.20s (Total: 0.20s)
[36m(task, pid=2574)[0m 
  0%|          | 0/5 [00:00<?, ?it/s]Starting Get batch samples...
[36m(task, pid=2574)[0m Completed Get batch samples in 0.13 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.13s (Total: 0.13s)
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m Completed Training in 21.18 seconds
[36m(task, pid=2574)[0m Completed Training in 21.06 seconds
[36m(task, pid=2574)[0m [rank5]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank5]:     main()
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank5]:     trainer.train()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank5]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank5]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank5]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank5]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank5]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank5]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank5]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank5]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank5]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank5]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank5]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank5]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank5]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank5]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank5]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank5]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank5]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank5]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank5]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank5]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank5]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank5]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank5]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 5 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2295695 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 21.05 seconds
[36m(task, pid=2574)[0m [rank4]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank4]:     main()
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank4]:     trainer.train()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank4]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank4]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank4]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank4]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank4]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank4]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank4]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank4]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank4]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank4]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank4]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank4]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank4]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank4]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank4]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank4]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank4]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank4]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank4]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank4]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank4]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank4]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 4 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2295693 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 20.24 seconds
[36m(task, pid=2574)[0m Completed Training in 21.04 seconds
[36m(task, pid=2574)[0m Completed Training in 21.07 seconds
[36m(task, pid=2574)[0m Completed Training in 21.13 seconds
[36m(task, pid=2574)[0m [rank2]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank2]:     main()
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank2]:     trainer.train()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank2]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank2]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank2]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank2]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank2]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank2]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank2]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank2]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank2]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank2]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank2]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank2]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank2]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank2]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank2]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank2]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank2]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank2]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank2]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank2]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank2]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank2]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 2 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2295689 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank0]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank0]:     main()
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank0]:     trainer.train()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank0]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank0]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank0]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank0]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank0]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank0]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank0]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank0]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank0]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank0]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank0]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank0]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank0]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank0]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank0]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank0]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank0]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank0]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank0]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank0]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank0]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2295684 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 21.06 seconds
[36m(task, pid=2574)[0m [rank1]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank1]:     main()
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank1]:     trainer.train()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank1]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank1]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank1]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank1]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank1]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank1]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank1]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank1]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank1]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank1]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank1]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank1]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank1]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank1]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank1]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank1]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank1]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank1]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank1]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank1]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank1]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 1 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2295687 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank3]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank3]:     main()
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank3]:     trainer.train()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank3]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank3]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank3]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank3]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank3]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank3]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank3]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank3]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank3]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank3]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank3]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank3]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank3]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank3]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank3]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank3]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank3]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank3]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank3]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank3]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank3]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank3]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 3 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2295691 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank7]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank7]:     main()
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank7]:     trainer.train()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank7]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank7]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank7]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank7]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank7]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank7]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank7]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank7]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank7]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank7]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank7]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank7]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank7]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank7]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank7]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank7]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank7]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank7]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank7]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank7]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank7]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank7]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 7 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2295699 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank6]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank6]:     main()
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank6]:     trainer.train()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank6]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank6]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank6]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank6]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank6]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank6]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank6]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank6]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank6]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank6]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank6]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank6]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank6]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank6]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank6]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank6]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank6]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank6]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank6]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank6]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank6]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank6]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 6 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2295697 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m 
  0%|          | 0/5 [00:02<?, ?it/s]
[36m(task, pid=2574)[0m W0731 22:07:48.202000 329161 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 332378 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:07:48.203000 329161 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 332381 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:07:48.203000 329161 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 332385 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:07:48.203000 329161 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 332387 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:07:48.203000 329161 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 332389 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:07:48.203000 329161 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 332391 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:07:48.204000 329161 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 332393 closing signal SIGTERM
[36m(task, pid=2574)[0m E0731 22:07:49.791000 329161 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 2 (pid: 332383) of binary: /home/sky/training/bin/python3
[36m(task, pid=2574)[0m Traceback (most recent call last):
[36m(task, pid=2574)[0m   File "/home/sky/training/bin/accelerate", line 10, in <module>
[36m(task, pid=2574)[0m     sys.exit(main())
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
[36m(task, pid=2574)[0m     args.func(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1184, in launch_command
[36m(task, pid=2574)[0m     deepspeed_launcher(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/launch.py", line 868, in deepspeed_launcher
[36m(task, pid=2574)[0m     distrib_run.run(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
[36m(task, pid=2574)[0m     elastic_launch(
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
[36m(task, pid=2574)[0m     return launch_agent(self._config, self._entrypoint, list(args))
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
[36m(task, pid=2574)[0m     raise ChildFailedError(
[36m(task, pid=2574)[0m torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
[36m(task, pid=2574)[0m ============================================================
[36m(task, pid=2574)[0m /e2e/train.py FAILED
[36m(task, pid=2574)[0m ------------------------------------------------------------
[36m(task, pid=2574)[0m Failures:
[36m(task, pid=2574)[0m   <NO_OTHER_FAILURES>
[36m(task, pid=2574)[0m ------------------------------------------------------------
[36m(task, pid=2574)[0m Root Cause (first observed failure):
[36m(task, pid=2574)[0m [0]:
[36m(task, pid=2574)[0m   time      : 2025-07-31_22:07:48
[36m(task, pid=2574)[0m   host      : cc-d87e1263-head
[36m(task, pid=2574)[0m   rank      : 2 (local_rank: 2)
[36m(task, pid=2574)[0m   exitcode  : 1 (pid: 332383)
[36m(task, pid=2574)[0m   error_file: <N/A>
[36m(task, pid=2574)[0m   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[36m(task, pid=2574)[0m ============================================================
[36m(task, pid=2574)[0m ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
[36m(task, pid=2574)[0m [2025-07-31 22:07:58,817] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:08:00,103] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:08:09,645] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:08:10,138] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:08:11,356] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:08:11,374] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:08:11,493] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:08:11,755] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:08:11,773] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:08:11,773] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(task, pid=2574)[0m [2025-07-31 22:08:12,626] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:08:12,642] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:08:12,692] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:08:12,898] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:08:12,907] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:08:13,298] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:08:13,314] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:08:14,134] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:08:14,152] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:08:14,227] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:08:14,246] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:08:14,352] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:08:14,370] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:08:14,396] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:08:14,415] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:08:14,495] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:08:14,514] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3/checkpoints
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/checkpoints_s3/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/checkpoints_s3/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/checkpoints_s3/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/checkpoints_s3/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/checkpoints_s3/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Completed Load dataset in 2.64 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.65 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.69 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.67 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.62 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.65 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.65 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.65 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3/model_cache
[36m(task, pid=2574)[0m [2025-07-31 22:08:27,221] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:08:27,227] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:08:27,231] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:08:27,233] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:08:27,245] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:08:27,250] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:08:27,255] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:08:27,278] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:08:28,030] [INFO] [partition_parameters.py:366:__exit__] finished initializing model - num_params = 1066, num_elems = 13.19B
[36m(task, pid=2574)[0m 
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:36, 39.11s/it]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:36, 39.11s/it]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:36, 39.11s/it]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:36, 39.11s/it]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:36, 39.11s/it]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:36, 39.11s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:36, 39.09s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [01:26<05:47, 86.79s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:55<03:02, 60.83s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:55<03:02, 60.83s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:55<03:02, 60.83s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:55<03:02, 60.83s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:55<03:02, 60.83s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:55<03:02, 60.82s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:55<03:02, 60.83s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [03:10<04:50, 96.85s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [03:38<02:40, 80.22s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [03:38<02:40, 80.22s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [03:38<02:40, 80.22s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [03:38<02:40, 80.22s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [03:38<02:40, 80.22s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [03:38<02:40, 80.22s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [03:38<02:40, 80.22s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [04:52<03:18, 99.23s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [05:20<01:28, 88.88s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [05:20<01:28, 88.88s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [05:20<01:28, 88.88s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [05:20<01:28, 88.88s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [05:20<01:28, 88.88s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [05:20<01:28, 88.87s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [05:20<01:28, 88.88s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [06:39<01:42, 102.12s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 92.95s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 84.15s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 92.95s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 92.95s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 92.95s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 84.15s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 92.95s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 92.95s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 84.15s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 92.95s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 84.15s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 84.15s/it]
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 84.15s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00, 84.15s/it]
[36m(task, pid=2574)[0m Completed Load model in 422.05 seconds
[36m(task, pid=2574)[0m Completed Load model in 422.05 seconds
[36m(task, pid=2574)[0m Completed Load model in 422.07 seconds
[36m(task, pid=2574)[0m Completed Load model in 422.07 seconds
[36m(task, pid=2574)[0m Completed Load model in 422.08 seconds
[36m(task, pid=2574)[0m Completed Load model in 422.08 seconds
[36m(task, pid=2574)[0m Completed Load model in 422.08 seconds
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [08:20<00:00, 101.70s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [08:20<00:00, 100.05s/it]
[36m(task, pid=2574)[0m Completed Load model in 501.59 seconds
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Parameter Offload - Persistent parameters statistics: param_count = 563, numel = 1166448
[36m(task, pid=2574)[0m Starting Get batch samples...Starting Get batch samples...
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m 
  0%|          | 0/5 [00:00<?, ?it/s]Starting Get batch samples...
[36m(task, pid=2574)[0m Completed Get batch samples in 1.98 seconds
[36m(task, pid=2574)[0m Batch sample time: 1.98s (Total: 1.98s)
[36m(task, pid=2574)[0m Completed Get batch samples in 2.01 seconds
[36m(task, pid=2574)[0m Batch sample time: 2.01s (Total: 2.01s)
[36m(task, pid=2574)[0m Completed Get batch samples in 2.05 seconds
[36m(task, pid=2574)[0m Batch sample time: 2.05s (Total: 2.05s)
[36m(task, pid=2574)[0m Completed Get batch samples in 2.10 seconds
[36m(task, pid=2574)[0m Batch sample time: 2.10s (Total: 2.10s)
[36m(task, pid=2574)[0m Completed Get batch samples in 2.20 seconds
[36m(task, pid=2574)[0m Batch sample time: 2.20s (Total: 2.20s)
[36m(task, pid=2574)[0m Completed Get batch samples in 2.50 seconds
[36m(task, pid=2574)[0m Batch sample time: 2.50s (Total: 2.50s)
[36m(task, pid=2574)[0m Completed Get batch samples in 2.55 seconds
[36m(task, pid=2574)[0m Batch sample time: 2.55s (Total: 2.55s)
[36m(task, pid=2574)[0m Completed Get batch samples in 2.75 seconds
[36m(task, pid=2574)[0m Batch sample time: 2.75s (Total: 2.75s)
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m Completed Training in 146.28 seconds
[36m(task, pid=2574)[0m Completed Training in 146.28 seconds
[36m(task, pid=2574)[0m [rank6]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank6]:     main()
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank6]:     trainer.train()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank6]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank6]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank6]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank6]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank6]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank6]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank6]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank6]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank6]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank6]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank6]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank6]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank6]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank6]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank6]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank6]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank6]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank6]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank6]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank6]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank6]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank6]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 6 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2354460 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 146.29 seconds
[36m(task, pid=2574)[0m [rank3]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank3]:     main()
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank3]:     trainer.train()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank3]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank3]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank3]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank3]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank3]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank3]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank3]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank3]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank3]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank3]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank3]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank3]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank3]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank3]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank3]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank3]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank3]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank3]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank3]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank3]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank3]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank3]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 3 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2354457 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 146.37 seconds
[36m(task, pid=2574)[0m [rank2]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank2]:     main()
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank2]:     trainer.train()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank2]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank2]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank2]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank2]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank2]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank2]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank2]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank2]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank2]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank2]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank2]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank2]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank2]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank2]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank2]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank2]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank2]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank2]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank2]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank2]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank2]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank2]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 2 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2354456 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 67.55 seconds
[36m(task, pid=2574)[0m [rank5]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank5]:     main()
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank5]:     trainer.train()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank5]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank5]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank5]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank5]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank5]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank5]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank5]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank5]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank5]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank5]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank5]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank5]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank5]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank5]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank5]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank5]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank5]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank5]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank5]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank5]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank5]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank5]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank5]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 5 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2354459 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 146.41 seconds
[36m(task, pid=2574)[0m [rank0]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank0]:     main()
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank0]:     trainer.train()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank0]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank0]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank0]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank0]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank0]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank0]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank0]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank0]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank0]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank0]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank0]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank0]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank0]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank0]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank0]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank0]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank0]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank0]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank0]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank0]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank0]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2354454 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank7]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank7]:     main()
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank7]:     trainer.train()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank7]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank7]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank7]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank7]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank7]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank7]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank7]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank7]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank7]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank7]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank7]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank7]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank7]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank7]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank7]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank7]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank7]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank7]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank7]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank7]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank7]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank7]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 7 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2354461 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 146.39 seconds
[36m(task, pid=2574)[0m Completed Training in 146.28 seconds
[36m(task, pid=2574)[0m [rank1]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank1]:     main()
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank1]:     trainer.train()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank1]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank1]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank1]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank1]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank1]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank1]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank1]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank1]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank1]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank1]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank1]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank1]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank1]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank1]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank1]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank1]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank1]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank1]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank1]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank1]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank1]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 1 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2354455 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank4]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank4]:     main()
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank4]:     trainer.train()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank4]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank4]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank4]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank4]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank4]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank4]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank4]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank4]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank4]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank4]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank4]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank4]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank4]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank4]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank4]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank4]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank4]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank4]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank4]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank4]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank4]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank4]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 4 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2354458 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m 
  0%|          | 0/5 [00:04<?, ?it/s]
[36m(task, pid=2574)[0m W0731 22:17:58.779000 385856 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 390605 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:17:58.780000 385856 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 390606 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:17:58.780000 385856 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 390607 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:17:58.780000 385856 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 390608 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:17:58.780000 385856 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 390609 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:17:58.780000 385856 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 390610 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:17:58.781000 385856 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 390611 closing signal SIGTERM
[36m(task, pid=2574)[0m E0731 22:18:00.231000 385856 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 7 (pid: 390612) of binary: /home/sky/training/bin/python3
[36m(task, pid=2574)[0m Traceback (most recent call last):
[36m(task, pid=2574)[0m   File "/home/sky/training/bin/accelerate", line 10, in <module>
[36m(task, pid=2574)[0m     sys.exit(main())
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
[36m(task, pid=2574)[0m     args.func(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1184, in launch_command
[36m(task, pid=2574)[0m     deepspeed_launcher(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/launch.py", line 868, in deepspeed_launcher
[36m(task, pid=2574)[0m     distrib_run.run(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
[36m(task, pid=2574)[0m     elastic_launch(
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
[36m(task, pid=2574)[0m     return launch_agent(self._config, self._entrypoint, list(args))
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
[36m(task, pid=2574)[0m     raise ChildFailedError(
[36m(task, pid=2574)[0m torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
[36m(task, pid=2574)[0m ============================================================
[36m(task, pid=2574)[0m /e2e/train.py FAILED
[36m(task, pid=2574)[0m ------------------------------------------------------------
[36m(task, pid=2574)[0m Failures:
[36m(task, pid=2574)[0m   <NO_OTHER_FAILURES>
[36m(task, pid=2574)[0m ------------------------------------------------------------
[36m(task, pid=2574)[0m Root Cause (first observed failure):
[36m(task, pid=2574)[0m [0]:
[36m(task, pid=2574)[0m   time      : 2025-07-31_22:17:58
[36m(task, pid=2574)[0m   host      : cc-d87e1263-head
[36m(task, pid=2574)[0m   rank      : 7 (local_rank: 7)
[36m(task, pid=2574)[0m   exitcode  : 1 (pid: 390612)
[36m(task, pid=2574)[0m   error_file: <N/A>
[36m(task, pid=2574)[0m   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[36m(task, pid=2574)[0m ============================================================
[36m(task, pid=2574)[0m ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
[36m(task, pid=2574)[0m [2025-07-31 22:18:08,633] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:18:09,798] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:18:21,040] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:18:21,047] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:18:21,197] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:18:21,682] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:18:21,846] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:18:22,061] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:18:22,087] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:18:22,151] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(task, pid=2574)[0m [2025-07-31 22:18:22,743] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:18:22,761] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:18:22,891] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:18:22,909] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,053] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,071] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,360] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,378] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,415] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,435] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,578] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,596] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,596] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,670] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,689] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,697] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[36m(task, pid=2574)[0m [2025-07-31 22:18:23,715] [INFO] [comm.py:821:init_distributed] cdb=None
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3_mount_cached/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3_mount_cached/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3_mount_cached/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3_mount_cached/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3_mount_cached/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3_mount_cached/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3_mount_cached/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3_mount_cached/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3_mount_cached/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3_mount_cached/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3_mount_cached/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3_mount_cached/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3_mount_cached/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3_mount_cached/checkpoints
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m === Starting Run 1/1 ===
[36m(task, pid=2574)[0m Dataset: open-r1/codeforces-cots
[36m(task, pid=2574)[0m Dataset cache: /checkpoints_s3_mount_cached/dataset_cache
[36m(task, pid=2574)[0m Model cache: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Checkpoint saving dir: /checkpoints_s3_mount_cached/checkpoints
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/checkpoints_s3_mount_cached/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/checkpoints_s3_mount_cached/checkpoints'
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/checkpoints_s3_mount_cached/checkpoints'
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/checkpoints_s3_mount_cached/checkpoints'
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/checkpoints_s3_mount_cached/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Removing checkpoint exception [Errno 2] No such file or directory: '/checkpoints_s3_mount_cached/checkpoints'
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Starting Load dataset...
[36m(task, pid=2574)[0m Completed Load dataset in 2.45 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.45 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.46 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.45 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.47 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m Completed Load dataset in 2.47 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m [2025-07-31 22:18:38,271] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:18:38,271] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:18:38,271] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:18:38,271] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:18:38,271] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m Completed Load dataset in 2.70 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m [2025-07-31 22:18:38,450] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m Completed Load dataset in 2.81 seconds
[36m(task, pid=2574)[0m Starting Load model...
[36m(task, pid=2574)[0m Loading model from cache directory: /checkpoints_s3_mount_cached/model_cache
[36m(task, pid=2574)[0m [2025-07-31 22:18:38,553] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:18:40,657] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[36m(task, pid=2574)[0m [2025-07-31 22:18:41,400] [INFO] [partition_parameters.py:366:__exit__] finished initializing model - num_params = 1066, num_elems = 13.19B
[36m(task, pid=2574)[0m 
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
[36m(task, pid=2574)[0m Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:02<00:11,  2.89s/it]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:02<00:11,  2.89s/it]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:02<00:11,  2.89s/it]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:02<00:11,  2.89s/it]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:02<00:11,  2.89s/it]
Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:02<00:11,  2.88s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:02<00:11,  2.88s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:14<00:56, 14.06s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:19<00:32, 10.67s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:19<00:32, 10.67s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:19<00:32, 10.67s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:18<00:31, 10.67s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:18<00:31, 10.66s/it]
Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:18<00:31, 10.66s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:18<00:31, 10.67s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:33<00:51, 17.02s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:38<00:29, 14.78s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:38<00:29, 14.78s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:38<00:29, 14.78s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:38<00:29, 14.77s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:38<00:29, 14.78s/it]
Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:38<00:29, 14.78s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:38<00:29, 14.77s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:52<00:36, 18.09s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:59<00:17, 17.12s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:59<00:17, 17.12s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:59<00:17, 17.12s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:59<00:17, 17.12s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:59<00:17, 17.12s/it]
Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:59<00:17, 17.12s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:59<00:17, 17.12s/it]
[36m(task, pid=2574)[0m Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [01:12<00:18, 18.78s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 17.17s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 15.33s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 17.16s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 17.16s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 17.16s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 17.16s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 15.32s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 15.33s/it]
[36m(task, pid=2574)[0m 
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 17.16s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 15.33s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 15.32s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 15.32s/it]
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 17.16s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:16<00:00, 15.32s/it]
[36m(task, pid=2574)[0m Completed Load model in 80.15 seconds
[36m(task, pid=2574)[0m Completed Load model in 79.79 seconds
[36m(task, pid=2574)[0m Completed Load model in 80.13 seconds
[36m(task, pid=2574)[0m Completed Load model in 80.14 seconds
[36m(task, pid=2574)[0m Completed Load model in 80.15 seconds
[36m(task, pid=2574)[0m Completed Load model in 80.15 seconds
[36m(task, pid=2574)[0m Completed Load model in 79.91 seconds
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m 
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:29<00:00, 18.08s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:29<00:00, 17.84s/it]
[36m(task, pid=2574)[0m Completed Load model in 92.73 seconds
[36m(task, pid=2574)[0m Starting Training...
[36m(task, pid=2574)[0m Parameter Offload - Persistent parameters statistics: param_count = 563, numel = 1166448
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m Starting Get batch samples...
[36m(task, pid=2574)[0m 
  0%|          | 0/5 [00:00<?, ?it/s]Starting Get batch samples...
[36m(task, pid=2574)[0m Completed Get batch samples in 0.46 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.46s (Total: 0.46s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.47 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.47s (Total: 0.47s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.47 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.47s (Total: 0.47s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.54 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.54s (Total: 0.54s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.58 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.58s (Total: 0.58s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.38 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.38s (Total: 0.38s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.62 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.62s (Total: 0.62s)
[36m(task, pid=2574)[0m Completed Get batch samples in 0.62 seconds
[36m(task, pid=2574)[0m Batch sample time: 0.62s (Total: 0.62s)
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[36m(task, pid=2574)[0m Completed Training in 47.65 seconds
[36m(task, pid=2574)[0m [rank7]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank7]:     main()
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank7]:     trainer.train()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank7]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank7]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank7]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank7]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank7]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank7]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank7]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank7]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank7]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank7]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank7]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank7]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank7]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank7]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank7]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank7]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank7]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank7]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank7]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank7]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank7]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank7]:     return inner()
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank7]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank7]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank7]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank7]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank7]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 7 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2966217 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 47.69 seconds
[36m(task, pid=2574)[0m [rank4]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank4]:     main()
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank4]:     trainer.train()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank4]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank4]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank4]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank4]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank4]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank4]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank4]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank4]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank4]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank4]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank4]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank4]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank4]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank4]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank4]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank4]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank4]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank4]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank4]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank4]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank4]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank4]:     return inner()
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank4]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank4]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank4]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank4]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank4]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 4 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2966212 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 47.83 seconds
[36m(task, pid=2574)[0m Completed Training in 47.77 seconds
[36m(task, pid=2574)[0m [rank5]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank5]:     main()
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank5]:     trainer.train()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank5]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank5]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank5]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank5]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank5]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank5]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank5]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank5]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank5]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank5]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank5]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank5]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank5]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank5]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank5]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank5]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank5]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank5]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank5]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank5]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank5]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank5]:     return inner()
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank5]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank5]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank5]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank5]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank5]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank5]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 5 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2966214 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 47.63 seconds
[36m(task, pid=2574)[0m Completed Training in 35.71 seconds
[36m(task, pid=2574)[0m [rank1]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank1]:     main()
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank1]:     trainer.train()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank1]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank1]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank1]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank1]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank1]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank1]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank1]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank1]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank1]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank1]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank1]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank1]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank1]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank1]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank1]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank1]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank1]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank1]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank1]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank1]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank1]:     return inner()
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank1]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank1]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank1]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank1]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank1]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 1 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2966204 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m Completed Training in 47.70 seconds
[36m(task, pid=2574)[0m Completed Training in 47.70 seconds
[36m(task, pid=2574)[0m [rank2]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank2]:     main()
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank2]:     trainer.train()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank2]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank2]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank2]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank2]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank2]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank2]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank2]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank2]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank2]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank2]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank2]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank2]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank2]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank2]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank2]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank2]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank2]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank2]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank2]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank2]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank2]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank2]:     return inner()
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank2]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank2]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank2]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank2]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank2]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 2 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2966207 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank0]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank0]:     main()
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank0]:     trainer.train()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank0]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank0]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank0]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank0]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank0]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank0]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank0]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank0]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank0]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank0]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank0]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank0]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank0]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank0]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank0]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank0]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank0]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank0]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank0]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank0]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank0]:     return inner()
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank0]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank0]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank0]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank0]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank0]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2966202 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank6]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank6]:     main()
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank6]:     trainer.train()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank6]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank6]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank6]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank6]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank6]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank6]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank6]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank6]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank6]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank6]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank6]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank6]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank6]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank6]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank6]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank6]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank6]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank6]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank6]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank6]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank6]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank6]:     return inner()
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank6]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank6]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank6]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank6]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank6]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 6 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2966215 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m [rank3]: Traceback (most recent call last):
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 615, in <module>
[36m(task, pid=2574)[0m [rank3]:     main()
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 536, in main
[36m(task, pid=2574)[0m [rank3]:     trainer.train()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2237, in train
[36m(task, pid=2574)[0m [rank3]:     return inner_training_loop(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[36m(task, pid=2574)[0m [rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[36m(task, pid=2574)[0m [rank3]:   File "/e2e/train.py", line 109, in training_step
[36m(task, pid=2574)[0m [rank3]:     result = super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 914, in training_step
[36m(task, pid=2574)[0m [rank3]:     return super().training_step(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3792, in training_step
[36m(task, pid=2574)[0m [rank3]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 868, in compute_loss
[36m(task, pid=2574)[0m [rank3]:     (loss, outputs) = super().compute_loss(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/trainer.py", line 3879, in compute_loss
[36m(task, pid=2574)[0m [rank3]:     outputs = model(**inputs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[36m(task, pid=2574)[0m [rank3]:     ret_val = func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2105, in forward
[36m(task, pid=2574)[0m [rank3]:     loss = self.module(*inputs, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/liger_kernel/transformers/model/gemma3.py", line 208, in multimodal_forward
[36m(task, pid=2574)[0m [rank3]:     outputs = self.model(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 961, in wrapper
[36m(task, pid=2574)[0m [rank3]:     output = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 936, in forward
[36m(task, pid=2574)[0m [rank3]:     outputs = self.language_model(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/generic.py", line 1069, in wrapper
[36m(task, pid=2574)[0m [rank3]:     outputs = func(self, *args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 559, in forward
[36m(task, pid=2574)[0m [rank3]:     layer_outputs = decoder_layer(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[36m(task, pid=2574)[0m [rank3]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[36m(task, pid=2574)[0m [rank3]:     return disable_fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[36m(task, pid=2574)[0m [rank3]:     return fn(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 495, in checkpoint
[36m(task, pid=2574)[0m [rank3]:     ret = function(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[36m(task, pid=2574)[0m [rank3]:     return func(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 393, in forward
[36m(task, pid=2574)[0m [rank3]:     hidden_states, self_attn_weights = self.self_attn(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[36m(task, pid=2574)[0m [rank3]:     return self._call_impl(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1857, in _call_impl
[36m(task, pid=2574)[0m [rank3]:     return inner()
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1805, in inner
[36m(task, pid=2574)[0m [rank3]:     result = forward_call(*args, **kwargs)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 338, in forward
[36m(task, pid=2574)[0m [rank3]:     attn_output, attn_weights = attention_interface(
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 268, in eager_attention_forward
[36m(task, pid=2574)[0m [rank3]:     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
[36m(task, pid=2574)[0m [rank3]:   File "/home/sky/training/lib/python3.10/site-packages/torch/nn/functional.py", line 2142, in softmax
[36m(task, pid=2574)[0m [rank3]:     ret = input.softmax(dim, dtype=dtype)
[36m(task, pid=2574)[0m [rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 3 has a total capacity of 79.10 GiB of which 10.94 GiB is free. Process 2966210 has 68.15 GiB memory in use. Of the allocated memory 65.27 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(task, pid=2574)[0m 
  0%|          | 0/5 [00:02<?, ?it/s]
[36m(task, pid=2574)[0m W0731 22:20:49.125000 993635 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 998164 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:20:49.126000 993635 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 998166 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:20:49.126000 993635 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 998169 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:20:49.126000 993635 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 998172 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:20:49.126000 993635 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 998174 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:20:49.126000 993635 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 998177 closing signal SIGTERM
[36m(task, pid=2574)[0m W0731 22:20:49.126000 993635 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 998179 closing signal SIGTERM
[36m(task, pid=2574)[0m E0731 22:20:50.778000 993635 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 5 (pid: 998176) of binary: /home/sky/training/bin/python3
[36m(task, pid=2574)[0m Traceback (most recent call last):
[36m(task, pid=2574)[0m   File "/home/sky/training/bin/accelerate", line 10, in <module>
[36m(task, pid=2574)[0m     sys.exit(main())
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
[36m(task, pid=2574)[0m     args.func(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1184, in launch_command
[36m(task, pid=2574)[0m     deepspeed_launcher(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/accelerate/commands/launch.py", line 868, in deepspeed_launcher
[36m(task, pid=2574)[0m     distrib_run.run(args)
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
[36m(task, pid=2574)[0m     elastic_launch(
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
[36m(task, pid=2574)[0m     return launch_agent(self._config, self._entrypoint, list(args))
[36m(task, pid=2574)[0m   File "/home/sky/training/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
[36m(task, pid=2574)[0m     raise ChildFailedError(
[36m(task, pid=2574)[0m torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
[36m(task, pid=2574)[0m ============================================================
[36m(task, pid=2574)[0m /e2e/train.py FAILED
[36m(task, pid=2574)[0m ------------------------------------------------------------
[36m(task, pid=2574)[0m Failures:
[36m(task, pid=2574)[0m   <NO_OTHER_FAILURES>
[36m(task, pid=2574)[0m ------------------------------------------------------------
[36m(task, pid=2574)[0m Root Cause (first observed failure):
[36m(task, pid=2574)[0m [0]:
[36m(task, pid=2574)[0m   time      : 2025-07-31_22:20:49
[36m(task, pid=2574)[0m   host      : cc-d87e1263-head
[36m(task, pid=2574)[0m   rank      : 5 (local_rank: 5)
[36m(task, pid=2574)[0m   exitcode  : 1 (pid: 998176)
[36m(task, pid=2574)[0m   error_file: <N/A>
[36m(task, pid=2574)[0m   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[36m(task, pid=2574)[0m ============================================================
[36m(task, pid=2574)[0m skypilot: cached mount uploaded complete
[0m[32mâœ“ Job finished (status: SUCCEEDED).[0m[0m
