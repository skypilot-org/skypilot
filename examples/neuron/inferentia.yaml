resources:
  cloud: aws
  accelerators: Inferentia:6
  disk_size: 512
  ports: 9000

envs:
  MODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct
  HF_TOKEN: # fill
  NEURON_RT_VISIBLE_CORES: "0-5"
  MASTER_PORT: 12355
  OMP_NUM_THREADS: 6

setup: |
  # Install transformers-neuronx and its dependencies
  sudo apt-get install -y python3.10-venv g++
  python3.10 -m venv aws_neuron_venv_pytorch
  source aws_neuron_venv_pytorch/bin/activate
  pip install ipykernel
  python3.10 -m ipykernel install --user --name aws_neuron_venv_pytorch --display-name "Python (torch-neuronx)"
  pip install jupyter notebook
  pip install environment_kernels
  python -m pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com
  python -m pip install wget
  python -m pip install awscli
  python -m pip install --upgrade neuronx-cc==2.* --pre torch-neuronx==2.1.* torchvision transformers-neuronx

  # Install latest version of triton.
  # Reference: https://github.com/vllm-project/vllm/issues/6987
  pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple triton-nightly

  # Install vLLM from source. Avoid using dir name 'vllm' due to imort conflict.
  # Reference: https://github.com/vllm-project/vllm/issues/1814#issuecomment-1837122930
  git clone https://github.com/vllm-project/vllm.git vllm_repo
  cd vllm_repo
  pip install -U -r requirements-neuron.txt
  VLLM_TARGET_DEVICE="neuron" pip install -e .

  python -c "import huggingface_hub; huggingface_hub.login('${HF_TOKEN}')"

  sudo apt update
  sudo apt install -y numactl

run: |
  source aws_neuron_venv_pytorch/bin/activate
  LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/home/ubuntu/miniconda3/lib" \
  numactl --cpunodebind=0 --membind=0 \
    python3 -m vllm.entrypoints.openai.api_server \
      --device neuron \
      --model $MODEL_NAME \
      --tensor-parallel-size 4 \
      --max-num-seqs 16 \
      --max-model-len 32 \
      --block-size 32 \
      --port 9000
