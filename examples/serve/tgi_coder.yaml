service:
  port: 8082
  readiness_probe: /health
  replicas: 2

resources:
  accelerators: A100:1

# TODO(tian): Maybe use some small model like 3b.
run: |
  docker run --gpus all --shm-size 1g -p 8082:80 -v ~/data:/data ghcr.io/huggingface/text-generation-inference --model-id WizardLM/WizardCoder-15B-V1.0
