name: multi-lb-llm

envs:
  # MODEL_NAME: meta-llama/Llama-3.2-3B-Instruct
  MODEL_NAME: meta-llama/Llama-3.1-8B-Instruct
  HF_TOKEN:

service:
  readiness_probe:
    # Don't use /v1/chat/completions as that will be
    # queued and it is possible to fail for timeout.
    path: /v1/models
  replicas: 8
  # TODO(tian): Change the config to a cloud-agnostic way.
  route53_hosted_zone: aws.cblmemo.net
  # Meta load balancing policy used to select the LB.
  load_balancing_policy: prefix_tree
  external_load_balancers:
    - resources:
        cloud: aws
        region: us-east-2
      load_balancing_policy: prefix_tree
    - resources:
        cloud: aws
        region: ap-northeast-1
      load_balancing_policy: prefix_tree
    - resources:
        cloud: aws
        region: eu-central-1
      load_balancing_policy: prefix_tree
  # max_concurrent_requests: 50

resources:
  cloud: aws
  cpus: 16+
  disk_tier: high
  accelerators: L4:1
  ordered:
    - region: us-east-2
    - region: ap-northeast-1
    - region: eu-central-1
  ports: 8081

setup: |
  # pip install vllm==0.6.2
  # pip install transformers==4.48.3 "sglang[all]==0.4.3.post2" \
  #   --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/
  # For cache report
  # pip install "sglang[all]>=0.4.5" \
  #   --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python

  uv venv -p 3.10
  source .venv/bin/activate
  uv pip install huggingface-hub==0.30.2
  # Parallel the installation and download the model.
  uv pip install setuptools==79.0.0 "sglang[all]==0.4.5" \
    --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python &
  huggingface-cli download $MODEL_NAME \
    --max-workers 32 \
    --include "*.safetensors"
  wait

run: |
  # vllm serve $MODEL_NAME \
  #   --port 8081 \
  #   --enable-prefix-caching \
  #   --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \
  #   --max-model-len 4096

  source .venv/bin/activate
  python -m sglang.launch_server \
    --model-path $MODEL_NAME \
    --port 8081 \
    --host 0.0.0.0 \
    --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \
    --enable-metrics \
    --enable-cache-report \
    --mem-fraction-static 0.8
