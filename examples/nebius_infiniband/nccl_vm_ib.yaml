# This example is used to test the NCCL performance
# with InfiniBand on Nebius VMs.
resources:
  cloud: nebius
  region: eu-north1
  accelerators: H100:8
  image_id: docker:cr.eu-north1.nebius.cloud/nebius-benchmarks/nccl-tests:2.23.4-ubu22.04-cu12.4

num_nodes: 2

setup: |
  sudo apt-get install -y iproute2

run: | 
  # port 10022 in containers on Nebius VMs
  export SSH_PORT=$(ss -tlnp | grep sshd | awk '{print $4}' | cut -d':' -f2)
  export NCCL_SOCKET_IFNAME=$(ip -o -4 route show to default | awk '{print $5}')

  # Total number of processes, NP should be the total number of GPUs in the cluster
  NP=$(($SKYPILOT_NUM_GPUS_PER_NODE * $SKYPILOT_NUM_NODES))

  # Append :${SKYPILOT_NUM_GPUS_PER_NODE} to each IP as slots
  nodes=""
  for ip in $SKYPILOT_NODE_IPS; do
    nodes="${nodes}${ip}:${SKYPILOT_NUM_GPUS_PER_NODE},"
  done
  nodes=${nodes::-1}

  if [ "${SKYPILOT_NODE_RANK}" == "0" ]; then
    mpirun \
      --allow-run-as-root \
      -H $nodes \
      -np $NP \
      -N $SKYPILOT_NUM_GPUS_PER_NODE \
      -bind-to none \
      -x LD_LIBRARY_PATH \
      -x NCCL_SOCKET_IFNAME \
      -x NCCL_IB_HCA=mlx5 \
      -x NCCL_ALGO=NVLSTree \
      -x UCX_NET_DEVICES=mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1 \
      -x SHARP_COLL_ENABLE_PCI_RELAXED_ORDERING=1 \
      -x NCCL_COLLNET_ENABLE=0 \
      --mca plm_rsh_args "-p $SSH_PORT" \
      /opt/nccl_tests/build/all_reduce_perf \
      -b 512M \
      -e 8G \
      -f 2 \
      -g 1
  else
    echo "worker node"
  fi

config:
  docker:
    run_options:
      - --device=/dev/infiniband
      - --cap-add=IPC_LOCK
