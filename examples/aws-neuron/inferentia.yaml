resources:
  accelerators: Inferentia:6
  disk_size: 512
  ports: 9000

envs:
  MODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct

secrets:
  HF_TOKEN: null # Pass with `--secret HF_TOKEN` in CLI

setup: |
  # Install transformers-neuronx and its dependencies
  sudo apt-get install -y python3.10-venv g++
  python3.10 -m venv aws_neuron_venv_pytorch
  source aws_neuron_venv_pytorch/bin/activate
  pip install ipykernel
  python3.10 -m ipykernel install --user --name aws_neuron_venv_pytorch --display-name "Python (torch-neuronx)"
  pip install jupyter notebook
  pip install environment_kernels
  python -m pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com
  python -m pip install wget
  python -m pip install awscli
  python -m pip install --upgrade neuronx-cc==2.* --pre torch-neuronx==2.1.* torchvision transformers-neuronx

  # Install latest version of triton.
  # Reference: https://github.com/vllm-project/vllm/issues/6987
  pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple triton-nightly

  # Install vLLM from source. Avoid using dir name 'vllm' due to import conflict.
  # Reference: https://github.com/vllm-project/vllm/issues/1814#issuecomment-1837122930
  git clone https://github.com/vllm-project/vllm.git vllm_repo
  cd vllm_repo
  pip install -U -r requirements-neuron.txt
  VLLM_TARGET_DEVICE="neuron" pip install -e .

  python -c "import huggingface_hub; huggingface_hub.login('${HF_TOKEN}')"

  sudo apt update
  sudo apt install -y numactl

run: |
  source aws_neuron_venv_pytorch/bin/activate
  # Calculate the tensor parallel size. vLLM requires the tensor parallel size
  # to be a factor of the number of attention heads, which is 32 for the model.
  # Here we calculate the largest power of 2 that is less than or equal to the
  # number of GPUs per node.
  TENSOR_PARALLEL_SIZE=1
  while [ $(($TENSOR_PARALLEL_SIZE * 2)) -le $SKYPILOT_NUM_GPUS_PER_NODE ]; do
    TENSOR_PARALLEL_SIZE=$(($TENSOR_PARALLEL_SIZE * 2))
  done
  NEURON_RT_VISIBLE_CORES="0-$(($TENSOR_PARALLEL_SIZE - 1))"
  OMP_NUM_THREADS=$SKYPILOT_NUM_GPUS_PER_NODE
  MASTER_PORT=12355
  LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/home/ubuntu/miniconda3/lib"
  numactl --cpunodebind=0 --membind=0 \
    python3 -m vllm.entrypoints.openai.api_server \
      --device neuron \
      --model $MODEL_NAME \
      --tensor-parallel-size $TENSOR_PARALLEL_SIZE \
      --max-num-seqs 16 \
      --max-model-len 32 \
      --block-size 32 \
      --port 9000
