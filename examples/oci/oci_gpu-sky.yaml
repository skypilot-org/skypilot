name: gpu-task1

resources:
  # Optional; if left out, automatically pick the cheapest cloud.
  cloud: oci

  accelerators: A10:1  # 1x NVIDIA A10 GPU

  region: ap-seoul-1
  
  #zone: bxtG:AP-SEOUL-1-AD-1
  
  #instance_type: VM.GPU.A10.1

  #image_id: skypilot:gpu-ubuntu-2004

# Working directory (optional) containing the project codebase.
# Its contents are synced to ~/sky_workdir/ on the cluster.
workdir: ~/torch_examples

num_nodes: 1

# Typical use: pip install -r requirements.txt
# Invoked under the workdir (i.e., can use its files).
setup: |
  echo "*** Running setup. ***"
  pip install torch torchvision

# Typical use: make use of resources, such as running training.
# Invoked under the workdir (i.e., can use its files).
run: |
  echo "*** Running the task on OCI ***"
  cd mnist
  python main.py --epochs 1
  echo "The task is completed."



# To launch a cluster using this file, run
#
# sky launch -c gpu001 oci_gpu-sky.yaml
#
############## Outputs ##############
#......
#
# (sky) hysunhe@HYHE-PF1ZGYCQ:~/projects/skypilot/oci-sky$ sky launch -c gpu001 oci_gpu-sky.yaml
# Task from YAML spec: oci_gpu-sky.yaml
# D 06-10 09:02:20 optimizer.py:245] ## gpu-task1 ##
# D 06-10 09:02:20 optimizer.py:285] Defaulting the task's estimated time to 1 hour.
# D 06-10 09:02:20 optimizer.py:302] resources: OCI(VM.GPU.A10.1, {'A10': 1})
# D 06-10 09:02:20 optimizer.py:311]   estimated_runtime: 3600 s (1.0 hr)
# D 06-10 09:02:20 optimizer.py:315]   estimated_cost (not incl. egress): $2.0
# I 06-10 09:02:20 optimizer.py:636] == Optimizer ==
# I 06-10 09:02:20 optimizer.py:659] Estimated cost: $2.0 / hour
# I 06-10 09:02:20 optimizer.py:659]
# I 06-10 09:02:20 optimizer.py:732] Considered resources (1 node):
# I 06-10 09:02:20 optimizer.py:781] -------------------------------------------------------------------------------------------
# I 06-10 09:02:20 optimizer.py:781]  CLOUD   INSTANCE       vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE   COST ($)   CHOSEN
# I 06-10 09:02:20 optimizer.py:781] -------------------------------------------------------------------------------------------
# I 06-10 09:02:20 optimizer.py:781]  OCI     VM.GPU.A10.1   30      240       A10:1          ap-seoul-1    2.00          ✔
# I 06-10 09:02:20 optimizer.py:781] -------------------------------------------------------------------------------------------
# I 06-10 09:02:20 optimizer.py:781]
# Launching a new cluster 'gpu001'. Proceed? [Y/n]: y
# I 06-10 09:02:22 cloud_vm_ray_backend.py:3676] Creating a new cluster: "gpu001" [1x OCI(VM.GPU.A10.1, {'A10': 1})].
# I 06-10 09:02:22 cloud_vm_ray_backend.py:3676] Tip: to reuse an existing cluster, specify --cluster (-c). Run `sky status` to see existing clusters.
# I 06-10 09:02:24 cloud_vm_ray_backend.py:1303] To view detailed progress: tail -n100 -f /home/hysunhe/sky_logs/sky-2023-06-10-09-02-20-939775/provision.log
# D 06-10 09:02:24 oci.py:482] Got default image_id ocid1.image.oc1..aaaaaaaaupqzeltt5xqtbtf3mspimqam4ozpmt3r3o5zyg2xg7u5hevlvpaq|ocid1.appcataloglisting.oc1..aaaaaaaapnjluswlxhczeqgao6t737bepuvrvh5tfrlhy2lmalh6bh2fr4hq|20230425 from tag skypilot:gpu-ubuntu-2004
# D 06-10 09:02:24 oci.py:401] OCI credential file mounts: {'~/.oci/config': '~/.oci/config', '~/.oci/oci_api_key2.pem': '~/.oci/oci_api_key2.pem'}
# D 06-10 09:02:24 backend_utils.py:885] Using ssh_proxy_command: None
# I 06-10 09:02:24 cloud_vm_ray_backend.py:1627] Launching on OCI ap-seoul-1 (AP-SEOUL-1-AD-1)
# D 06-10 09:02:24 cloud_vm_ray_backend.py:159] `ray up` script: /tmp/skypilot_ray_up_u1_hye6h.py
# I 06-10 09:04:02 log_utils.py:89] Head node is up.
# D 06-10 09:04:55 cloud_vm_ray_backend.py:1717] `ray up` takes 150.6 seconds with 1 retries.
# I 06-10 09:04:55 cloud_vm_ray_backend.py:1440] Successfully provisioned or found existing VM.
# D 06-10 09:04:59 backend_utils.py:1381] Retrying to get head ip.
# I 06-10 09:05:12 cloud_vm_ray_backend.py:2601] Syncing workdir (to 1 node): ~/torch_examples -> ~/sky_workdir
# I 06-10 09:05:12 cloud_vm_ray_backend.py:2609] To view detailed progress: tail -n100 -f ~/sky_logs/sky-2023-06-10-09-02-20-939775/workdir_sync.log
# I 06-10 09:06:24 cloud_vm_ray_backend.py:2704] Running setup on 1 node.
# Warning: Permanently added '129.154.222.76' (ED25519) to the list of known hosts.
# *** Running setup. ***
#Collecting torch
#  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 604.5 kB/s eta 0:00:00
#Collecting torchvision
#  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 3.6 MB/s eta 0:00:00
#Requirement already satisfied: typing-extensions in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from torch) (4.6.3)
#Collecting sympy
#  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/5.7 MB 3.1 MB/s eta 0:00:00
#Collecting nvidia-cuda-runtime-cu11==11.7.99
#  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 kB 1.5 MB/s eta 0:00:00
#Collecting nvidia-cublas-cu11==11.10.3.66
#  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 1.9 MB/s eta 0:00:00
#Collecting nvidia-nccl-cu11==2.14.3
#  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.1/177.1 MB 4.4 MB/s eta 0:00:00
#Requirement already satisfied: filelock in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from torch) (3.12.0)
#Collecting nvidia-cuda-nvrtc-cu11==11.7.99
#  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 71.0 MB/s eta 0:00:00
#Collecting nvidia-cufft-cu11==10.9.0.58
#  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.4/168.4 MB 4.3 MB/s eta 0:00:00
#Collecting nvidia-curand-cu11==10.2.10.91
#  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.6/54.6 MB 18.1 MB/s eta 0:00:00
#Collecting nvidia-nvtx-cu11==11.7.91
#  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.6/98.6 kB 27.5 MB/s eta 0:00:00
#Collecting nvidia-cusparse-cu11==11.7.4.91
#  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.2/173.2 MB 4.6 MB/s eta 0:00:00
#Requirement already satisfied: networkx in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from torch) (3.1)
#Collecting nvidia-cudnn-cu11==8.5.0.96
#  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 966.5 kB/s eta 0:00:00
#Collecting triton==2.0.0
#  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.3/63.3 MB 3.7 MB/s eta 0:00:00
#Collecting nvidia-cuda-cupti-cu11==11.7.101
#  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 7.5 MB/s eta 0:00:00
#Collecting nvidia-cusolver-cu11==11.4.0.1
#  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.6/102.6 MB 2.6 MB/s eta 0:00:00
#Requirement already satisfied: jinja2 in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from torch) (3.1.2)
#Requirement already satisfied: setuptools in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.6.3)
#Requirement already satisfied: wheel in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)
#Collecting lit
#  Downloading lit-16.0.5.post0.tar.gz (138 kB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.1/138.1 kB 182.8 kB/s eta 0:00:00
#  Preparing metadata (setup.py): started
#  Preparing metadata (setup.py): finished with status 'done'
#Collecting cmake
#  Downloading cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.0/24.0 MB 5.8 MB/s eta 0:00:00
#Requirement already satisfied: numpy in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from torchvision) (1.24.3)
#Collecting pillow!=8.3.*,>=5.3.0
#  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 2.4 MB/s eta 0:00:00
#Requirement already satisfied: requests in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from torchvision) (2.28.2)
#Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)
#Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)
#Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)
#Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)
#Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniforge3/lib/python3.10/site-packages (from requests->torchvision) (3.4)
#Collecting mpmath>=0.19
#  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 356.2 kB/s eta 0:00:00
#Building wheels for collected packages: lit
#  Building wheel for lit (setup.py): started
#  Building wheel for lit (setup.py): finished with status 'done'
#  Created wheel for lit: filename=lit-16.0.5.post0-py3-none-any.whl size=88257 sha256=dbd038637720b739bf3f78985850822e2814e72410d940f17a96b6754269da1d
#  Stored in directory: /home/ubuntu/.cache/pip/wheels/1a/24/92/1e1c9e37be8411a7c7c18a4c54962f5d0a75c56bab4a6f7f57
#Successfully built lit
#Installing collected packages: mpmath, lit, cmake, sympy, pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchvision
#Successfully installed cmake-3.26.4 lit-16.0.5.post0 mpmath-1.3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pillow-9.5.0 sympy-1.12 torch-2.0.1 torchvision-0.15.2 triton-2.0.0
#I 06-10 09:08:36 cloud_vm_ray_backend.py:2714] Setup completed.
#D 06-10 09:08:36 cloud_vm_ray_backend.py:2716] Setup took 132.76441979408264 seconds.
#D 06-10 09:08:38 cloud_vm_ray_backend.py:495] Added Task with options: , num_cpus=0.5, resources={"A10": 1}, placement_group=pg, placement_group_bundle_index=0, num_gpus=1
#I 06-10 09:08:52 cloud_vm_ray_backend.py:2817] Job submitted with Job ID: 1
#I 06-10 01:08:54 log_lib.py:395] Start streaming logs for job 1.
#INFO: Tip: use Ctrl-C to exit log streaming (task will not be killed).
#INFO: Waiting for task resources on 1 node. This will block if the cluster is full.
#INFO: All task resources reserved.
#INFO: Reserved IPs: ['192.168.2.33']
#(gpu-task1, pid=9493) *** Running the task on OCI ***
#(gpu-task1, pid=9493) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
#(gpu-task1, pid=9493) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz
#100%|██████████| 9912422/9912422 [00:01<00:00, 8868034.02it/s] 0:00, 10939126.46it/s]
#(gpu-task1, pid=9493) Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw
#(gpu-task1, pid=9493)
#(gpu-task1, pid=9493) Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
#(gpu-task1, pid=9493) Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz
#100%|██████████| 28881/28881 [00:00<00:00, 6941873.57it/s]
#(gpu-task1, pid=9493) Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw
#(gpu-task1, pid=9493)
#(gpu-task1, pid=9493) Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
#(gpu-task1, pid=9493) Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz
#100%|██████████| 1648877/1648877 [00:00<00:00, 7370229.17it/s]0:00, 1832775.97it/s]
#(gpu-task1, pid=9493) Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw
#(gpu-task1, pid=9493)
#(gpu-task1, pid=9493) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
#(gpu-task1, pid=9493) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz
#100%|██████████| 4542/4542 [00:00<00:00, 61059387.08it/s]
#(gpu-task1, pid=9493) Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw
#(gpu-task1, pid=9493)
#(gpu-task1, pid=9493) Train Epoch: 1 [0/60000 (0%)]     Loss: 2.283599
#(gpu-task1, pid=9493) Train Epoch: 1 [640/60000 (1%)]   Loss: 1.252327
#(gpu-task1, pid=9493) Train Epoch: 1 [1280/60000 (2%)]  Loss: 0.949825
#(gpu-task1, pid=9493) Train Epoch: 1 [1920/60000 (3%)]  Loss: 0.604047
#(gpu-task1, pid=9493) Train Epoch: 1 [2560/60000 (4%)]  Loss: 0.530467
#(gpu-task1, pid=9493) Train Epoch: 1 [3200/60000 (5%)]  Loss: 0.459707
#(gpu-task1, pid=9493) Train Epoch: 1 [3840/60000 (6%)]  Loss: 0.247857
#(gpu-task1, pid=9493) Train Epoch: 1 [4480/60000 (7%)]  Loss: 0.695259
#(gpu-task1, pid=9493) Train Epoch: 1 [5120/60000 (9%)]  Loss: 0.233646
#(gpu-task1, pid=9493) Train Epoch: 1 [5760/60000 (10%)] Loss: 0.258125
#(gpu-task1, pid=9493) Train Epoch: 1 [6400/60000 (11%)] Loss: 0.328699
#(gpu-task1, pid=9493) Train Epoch: 1 [7040/60000 (12%)] Loss: 0.228363
#(gpu-task1, pid=9493) Train Epoch: 1 [7680/60000 (13%)] Loss: 0.280207
#(gpu-task1, pid=9493) Train Epoch: 1 [8320/60000 (14%)] Loss: 0.218926
#(gpu-task1, pid=9493) Train Epoch: 1 [8960/60000 (15%)] Loss: 0.429853
#(gpu-task1, pid=9493) Train Epoch: 1 [9600/60000 (16%)] Loss: 0.130480
#(gpu-task1, pid=9493) Train Epoch: 1 [10240/60000 (17%)]        Loss: 0.186145
#(gpu-task1, pid=9493) Train Epoch: 1 [10880/60000 (18%)]        Loss: 0.200695
#(gpu-task1, pid=9493) Train Epoch: 1 [11520/60000 (19%)]        Loss: 0.284932
#(gpu-task1, pid=9493) Train Epoch: 1 [12160/60000 (20%)]        Loss: 0.110433
#(gpu-task1, pid=9493) Train Epoch: 1 [12800/60000 (21%)]        Loss: 0.155883
#(gpu-task1, pid=9493) Train Epoch: 1 [13440/60000 (22%)]        Loss: 0.157380
#(gpu-task1, pid=9493) Train Epoch: 1 [14080/60000 (23%)]        Loss: 0.129046
#(gpu-task1, pid=9493) Train Epoch: 1 [14720/60000 (25%)]        Loss: 0.162695
#(gpu-task1, pid=9493) Train Epoch: 1 [15360/60000 (26%)]        Loss: 0.326700
#(gpu-task1, pid=9493) Train Epoch: 1 [16000/60000 (27%)]        Loss: 0.284656
#(gpu-task1, pid=9493) Train Epoch: 1 [16640/60000 (28%)]        Loss: 0.130473
#(gpu-task1, pid=9493) Train Epoch: 1 [17280/60000 (29%)]        Loss: 0.126890
#(gpu-task1, pid=9493) Train Epoch: 1 [17920/60000 (30%)]        Loss: 0.200452
#(gpu-task1, pid=9493) Train Epoch: 1 [18560/60000 (31%)]        Loss: 0.216385
#(gpu-task1, pid=9493) Train Epoch: 1 [19200/60000 (32%)]        Loss: 0.136077
#(gpu-task1, pid=9493) Train Epoch: 1 [19840/60000 (33%)]        Loss: 0.121466
#(gpu-task1, pid=9493) Train Epoch: 1 [20480/60000 (34%)]        Loss: 0.178021
#(gpu-task1, pid=9493) Train Epoch: 1 [21120/60000 (35%)]        Loss: 0.177888
#(gpu-task1, pid=9493) Train Epoch: 1 [21760/60000 (36%)]        Loss: 0.302803
#(gpu-task1, pid=9493) Train Epoch: 1 [22400/60000 (37%)]        Loss: 0.105803
#(gpu-task1, pid=9493) Train Epoch: 1 [23040/60000 (38%)]        Loss: 0.235509
#(gpu-task1, pid=9493) Train Epoch: 1 [23680/60000 (39%)]        Loss: 0.265663
#(gpu-task1, pid=9493) Train Epoch: 1 [24320/60000 (41%)]        Loss: 0.090274
#(gpu-task1, pid=9493) Train Epoch: 1 [24960/60000 (42%)]        Loss: 0.102950
#(gpu-task1, pid=9493) Train Epoch: 1 [25600/60000 (43%)]        Loss: 0.103180
#(gpu-task1, pid=9493) Train Epoch: 1 [26240/60000 (44%)]        Loss: 0.169355
#(gpu-task1, pid=9493) Train Epoch: 1 [26880/60000 (45%)]        Loss: 0.272416
#(gpu-task1, pid=9493) Train Epoch: 1 [27520/60000 (46%)]        Loss: 0.100975
#(gpu-task1, pid=9493) Train Epoch: 1 [28160/60000 (47%)]        Loss: 0.132255
#(gpu-task1, pid=9493) Train Epoch: 1 [28800/60000 (48%)]        Loss: 0.020894
#(gpu-task1, pid=9493) Train Epoch: 1 [29440/60000 (49%)]        Loss: 0.053596
#(gpu-task1, pid=9493) Train Epoch: 1 [30080/60000 (50%)]        Loss: 0.213987
#(gpu-task1, pid=9493) Train Epoch: 1 [30720/60000 (51%)]        Loss: 0.140680
#(gpu-task1, pid=9493) Train Epoch: 1 [31360/60000 (52%)]        Loss: 0.202938
#(gpu-task1, pid=9493) Train Epoch: 1 [32000/60000 (53%)]        Loss: 0.121934
#(gpu-task1, pid=9493) Train Epoch: 1 [32640/60000 (54%)]        Loss: 0.084534
#(gpu-task1, pid=9493) Train Epoch: 1 [33280/60000 (55%)]        Loss: 0.147065
#(gpu-task1, pid=9493) Train Epoch: 1 [33920/60000 (57%)]        Loss: 0.296672
#(gpu-task1, pid=9493) Train Epoch: 1 [34560/60000 (58%)]        Loss: 0.114485
#(gpu-task1, pid=9493) Train Epoch: 1 [35200/60000 (59%)]        Loss: 0.179824
#(gpu-task1, pid=9493) Train Epoch: 1 [35840/60000 (60%)]        Loss: 0.080043
#(gpu-task1, pid=9493) Train Epoch: 1 [36480/60000 (61%)]        Loss: 0.300353
#(gpu-task1, pid=9493) Train Epoch: 1 [37120/60000 (62%)]        Loss: 0.096971
#(gpu-task1, pid=9493) Train Epoch: 1 [37760/60000 (63%)]        Loss: 0.052622
#(gpu-task1, pid=9493) Train Epoch: 1 [38400/60000 (64%)]        Loss: 0.117060
#(gpu-task1, pid=9493) Train Epoch: 1 [39040/60000 (65%)]        Loss: 0.036659
#(gpu-task1, pid=9493) Train Epoch: 1 [39680/60000 (66%)]        Loss: 0.483730
#(gpu-task1, pid=9493) Train Epoch: 1 [40320/60000 (67%)]        Loss: 0.074531
#(gpu-task1, pid=9493) Train Epoch: 1 [40960/60000 (68%)]        Loss: 0.095874
#(gpu-task1, pid=9493) Train Epoch: 1 [41600/60000 (69%)]        Loss: 0.211131
#(gpu-task1, pid=9493) Train Epoch: 1 [42240/60000 (70%)]        Loss: 0.178158
#(gpu-task1, pid=9493) Train Epoch: 1 [42880/60000 (71%)]        Loss: 0.034227
#(gpu-task1, pid=9493) Train Epoch: 1 [43520/60000 (72%)]        Loss: 0.038770
#(gpu-task1, pid=9493) Train Epoch: 1 [44160/60000 (74%)]        Loss: 0.075408
#(gpu-task1, pid=9493) Train Epoch: 1 [44800/60000 (75%)]        Loss: 0.090562
#(gpu-task1, pid=9493) Train Epoch: 1 [45440/60000 (76%)]        Loss: 0.185449
#(gpu-task1, pid=9493) Train Epoch: 1 [46080/60000 (77%)]        Loss: 0.063037
#(gpu-task1, pid=9493) Train Epoch: 1 [46720/60000 (78%)]        Loss: 0.057180
#(gpu-task1, pid=9493) Train Epoch: 1 [47360/60000 (79%)]        Loss: 0.074373
#(gpu-task1, pid=9493) Train Epoch: 1 [48000/60000 (80%)]        Loss: 0.046954
#(gpu-task1, pid=9493) Train Epoch: 1 [48640/60000 (81%)]        Loss: 0.265737
#(gpu-task1, pid=9493) Train Epoch: 1 [49280/60000 (82%)]        Loss: 0.075507
#(gpu-task1, pid=9493) Train Epoch: 1 [49920/60000 (83%)]        Loss: 0.083043
#(gpu-task1, pid=9493) Train Epoch: 1 [50560/60000 (84%)]        Loss: 0.062400
#(gpu-task1, pid=9493) Train Epoch: 1 [51200/60000 (85%)]        Loss: 0.107635
#(gpu-task1, pid=9493) Train Epoch: 1 [51840/60000 (86%)]        Loss: 0.128209
#(gpu-task1, pid=9493) Train Epoch: 1 [52480/60000 (87%)]        Loss: 0.047502
#(gpu-task1, pid=9493) Train Epoch: 1 [53120/60000 (88%)]        Loss: 0.086729
#(gpu-task1, pid=9493) Train Epoch: 1 [53760/60000 (90%)]        Loss: 0.068962
#(gpu-task1, pid=9493) Train Epoch: 1 [54400/60000 (91%)]        Loss: 0.041404
#(gpu-task1, pid=9493) Train Epoch: 1 [55040/60000 (92%)]        Loss: 0.203918
#(gpu-task1, pid=9493) Train Epoch: 1 [55680/60000 (93%)]        Loss: 0.144886
#(gpu-task1, pid=9493) Train Epoch: 1 [56320/60000 (94%)]        Loss: 0.247239
#(gpu-task1, pid=9493) Train Epoch: 1 [56960/60000 (95%)]        Loss: 0.100990
#(gpu-task1, pid=9493) Train Epoch: 1 [57600/60000 (96%)]        Loss: 0.118975
#(gpu-task1, pid=9493) Train Epoch: 1 [58240/60000 (97%)]        Loss: 0.081676
#(gpu-task1, pid=9493) Train Epoch: 1 [58880/60000 (98%)]        Loss: 0.061787
#(gpu-task1, pid=9493) Train Epoch: 1 [59520/60000 (99%)]        Loss: 0.059713
#(gpu-task1, pid=9493)
#(gpu-task1, pid=9493) Test set: Average loss: 0.0480, Accuracy: 9831/10000 (98%)
#(gpu-task1, pid=9493)
#(gpu-task1, pid=9493) The task is completed.
#INFO: Job finished (status: SUCCEEDED).
#Shared connection to 129.154.222.76 closed.
#I 06-10 09:09:33 cloud_vm_ray_backend.py:2847] Job ID: 1
#I 06-10 09:09:33 cloud_vm_ray_backend.py:2847] To cancel the job:       sky cancel gpu001 1
#I 06-10 09:09:33 cloud_vm_ray_backend.py:2847] To stream job logs:      sky logs gpu001 1
#I 06-10 09:09:33 cloud_vm_ray_backend.py:2847] To view the job queue:   sky queue gpu001
#I 06-10 09:09:33 cloud_vm_ray_backend.py:2962]
#I 06-10 09:09:33 cloud_vm_ray_backend.py:2962] Cluster name: gpu001
#I 06-10 09:09:33 cloud_vm_ray_backend.py:2962] To log into the head VM: ssh gpu001
#I 06-10 09:09:33 cloud_vm_ray_backend.py:2962] To submit a job:         sky exec gpu001 yaml_file
#I 06-10 09:09:33 cloud_vm_ray_backend.py:2962] To stop the cluster:     sky stop gpu001
#I 06-10 09:09:33 cloud_vm_ray_backend.py:2962] To teardown the cluster: sky down gpu001
#Clusters
#NAME         LAUNCHED    RESOURCES                         STATUS  AUTOSTOP  COMMAND
#gpu001       4 mins ago  1x OCI(VM.GPU.A10.1, {'A10': 1})  UP      -         sky launch -c gpu001 oci_...
