# service.yaml
service:
  readiness_probe: /v1/models
  replicas: 2

# Fields below describe each replica.
resources:
  infra: oci/us-sanjose-1
  ports: 8080
  accelerators: {A10:1}

setup: |
  if [ ! -d ~/vllm ]; then
    uv venv ~/vllm --python 3.12
  fi
  source ~/vllm/bin/activate
  pip install vllm==0.6.3.post1
  pip install vllm-flash-attn==2.6.2

run: |
  source ~/vllm/bin/activate
  python -u -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 --port 8080 \
    --model Qwen/Qwen2-7B-Instruct \
    --served-model-name Qwen2-7B-Instruct \
    --device=cuda --dtype auto --max-model-len=2048
