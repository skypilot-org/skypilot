# Distributed training a GPT style model with Nvidia NeMo on multiple nodes.
#
# Inspired from https://github.com/NVIDIA/NeMo/blob/main/docs/source/nlp/nemo_megatron/gpt/gpt_training.rst
#
# Note that we provide a read-only bucket at gs://sky-wiki-data that is used to
# download preprocessed data to your bucket. If you want to preprocess the data
# yourself, see nemo_gpt_preprocessing.yaml.
#
# After the script completes, the model checkpoints will be saved in
# ~/sky_workdir/nemo_experiments/megatron_gpt/checkpoints on the head node.
#
# Usage:
#   sky launch -s -c nemo_gpt_train nemo_gpt_train.yaml
#
#   # The setup will take some time (~1 hr), feel free to ctrl-c once the setup script starts
#   # You can reconnect to log stream using `sky logs nemo_gpt_train`
#
#   # Terminate cluster after you're done
#   sky down nemo_gpt_train

resources:
  accelerators: A100:1
  image_id: docker:nvcr.io/nvidia/nemo:24.05
  cloud: gcp
  disk_tier: best
  disk_size: 500

num_nodes: 2

envs:
  DATASET_ROOT: /juicefs
  CHECKPOINT_PATH: /juicefs # Write to same bucket as dataset
  VAL_CHECK_INTERVAL: 5

run: |
  conda deactivate
  
  # ============== Dependency Setup ==============
  # Clone NeMo if not already present
  if [ ! -d NeMo ]; then
      git clone https://github.com/NVIDIA/NeMo.git
      cd NeMo 
      git checkout 5df8e11255802a2ce2f33db6362e60990e215b64
      cd ..
  fi
  
  # ============= Training =============  
  # Get the number of nodes and master address from SkyPilot envvars
  num_nodes=`echo "$SKYPILOT_NODE_IPS" | wc -l`
  master_addr=`echo "$SKYPILOT_NODE_IPS" | head -n1`
  
  # Kill any existing megatron processes
  pkill -f -9 megatron
  
  python -m torch.distributed.run \
    --nproc_per_node=${SKYPILOT_NUM_GPUS_PER_NODE} \
    --nnodes=${num_nodes} \
    --node_rank=${SKYPILOT_NODE_RANK} \
    --master_addr=${master_addr} \
    --master_port=12375 \
    NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py  \
    --config-path=conf \
    --config-name=megatron_gpt_config \
    trainer.devices=${SKYPILOT_NUM_GPUS_PER_NODE} \
    trainer.num_nodes=${num_nodes} \
    trainer.max_epochs=null \
    trainer.max_steps=300000 \
    trainer.val_check_interval=${VAL_CHECK_INTERVAL} \
    trainer.log_every_n_steps=50 \
    trainer.limit_val_batches=50 \
    trainer.limit_test_batches=50 \
    trainer.accumulate_grad_batches=1 \
    trainer.precision=16 \
    model.mcore_gpt=True \
    model.micro_batch_size=6 \
    model.global_batch_size=192 \
    model.tensor_model_parallel_size=1 \
    model.pipeline_model_parallel_size=1 \
    model.max_position_embeddings=1024 \
    model.encoder_seq_length=1024 \
    model.hidden_size=768 \
    model.ffn_hidden_size=3072 \
    model.num_layers=12 \
    model.num_attention_heads=12 \
    model.init_method_std=0.021 \
    model.hidden_dropout=0.1 \
    model.layernorm_epsilon=1e-5 \
    model.tokenizer.vocab_file=${DATASET_ROOT}/gpt2-vocab.json \
    model.tokenizer.merge_file=${DATASET_ROOT}/gpt2-merges.txt \
    model.data.data_prefix=[1.0,${DATASET_ROOT}/hfbpe_gpt_training_data_text_document] \
    model.data.num_workers=4 \
    model.data.seq_length=1024 \
    model.data.splits_string=\'980,10,10\' \
    model.optim.name=fused_adam \
    model.optim.lr=6e-4 \
    model.optim.betas=[0.9,0.95] \
    model.optim.weight_decay=0.1 \
    model.optim.sched.name=CosineAnnealing \
    model.optim.sched.warmup_steps=750 \
    model.optim.sched.constant_steps=80000 \
    model.optim.sched.min_lr=6e-5 \
    exp_manager.resume_if_exists=True \
    exp_manager.resume_ignore_no_checkpoint=True \
    exp_manager.create_checkpoint_callback=True \
    +exp_manager.checkpoint_callback_params.dirpath=${CHECKPOINT_PATH} \
    exp_manager.checkpoint_callback_params.monitor=val_loss \
    exp_manager.checkpoint_callback_params.save_top_k=3 \
    exp_manager.checkpoint_callback_params.mode=min \
    exp_manager.checkpoint_callback_params.always_save_nemo=True
  
  # Optional - copy checkpoints to the mounted dataset bucket (~6 GB)
  # if [ ${SKYPILOT_NODE_RANK} -eq 0 ]; then
  #     mkdir -p ${DATASET_ROOT}/results
  #     cp -R ~/sky_workdir/nemo_experiments 
  # fi