resources:
  cloud: aws
  cpus: 4+
  disk_size: 50
  # Uncomment the next line to request GPUs
  # accelerators: V100:4  # Can be changed as needed (e.g., A100, T4, etc.)
  accelerators: L4:1  # Can be changed as needed (e.g., A100, T4, etc.)

num_nodes: 2  # Can be adjusted based on your needs

# Define environment variables with default values
envs:
  # If CONTROLLER_IP is not set, this is a primary controller
  # If CONTROLLER_IP is set, this is a secondary node joining an existing cluster
  CONTROLLER_IP: ""
  # Path to SSH key for connecting to the controller (for secondary clusters)
  CONTROLLER_SSH_PRIVATE_KEY: ""

# Using block scalar with chomping indicator (|) for the script
run: |
  set -ex
  sudo apt update
  sudo apt install -y munge slurm-wlm slurm-wlm-basic-plugins slurmctld slurmd jq

  # Get the SkyPilot cluster name
  CLUSTER_NAME=$(echo $SKYPILOT_CLUSTER_INFO | jq -r .cluster_name)
  echo "SkyPilot cluster name: $CLUSTER_NAME"

  # Get username - works on both AWS (ubuntu) and other clouds
  USERNAME=$(whoami)
  
  # Check if this is the primary controller or a secondary cluster
  # If CONTROLLER_IP is empty, this is the primary controller
  if [ -z "$CONTROLLER_IP" ]; then
    echo "Setting up primary Slurm controller cluster..."
    # Keep CONTROLLER_IP empty to signal this is primary controller
    # Store the primary IP in a different variable 
    PRIMARY_IP=$(echo "$SKYPILOT_NODE_IPS" | head -n1)
  else
    echo "Setting up secondary cluster as Slurm partition..."
    echo "Using controller IP: $CONTROLLER_IP"
    
    # If SSH key is provided, set it up for authentication to the controller
    if [ -n "$CONTROLLER_SSH_PRIVATE_KEY" ]; then
      echo "Setting up SSH key for controller access..."
      mkdir -p ~/.ssh
      
      # Copy the key to a location on the VM
      INTERNAL_SSH_KEY="/tmp/controller_ssh_key"
      echo "$CONTROLLER_SSH_PRIVATE_KEY" > $INTERNAL_SSH_KEY
      chmod 600 "$INTERNAL_SSH_KEY"
      
      # Get username - works on both AWS (ubuntu) and other clouds
      USERNAME=$(whoami)
      
      # Set up SSH config to use this key for the controller
      cat >> ~/.ssh/config << EOF
  Host $CONTROLLER_IP
    IdentityFile $INTERNAL_SSH_KEY
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
  EOF
    fi
  fi

  # Skip NVIDIA driver installation as it's typically pre-installed on GPU instances
  # If on a GPU instance, report driver status
  if command -v nvidia-smi &> /dev/null; then
    echo "Using pre-installed NVIDIA drivers"
    nvidia-smi --query-gpu=driver_version --format=csv,noheader
  fi

  # Define barrier function
  create_barrier() {
    local barrier_name=$1
    local barrier_file="/tmp/${barrier_name}${SKYPILOT_TASK_ID}_barrier"
    
    if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
      # Controller node creates and distributes barrier file
      touch $barrier_file
      # Get username if not already defined
      [ -z "$USERNAME" ] && USERNAME=$(whoami)
      for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
        scp -o StrictHostKeyChecking=no $barrier_file $USERNAME@$node:$barrier_file
      done
    else
      # Non-controller nodes wait for barrier file
      while [ ! -f "$barrier_file" ]; do
        echo "Waiting for $barrier_name barrier..."
        sleep 5
      done
    fi
  }

  # --- Define Slurm Node Processing Function ---
  # Usage: process_node <node_ip> <rank> <is_secondary_cluster> <target_cluster_name>
  # Populates: NODE_INFO array
  process_node() {
    local node_ip=$1
    local rank=$2
    local is_secondary=$3 # 1 if secondary, 0 if primary
    local target_cluster_name=$4
    
    local current_node_hostname=""
    local current_node_ip=""
    local current_node_cpus=""
    local current_node_sockets=""
    local current_node_cores_per_socket=""
    local current_node_threads_per_core=""
    local current_node_gres=""
    local current_gpu_count=0
    local CURRENT_GPU_TYPE=""
    
    if [ "$is_secondary" -eq 1 ]; then
        # Secondary cluster nodes need unique hostnames within Slurm config
        current_node_hostname="${target_cluster_name}-node-${rank}"
        current_node_ip=$node_ip # Use the IP from the list
        
        # Set the hostname on the node itself
        if ! ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "sudo hostnamectl set-hostname $current_node_hostname" </dev/null; then
          echo "Warning: Failed to set hostname using hostnamectl on $current_node_ip, trying hostname command..."
          ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "sudo hostname $current_node_hostname" </dev/null || echo "Warning: Failed to set hostname on $current_node_ip!"
        fi
        # Update /etc/hosts file
        ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "echo '127.0.0.1 $current_node_hostname' | sudo tee -a /etc/hosts" </dev/null
        
        # Create symlinks remotely
        ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "sudo mkdir -p /etc/slurm && sudo rm -f /etc/slurm/slurm.conf /etc/slurm/gres.conf && sudo ln -sf /mnt/slurm_config/slurm.conf /etc/slurm/slurm.conf && sudo ln -sf /mnt/slurm_config/gres.conf /etc/slurm/gres.conf" </dev/null
        
        # Get remote CPU info
        current_node_cpus=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip nproc </dev/null)
        current_node_sockets=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "lscpu | grep \"Socket(s):\" | awk '{print \$2}'" </dev/null)
        current_node_cores_per_socket=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "lscpu | grep \"Core(s) per socket:\" | awk '{print \$4}'" </dev/null)
        current_node_threads_per_core=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "lscpu | grep \"Thread(s) per core:\" | awk '{print \$4}'" </dev/null)
        
        # Check remote GPUs
        if ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "command -v nvidia-smi" &> /dev/null </dev/null; then
          current_gpu_count=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "nvidia-smi --query-gpu=name --format=csv,noheader | wc -l" </dev/null)
          if [ $current_gpu_count -gt 0 ]; then
            CURRENT_GPU_TYPE=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "nvidia-smi --query-gpu=name --format=csv,noheader | head -n1 | sed 's/NVIDIA //g' | sed 's/ /_/g'" </dev/null)
            current_node_gres="Gres=gpu:${CURRENT_GPU_TYPE}:${current_gpu_count}"
          fi
        fi
    elif [ "$rank" -eq 0 ]; then
      # Primary controller node (itself)
      current_node_hostname=$CONTROLLER_HOSTNAME # Already set globally
      current_node_ip=$(hostname -I | awk '{print $1}') # Use primary IP
      
      # Symlinks created earlier for controller
      
      # Get local CPU info
      unset OMP_NUM_THREADS
      current_node_cpus=$(nproc)
      current_node_sockets=$(lscpu | grep "Socket(s):" | awk '{print $2}')
      current_node_cores_per_socket=$(lscpu | grep "Core(s) per socket:" | awk '{print $4}')
      current_node_threads_per_core=$(lscpu | grep "Thread(s) per core:" | awk '{print $4}')
      
      # Check local GPUs
      if command -v nvidia-smi &> /dev/null; then
        current_gpu_count=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
        if [ $current_gpu_count -gt 0 ]; then
          CURRENT_GPU_TYPE=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -n1 | sed 's/NVIDIA //g' | sed 's/ /_/g')
          current_node_gres="Gres=gpu:${CURRENT_GPU_TYPE}:${current_gpu_count}"
        fi
      fi
    else
      # Primary worker node
      current_node_ip=$node_ip # Use the IP from the list
      
      # Create symlinks remotely
      ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "sudo mkdir -p /etc/slurm && sudo rm -f /etc/slurm/slurm.conf /etc/slurm/gres.conf && sudo ln -sf /mnt/slurm_config/slurm.conf /etc/slurm/slurm.conf && sudo ln -sf /mnt/slurm_config/gres.conf /etc/slurm/gres.conf" </dev/null
      
      # Get remote hostname
      current_node_hostname=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip hostname </dev/null)
      
      # Get remote CPU info
      current_node_cpus=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip nproc </dev/null)
      current_node_sockets=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "lscpu | grep \"Socket(s):\" | awk '{print \$2}'" </dev/null)
      current_node_cores_per_socket=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "lscpu | grep \"Core(s) per socket:\" | awk '{print \$4}'" </dev/null)
      current_node_threads_per_core=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "lscpu | grep \"Thread(s) per core:\" | awk '{print \$4}'" </dev/null)
      
      # Check remote GPUs
      if ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "command -v nvidia-smi" &> /dev/null </dev/null; then
        current_gpu_count=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "nvidia-smi --query-gpu=name --format=csv,noheader | wc -l" </dev/null)
        if [ $current_gpu_count -gt 0 ]; then
          CURRENT_GPU_TYPE=$(ssh -o StrictHostKeyChecking=no $USERNAME@$current_node_ip "nvidia-smi --query-gpu=name --format=csv,noheader | head -n1 | sed 's/NVIDIA //g' | sed 's/ /_/g'" </dev/null)
          current_node_gres="Gres=gpu:${CURRENT_GPU_TYPE}:${current_gpu_count}"
        fi
      fi
    fi # End node type check
    
    # Store node info for partitioning (Global variable modified here)
    NODE_INFO+=("$current_node_hostname:$current_gpu_count:$target_cluster_name")
    
    # Add node to slurm.conf
    local node_line="NodeName=${current_node_hostname} NodeAddr=${current_node_ip} CPUs=${current_node_cpus} Sockets=${current_node_sockets} CoresPerSocket=${current_node_cores_per_socket} ThreadsPerCore=${current_node_threads_per_core}"
    if [ -n "$current_node_gres" ]; then
      node_line="${node_line} ${current_node_gres}"
    fi
    node_line="${node_line} State=UNKNOWN"
    echo "$node_line" | sudo tee -a /mnt/slurm_config/slurm.conf
    
    # Add gres line if GPUs exist
    if [ $current_gpu_count -gt 0 ]; then
      local gpu_id_max=$(($current_gpu_count - 1))
      echo "NodeName=${current_node_hostname} Name=gpu Type=${CURRENT_GPU_TYPE} File=/dev/nvidia[0-${gpu_id_max}]" | sudo tee -a /mnt/slurm_config/gres.conf
    fi
  }

  # --- Define Partition Creation Function ---
  # Usage: create_partitions <is_primary_cluster>
  # Uses: NODE_INFO array (populated by process_node calls)
  create_partitions() {
    local is_primary=$1 # 1 if primary, 0 if secondary
    
    # First get lists of nodes with and without GPU for this partition set
    local partition_cpu_nodes=""
    local partition_gpu_nodes=""
    local partition_cluster_name="" # Will be set from NODE_INFO
    
    for node_info_item in "${NODE_INFO[@]}"; do
      local node_name=$(echo "$node_info_item" | cut -d: -f1)
      local node_gpus=$(echo "$node_info_item" | cut -d: -f2)
      partition_cluster_name=$(echo "$node_info_item" | cut -d: -f3) # Get cluster name from info
      
      # Add to CPU nodes list
      if [ -z "$partition_cpu_nodes" ]; then
        partition_cpu_nodes="$node_name"
      else
        partition_cpu_nodes="${partition_cpu_nodes},${node_name}"
      fi
      
      # If has GPUs, add to GPU nodes list
      if [ "$node_gpus" -gt 0 ]; then
        if [ -z "$partition_gpu_nodes" ]; then
          partition_gpu_nodes="$node_name"
        else
          partition_gpu_nodes="${partition_gpu_nodes},${node_name}"
        fi
      fi
    done
    
    # Determine Default= flag
    local default_flag="NO"
    if [ "$is_primary" -eq 1 ]; then
        default_flag="YES" # Only the primary cluster's CPU partition is default
    fi
    
    # Create CPU partition
    echo "PartitionName=${partition_cluster_name}-cpu Nodes=${partition_cpu_nodes} Default=${default_flag} MaxTime=INFINITE State=UP OverSubscribe=FORCE:32" | sudo tee -a /mnt/slurm_config/slurm.conf
    
    # Create GPU partition if needed
    if [ -n "$partition_gpu_nodes" ]; then
      # Note: GPU partition is never default in this script
      echo "PartitionName=${partition_cluster_name}-gpu Nodes=${partition_gpu_nodes} Default=NO MaxTime=INFINITE State=UP OverSubscribe=FORCE:32" | sudo tee -a /mnt/slurm_config/slurm.conf
    fi
  }

  #################################################
  # PART 1: SLURM CLUSTER SETUP
  #################################################
  echo "Setting up Slurm cluster..."

  # Generate munge key
  if [ -z "$CONTROLLER_IP" ]; then
    # This is the primary controller
    if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
      # Generate munge key on the primary controller head node
      sudo sh -c 'dd if=/dev/urandom bs=1 count=1024 > /etc/munge/munge.key'
      sudo chown munge:munge /etc/munge/munge.key
      sudo chmod 400 /etc/munge/munge.key

      # Get username - works on both AWS (ubuntu) and other clouds
      USERNAME=$(whoami)

      # Copy munge.key temporarily for scp
      sudo cp /etc/munge/munge.key /tmp/munge.key
      sudo chown $USERNAME:$USERNAME /tmp/munge.key

      # Now you can scp from /tmp/munge.key to worker nodes
      for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
        scp -o StrictHostKeyChecking=no /tmp/munge.key $USERNAME@$node:/tmp/munge.key
        ssh -o StrictHostKeyChecking=no $USERNAME@$node "sudo mv /tmp/munge.key /etc/munge/munge.key && sudo chown munge:munge /etc/munge/munge.key && sudo chmod 400 /etc/munge/munge.key"
      done

      # Keep the munge.key file in /tmp for secondary clusters to fetch
      # We'll clean it up later
      # sudo rm /tmp/munge.key
    fi
  else
    echo "Setting up secondary cluster as Slurm partition..."
    # Secondary clusters - get munge key from primary controller
    if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
      # Get munge key from primary controller
      rsync -avz --rsync-path="sudo rsync" $USERNAME@$CONTROLLER_IP:/tmp/munge.key /tmp/munge.key
      sudo mv /tmp/munge.key /etc/munge/munge.key
      sudo chown munge:munge /etc/munge/munge.key
      sudo chmod 400 /etc/munge/munge.key
      
      # Distribute to other nodes in this cluster
      for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
        # Copy munge key to a temporary location with right permissions for scp
        sudo cp /etc/munge/munge.key /tmp/munge.key.copy
        sudo chown $USERNAME:$USERNAME /tmp/munge.key.copy
        sudo chmod 600 /tmp/munge.key.copy
        scp -o StrictHostKeyChecking=no /tmp/munge.key.copy $USERNAME@$node:/tmp/munge.key
        sudo rm /tmp/munge.key.copy
        ssh -o StrictHostKeyChecking=no $node "sudo mv /tmp/munge.key /etc/munge/munge.key && sudo chown munge:munge /etc/munge/munge.key && sudo chmod 400 /etc/munge/munge.key"
      done
    fi
  fi
  
  # Create barrier to ensure munge key is distributed before continuing
  create_barrier "munge_key_distributed"

  sudo systemctl enable munge
  sudo systemctl restart munge

  echo "Nodes: $SKYPILOT_NODE_IPS"

  #################################################
  # PART 2: NFS SETUP FOR SHARED STORAGE AND SLURM CONFIG
  #################################################
  echo "Setting up NFS for shared storage..."

  # Install NFS on all nodes (server on primary controller, client on all)
  if [ -z "$CONTROLLER_IP" ]; then
    # Primary controller node: Install NFS server if not already installed
    if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
      if ! dpkg -l | grep -q nfs-kernel-server; then
        echo "Installing NFS server..."
        sudo apt install -y nfs-kernel-server
      else
        echo "NFS server already installed"
      fi

      # Create /mnt directory if it doesn't exist
      sudo mkdir -p /mnt
      sudo chmod 755 /mnt

      # Add /mnt to exports if not already added
      if ! grep -q "^/mnt " /etc/exports; then
        echo "Adding /mnt to exports..."
        echo "/mnt *(rw,sync,no_subtree_check,no_root_squash)" | sudo tee -a /etc/exports
        sudo exportfs -a
      else
        echo "/mnt already in exports"
        # Check for and remove duplicate entries if they exist
        if [ $(grep -c "^/mnt " /etc/exports) -gt 1 ]; then
          echo "Cleaning up duplicate exports..."
          # Create a temporary file with deduplicated entries
          grep -v "^/mnt " /etc/exports > /tmp/exports.tmp
          echo "/mnt *(rw,sync,no_subtree_check,no_root_squash)" >> /tmp/exports.tmp
          sudo cp /tmp/exports.tmp /etc/exports
          rm /tmp/exports.tmp
        fi
        # Re-export to ensure configuration is applied
        sudo exportfs -ra
      fi
      
      # Ensure NFS server is running
      sudo systemctl restart nfs-kernel-server
      
      # Create a /mnt/home_template directory to copy for new users
      sudo mkdir -p /mnt/home_template
      if [ ! "$(ls -A /mnt/home_template)" ]; then
        echo "Populating home template directory..."
        sudo cp -r /etc/skel/. /mnt/home_template/
      fi
      
      # Create Slurm config directory on NFS
      sudo mkdir -p /mnt/slurm_config
      sudo chmod 755 /mnt/slurm_config
      
      echo "NFS server setup complete on controller node"
    else
      # Worker nodes: Install NFS client only
      if ! dpkg -l | grep -q nfs-common; then
        echo "Installing NFS client..."
        sudo apt install -y nfs-common
      else
        echo "NFS client already installed"
      fi
    fi
  else
    # Secondary clusters: Install NFS client only
    if ! dpkg -l | grep -q nfs-common; then
      echo "Installing NFS client..."
      sudo apt install -y nfs-common
    else
      echo "NFS client already installed"
    fi
  fi

  # Configure NFS on all nodes
  if [ -z "$CONTROLLER_IP" ]; then
    if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
      # On primary controller, create a script that will be used to mount NFS on worker nodes
      cat > /tmp/mount_nfs.sh << EOF
  #!/bin/bash
  # Idempotent mount script with retries
  sudo mkdir -p /mnt

  # Function to attempt mount with retries
  mount_with_retry() {
      local max_attempts=5
      local attempt=1
      local wait_time=10  # seconds between attempts
      
      while [ $attempt -le $max_attempts ]; do
          echo "Mount attempt $attempt of $max_attempts..."
          if sudo mount ${PRIMARY_IP}:/mnt /mnt; then
              echo "NFS mount successful on attempt $attempt"
              return 0
          else
              echo "Mount attempt $attempt failed, waiting $wait_time seconds..."
              sleep $wait_time
              attempt=$((attempt + 1))
          fi
      done
      
      echo "Failed to mount NFS after $max_attempts attempts"
      return 1
  }

  # Check if already mounted
  if ! mount | grep -q " on /mnt type nfs"; then
      echo "Mounting NFS share..."
      mount_with_retry
  else
      echo "NFS share already mounted"
  fi

  # Check if already in fstab
  if ! grep -q "${PRIMARY_IP}:/mnt /mnt nfs" /etc/fstab; then
      echo "Adding NFS mount to fstab..."
      echo "${PRIMARY_IP}:/mnt /mnt nfs defaults 0 0" | sudo tee -a /etc/fstab
  else
      echo "NFS mount already in fstab"
  fi
  EOF
      chmod +x /tmp/mount_nfs.sh
      
      # Distribute to all worker nodes in primary cluster
      for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
        scp -o StrictHostKeyChecking=no /tmp/mount_nfs.sh $USERNAME@$node:/tmp/
        ssh -o StrictHostKeyChecking=no $USERNAME@$node "bash /tmp/mount_nfs.sh"
      done
      
      # Create and distribute the NFS setup barrier
      create_barrier "nfs_setup_complete"
    else
      # Worker node in primary cluster
      echo "Primary worker node: Waiting for head node NFS setup to complete..."
      create_barrier "nfs_setup_complete" # Worker waits here
      echo "Primary worker node: Head node NFS setup reported complete."

      sudo mkdir -p /mnt
      
      # Check if already mounted
      if ! mount | grep -q " on /mnt type nfs"; then
        echo "Mounting NFS share..."
        sudo mount ${PRIMARY_IP}:/mnt /mnt
      else
        echo "NFS share already mounted"
      fi

      # Check if already in fstab
      if ! grep -q "${PRIMARY_IP}:/mnt /mnt nfs" /etc/fstab; then
        echo "Adding NFS mount to fstab..."
        echo "${PRIMARY_IP}:/mnt /mnt nfs defaults 0 0" | sudo tee -a /etc/fstab
      else
        echo "NFS mount already in fstab"
      fi
    fi
  elif [ "$SKYPILOT_NODE_RANK" = "0" ]; then
    # Secondary cluster lead node - get mount script from controller
    scp -o StrictHostKeyChecking=no $USERNAME@$CONTROLLER_IP:/tmp/mount_nfs.sh /tmp/
    bash /tmp/mount_nfs.sh
    
    # Distribute to all worker nodes in secondary cluster
    for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
      scp -o StrictHostKeyChecking=no /tmp/mount_nfs.sh $USERNAME@$node:/tmp/
      ssh -o StrictHostKeyChecking=no $USERNAME@$node "bash /tmp/mount_nfs.sh"
    done
    
    # Create and distribute the NFS setup barrier
    create_barrier "nfs_setup_complete"
  fi
  
  echo "NFS setup complete. The /mnt directory is now shared across all nodes."
  
  #################################################
  # PART 3: SLURM CONFIGURATION
  #################################################
  echo "Setting up Slurm configuration..."
  
  # Create and configure Slurm
  if [ -z "$CONTROLLER_IP" ]; then
    echo "Setting up primary Slurm controller cluster..."
    if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
      # Primary controller setup
      sudo mkdir -p /var/spool/slurmctld
      sudo chown slurm:slurm /var/spool/slurmctld

      CONTROLLER_HOSTNAME=$(hostname)
      
      # Initialize slurm.conf in the NFS shared directory
      sudo bash -c "cat > /mnt/slurm_config/slurm.conf" << 'EOF'
  ClusterName=skypilot_cluster
  SlurmctldHost=CONTROLLER_HOSTNAME_PLACEHOLDER
  SlurmUser=slurm
  StateSaveLocation=/var/spool/slurmctld
  SlurmdSpoolDir=/var/spool/slurmd
  AuthType=auth/munge
  # Linear is required for support running multiple jobs on the same node
  SelectType=select/linear
  # Use Linux process tracking instead of cgroups
  ProctrackType=proctrack/linuxproc
  # Enable GPU support via Generic Resources (if GPUs are present)
  GresTypes=gpu
  # Enable accounting
  AccountingStorageType=accounting_storage/none
  JobAcctGatherType=jobacct_gather/none
  EOF

      # Replace the placeholder with actual hostname
      sudo sed -i "s/CONTROLLER_HOSTNAME_PLACEHOLDER/$CONTROLLER_HOSTNAME/" /mnt/slurm_config/slurm.conf

      # Create empty gres.conf to be populated if GPUs are detected
      echo "# GPU configuration for Slurm" | sudo tee /mnt/slurm_config/gres.conf
      
      # Set correct permissions for Slurm config files
      sudo chown slurm:slurm /mnt/slurm_config/slurm.conf /mnt/slurm_config/gres.conf
      sudo chmod 644 /mnt/slurm_config/slurm.conf /mnt/slurm_config/gres.conf
      
      # Create symlinks on controller
      sudo mkdir -p /etc/slurm # Ensure dir exists
      sudo ln -sf /mnt/slurm_config/slurm.conf /etc/slurm/slurm.conf
      sudo ln -sf /mnt/slurm_config/gres.conf /etc/slurm/gres.conf
      
      # Process nodes from this cluster
      RANK=0
      declare -a NODE_INFO=()
      
      # Loop through ALL nodes in the primary cluster
      while read -r node_ip; do
        # Call function to process node: process_node <node_ip> <rank> <is_secondary_cluster> <target_cluster_name>
        process_node "$node_ip" "$RANK" 0 "$CLUSTER_NAME"
        RANK=$((RANK+1))
      done <<< "$(echo "$SKYPILOT_NODE_IPS")" # Feed all IPs to the loop
      
      # Create partitions for the primary cluster
      create_partitions 1 # 1 indicates primary cluster
      
      # Start slurmctld on controller
      sudo systemctl enable slurmctld
      sudo systemctl restart slurmctld
    else
      # Worker node in primary cluster
      # Create symlinks to the NFS Slurm config
      sudo mkdir -p /etc/slurm
      sudo rm -f /etc/slurm/slurm.conf /etc/slurm/gres.conf
      sudo ln -sf /mnt/slurm_config/slurm.conf /etc/slurm/slurm.conf
      sudo ln -sf /mnt/slurm_config/gres.conf /etc/slurm/gres.conf
    fi
  else
    # This is a secondary cluster
    if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
      # Secondary cluster lead node
      
      # Ensure NFS is mounted
      if ! mount | grep -q " on /mnt type nfs"; then
        echo "ERROR: NFS not mounted! Cannot access shared Slurm configuration."
        exit 1
      fi
      
      # Verify we can access the Slurm config on NFS
      if [ ! -f /mnt/slurm_config/slurm.conf ]; then
        echo "ERROR: Cannot find Slurm configuration on NFS!"
        exit 1
      fi
      
      # Create symlinks to the NFS Slurm config
      sudo mkdir -p /etc/slurm
      sudo rm -f /etc/slurm/slurm.conf /etc/slurm/gres.conf
      sudo ln -sf /mnt/slurm_config/slurm.conf /etc/slurm/slurm.conf
      sudo ln -sf /mnt/slurm_config/gres.conf /etc/slurm/gres.conf
      
      # Process nodes for this cluster
      echo "Processing nodes for cluster: $CLUSTER_NAME"
      RANK=0
      declare -a NODE_INFO=()
      
      # --- Idempotency: Clean up existing config for this cluster ---
      echo "Removing existing configuration for cluster ${CLUSTER_NAME}..."
      # Remove nodes from slurm.conf
      sudo sed -i "/^NodeName=${CLUSTER_NAME}-node-.*/d" /mnt/slurm_config/slurm.conf
      # Remove partitions from slurm.conf
      sudo sed -i "/^PartitionName=${CLUSTER_NAME}-cpu.*/d" /mnt/slurm_config/slurm.conf
      sudo sed -i "/^PartitionName=${CLUSTER_NAME}-gpu.*/d" /mnt/slurm_config/slurm.conf
      # Remove nodes from gres.conf
      sudo sed -i "/^NodeName=${CLUSTER_NAME}-node-.*/d" /mnt/slurm_config/gres.conf
      echo "Cleanup complete."
      # --- End Idempotency ---

      while read -r node; do
        # Call function to process node: process_node <node_ip> <rank> <is_secondary_cluster> <target_cluster_name>
        echo "Processing node: $node"
        process_node "$node" "$RANK" 1 "$CLUSTER_NAME"
        RANK=$((RANK+1))
      done <<< "$(echo "$SKYPILOT_NODE_IPS")" # Feed all IPs to the loop
      
      # Create partitions for this secondary cluster
      create_partitions 0 # 0 indicates secondary cluster
      
      # Tell controller to reconfigure
      # Use scontrol reconfigure for a less disruptive update if possible, fallback to restart
      if ! ssh -o StrictHostKeyChecking=no $USERNAME@$CONTROLLER_IP "sudo scontrol reconfigure"; then
          echo "scontrol reconfigure failed, restarting slurmctld..."
          ssh -o StrictHostKeyChecking=no $USERNAME@$CONTROLLER_IP "sudo systemctl restart slurmctld"
      fi
      
      # Verify configuration was updated
      echo "Verifying configuration update:"
      if grep "${CLUSTER_NAME}" /mnt/slurm_config/slurm.conf; then
        echo "Successfully added $CLUSTER_NAME to Slurm configuration!"
      else
        echo "WARNING: Could not find $CLUSTER_NAME in slurm.conf!"
      fi
    else
      # Worker node in secondary cluster
      # Create symlinks to the NFS Slurm config
      sudo mkdir -p /etc/slurm
      sudo rm -f /etc/slurm/slurm.conf /etc/slurm/gres.conf
      sudo ln -sf /mnt/slurm_config/slurm.conf /etc/slurm/slurm.conf
      sudo ln -sf /mnt/slurm_config/gres.conf /etc/slurm/gres.conf
    fi
  fi
  
  # Create barrier to ensure slurm.conf is set up before continuing
  create_barrier "slurm_conf_distributed"

  # Set up slurmd on all nodes
  sudo mkdir -p /var/spool/slurmd
  sudo chown slurm:slurm /var/spool/slurmd
  sudo systemctl enable slurmd
  sudo systemctl restart slurmd
  
  # Show cluster configuration and GPU status if applicable
  echo "=== Slurm Cluster Status ==="
  sinfo
  
  if command -v nvidia-smi &> /dev/null; then
    echo "=== GPU Status ==="
    nvidia-smi
    echo "=== Available GPU Resources in Slurm ==="
    scontrol show nodes | grep "Gres="
    
    echo "=== Example GPU job submission ==="
    echo "To request a GPU job on this cluster: sbatch --gres=gpu:1 --partition=${CLUSTER_NAME}-gpu my_gpu_job.sh"
    echo "To request multiple GPUs: sbatch --gres=gpu:4 --partition=${CLUSTER_NAME}-gpu my_gpu_job.sh"
  else
    echo "=== Example CPU job submission ==="
    echo "To submit a job on this cluster: sbatch --partition=${CLUSTER_NAME}-cpu my_job.sh"
  fi

  #################################################
  # SUMMARY
  #################################################
  echo ""
  echo "=== DEPLOYMENT SUMMARY ==="
  if [ -z "$CONTROLLER_IP" ]; then
    echo "Primary Slurm controller cluster deployed: $CLUSTER_NAME"
  else
    echo "Secondary Slurm partition deployed: $CLUSTER_NAME"
    echo "Connected to primary controller IP: $CONTROLLER_IP"
  fi
  echo "Partition names: ${CLUSTER_NAME}-cpu and ${CLUSTER_NAME}-gpu (if GPUs available)"
  echo "Controller node: $CONTROLLER_IP"
  echo "NFS shared directory: /mnt"
  echo ""
  echo "Next step: Create users with 'sky exec <cluster> examples/slurm_cloud_deploy/add-users.yaml [--env USERS=\"user1 user2 user3\"]'"
  echo ""
  echo "To use the cluster:"
  echo " - Submit jobs with: sbatch --partition=${CLUSTER_NAME}-cpu <script.sh>"
  if command -v nvidia-smi &> /dev/null; then
    echo " - Submit GPU jobs with: sbatch --partition=${CLUSTER_NAME}-gpu --gres=gpu:1 <script.sh>"
  fi
  echo " - View queue with: squeue"
  echo " - Check node status with: sinfo"
  echo ""
  echo "All files placed in /mnt will be accessible from all nodes in all partitions"

  # Restart Slurm daemons across all nodes
  for node_info in "${NODE_INFO[@]}"; do
    node_name=$(echo "$node_info" | cut -d: -f1)
    
    # Skip self
    if [ "$node_name" != "$CONTROLLER_HOSTNAME" ]; then
      echo "Restarting slurmd on $node_name..."
      # Get the node IP from hostname
      node_ip=$(grep -w "$node_name" /mnt/slurm_config/slurm.conf | grep -oP 'NodeAddr=\K[^ ]+')
      ssh -o StrictHostKeyChecking=no $USERNAME@$node_ip "sudo systemctl restart slurmd"
    fi
  done
