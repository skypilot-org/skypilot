# Single node training a GPT style model with Nvidia NeMo
#
# This script downloads data from read-only bucket at gs://sky-wiki-data.
# If you want to preprocess the data yourself, see nemo_gpt_preprocessing.yaml.
#
# The specific model used here should fit on GPU with 16GB memory.
#
# After the script completes, the model checkpoints will be saved in
# /ckpts (configurable through CHECKPOINT_PATH env var) on the head node.
#
# Usage:
#   sky launch -c nemo_gpt nemo_gpt_singlenode.yaml
#
#   # Or try on spot A100 GPUs:
#   sky launch -c nemo_gpt nemo_gpt_singlenode.yaml --use-spot --gpus A100:1
#
#   # Terminate cluster after you're done
#   sky down nemo_gpt

resources:
  cpus: 8+
  memory: 64+
  accelerators: A100-80GB:1
  image_id: docker:nvcr.io/nvidia/nemo:24.05

num_nodes: 1

envs:
  DATASET_ROOT: /wiki
  CHECKPOINT_PATH: /ckpts


file_mounts:
  ${DATASET_ROOT}:
    source: gs://sky-wiki-data    # This is a read-only bucket provided by SkyPilot for the dataset
    mode: COPY

setup: |
  conda deactivate
  
  # Clone NeMo repo if not already present
  if [ ! -d NeMo ]; then
      git clone https://github.com/NVIDIA/NeMo.git
      cd NeMo 
      git checkout 5df8e11255802a2ce2f33db6362e60990e215b64
  fi
  
  # Install gsutil if it doesn't exist
  if ! command -v gsutil &> /dev/null
  then
      pip install gsutil
  else
      echo "gsutil exists"
  fi

run: |
  conda deactivate

  # Kill any existing megatron processes
  pkill -f -9 megatron
  
  mkdir -p ${CHECKPOINT_PATH}
  
  # ============= Training =============    
  python NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py  \
    --config-path=conf \
    --config-name=megatron_gpt_config \
    trainer.devices=${SKYPILOT_NUM_GPUS_PER_NODE} \
    trainer.num_nodes=1 \
    trainer.max_epochs=null \
    trainer.max_steps=300000 \
    trainer.val_check_interval=50 \
    trainer.log_every_n_steps=50 \
    trainer.limit_val_batches=50 \
    trainer.limit_test_batches=50 \
    trainer.accumulate_grad_batches=1 \
    trainer.precision=16 \
    model.mcore_gpt=True \
    model.micro_batch_size=6 \
    model.global_batch_size=192 \
    model.tensor_model_parallel_size=1 \
    model.pipeline_model_parallel_size=1 \
    model.max_position_embeddings=1024 \
    model.encoder_seq_length=1024 \
    model.hidden_size=768 \
    model.ffn_hidden_size=3072 \
    model.num_layers=12 \
    model.num_attention_heads=12 \
    model.init_method_std=0.021 \
    model.hidden_dropout=0.1 \
    model.layernorm_epsilon=1e-5 \
    model.tokenizer.vocab_file=${DATASET_ROOT}/gpt2-vocab.json \
    model.tokenizer.merge_file=${DATASET_ROOT}/gpt2-merges.txt \
    model.data.data_prefix=[1.0,${DATASET_ROOT}/hfbpe_gpt_training_data_text_document] \
    model.data.num_workers=2 \
    model.data.seq_length=1024 \
    model.data.splits_string=\'980,10,10\' \
    model.optim.name=fused_adam \
    model.optim.lr=6e-4 \
    model.optim.betas=[0.9,0.95] \
    model.optim.weight_decay=0.1 \
    model.optim.sched.name=CosineAnnealing \
    model.optim.sched.warmup_steps=750 \
    model.optim.sched.constant_steps=80000 \
    model.optim.sched.min_lr=6e-5 \
    exp_manager.resume_if_exists=True \
    exp_manager.resume_ignore_no_checkpoint=True \
    exp_manager.create_checkpoint_callback=True \
    +exp_manager.checkpoint_callback_params.dirpath=${CHECKPOINT_PATH} \
    exp_manager.checkpoint_callback_params.monitor=val_loss \
    exp_manager.checkpoint_callback_params.save_top_k=3 \
    exp_manager.checkpoint_callback_params.mode=min \
    exp_manager.checkpoint_callback_params.always_save_nemo=False
