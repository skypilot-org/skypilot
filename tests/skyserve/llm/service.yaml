# auth.yaml
envs:
  MODEL_NAME: Qwen/Qwen3-0.6B

secrets:
  AUTH_TOKEN: # TODO: Fill with your own auth token (a random string), or use --secret to pass.

service:
  readiness_probe:
    path: /v1/models
    headers:
      Authorization: Bearer $AUTH_TOKEN
    initial_delay_seconds: 1800
  replicas: 1

resources:
  accelerators: L4
  cpus: 7+
  memory: 20+
  ports: 8087

setup: |
  uv venv --python 3.10 --seed
  source .venv/bin/activate
  uv pip install vllm==0.10.0
  # Have to use triton==3.2.0 to avoid https://github.com/triton-lang/triton/issues/6698
  uv pip install triton==3.2.0
  # Pin transformers to avoid https://github.com/verl-project/verl/issues/4337
  uv pip install transformers==4.57.3
  uv pip install openai

run: |
  source .venv/bin/activate
  export PATH=$PATH:/sbin
  # Use XFORMERS backend to avoid Triton compilation issues on T4/Turing GPUs
  # See: https://github.com/vllm-project/vllm/issues/17639
  export VLLM_ATTENTION_BACKEND=XFORMERS
  export VLLM_USE_TRITON_FLASH_ATTN=0
  vllm serve $MODEL_NAME --trust-remote-code \
    --host 0.0.0.0 --port 8087 \
    --api-key $AUTH_TOKEN
