cluster_name: {{cluster_name}}

# The maximum number of workers nodes to launch in addition to the head node.
max_workers: {{num_nodes - 1}}
upscaling_speed: {{num_nodes - 1}}
idle_timeout_minutes: 60

provider:
    type: external
    module: sky.skylet.providers.azure.AzureNodeProvider
    location: {{region}}
    # Ref: https://github.com/ray-project/ray/blob/2367a2cb9033913b68b1230316496ae273c25b54/python/ray/autoscaler/_private/_azure/node_provider.py#L87
    # For Azure, ray distinguishes different instances by the resource_group,
    # instead of the cluster_name. This ensures that ray creates new instances
    # for different cluster_name.
    resource_group: {{resource_group}}
    # Keep (otherwise cannot reuse when re-provisioning).
    # teardown(terminate=True) will override this.
    cache_stopped_nodes: True
    # subscription id of the azure user
    subscription_id: {{azure_subscription_id}}
    # Disable launch config check for worker nodes as it can cause resource
    # leakage.
    # Reference: https://github.com/ray-project/ray/blob/cd1ba65e239360c8a7b130f991ed414eccc063ce/python/ray/autoscaler/_private/autoscaler.py#L1115
    # The upper-level SkyPilot code has make sure there will not be resource
    # leakage.
    disable_launch_config_check: true


auth:
    ssh_user: azureuser
    ssh_private_key: {{ssh_private_key}}

available_node_types:
    ray.head.default:
      resources: {}
      node_config:
        tags:
          skypilot-user: {{ user }}
        azure_arm_parameters:
            adminUsername: skypilot:ssh_user
            publicKey: |
              skypilot:ssh_public_key_content
            vmSize: {{instance_type}}
            # List images https://docs.microsoft.com/en-us/azure/virtual-machines/linux/cli-ps-findimage
            imagePublisher: {{image_publisher}}
            imageOffer: {{image_offer}}
            imageSku: "{{image_sku}}"
            imageVersion: {{image_version}}
            osDiskSizeGB: {{disk_size}}
            osDiskTier: {{disk_tier}}
            # optionally set priority to use Spot instances
            {%- if use_spot %}
            priority: Spot
            # set a maximum price for spot instances if desired
            # billingProfile:
            #     maxPrice: -1
            {%- endif %}
        # TODO: attach disk
{% if num_nodes > 1 %}
    ray.worker.default:
      min_workers: {{num_nodes - 1}}
      max_workers: {{num_nodes - 1}}
      resources: {}
      node_config:
        tags:
          skypilot-user: {{ user }}
        azure_arm_parameters:
            adminUsername: skypilot:ssh_user
            publicKey: |
              skypilot:ssh_public_key_content
            vmSize: {{instance_type}}
            # List images https://docs.microsoft.com/en-us/azure/virtual-machines/linux/cli-ps-findimage
            imagePublisher: {{image_publisher}}
            imageOffer: {{image_offer}}
            imageSku: "{{image_sku}}"
            imageVersion: {{image_version}}
            osDiskSizeGB: {{disk_size}}
            osDiskTier: {{disk_tier}}
          {%- if use_spot %}
            priority: Spot
            # set a maximum price for spot instances if desired
            # billingProfile:
            #     maxPrice: -1
          {%- endif %}
{%- endif %}

head_node_type: ray.head.default

# Format: `REMOTE_PATH : LOCAL_PATH`
file_mounts: {
  "{{sky_ray_yaml_remote_path}}": "{{sky_ray_yaml_local_path}}",
  "{{sky_remote_path}}/{{sky_wheel_hash}}": "{{sky_local_path}}",
{%- for remote_path, local_path in credentials.items() %}
  "{{remote_path}}": "{{local_path}}",
{%- endfor %}
}

rsync_exclude: []

initialization_commands: []

# List of shell commands to run to set up nodes.
# NOTE: these are very performance-sensitive. Each new item opens/closes an SSH
# connection, which is expensive. Try your best to co-locate commands into fewer
# items!
#
# Increment the following for catching performance bugs easier:
#   current num items (num SSH connections): 1
setup_commands:
  # Disable `unattended-upgrades` to prevent apt-get from hanging. It should be called at the beginning before the process started to avoid being blocked. (This is a temporary fix.)
  # Create ~/.ssh/config file in case the file does not exist in the image.
  # Make sure python3 & pip3 are available on this image.
  # Line 'sudo bash ..': set the ulimit as suggested by ray docs for performance. https://docs.ray.io/en/latest/cluster/vms/user-guides/large-cluster-best-practices.html#system-configuration
  # Line 'sudo grep ..': set the number of threads per process to unlimited to avoid ray job submit stucking issue when the number of running ray jobs increase.
  # Line 'mkdir -p ..': disable host key check
  # Line 'python3 -c ..': patch the buggy ray files and enable `-o allow_other` option for `goofys`
  # This also kills the service that is holding the lock on dpkg (problem only exists on aws/azure, not gcp)
  - function mylsof { p=$(for pid in /proc/{0..9}*; do i=$(basename "$pid"); for file in "$pid"/fd/*; do link=$(readlink -e "$file"); if [ "$link" = "$1" ]; then echo "$i"; fi; done; done); echo "$p"; };
    sudo systemctl stop unattended-upgrades || true;
    sudo systemctl disable unattended-upgrades || true;
    sudo sed -i 's/Unattended-Upgrade "1"/Unattended-Upgrade "0"/g' /etc/apt/apt.conf.d/20auto-upgrades || true;
    p=$(mylsof "/var/lib/dpkg/lock-frontend"); echo "$p";
    sudo kill -9 `echo "$p" | tail -n 1` || true;
    sudo rm /var/lib/dpkg/lock-frontend;
    sudo pkill -9 dpkg;
    sudo pkill -9 apt-get;
    sudo dpkg --configure --force-overwrite -a;
    mkdir -p ~/.ssh; touch ~/.ssh/config;
    pip3 --version > /dev/null 2>&1 || (curl -sSL https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python3 get-pip.py && echo "PATH=$HOME/.local/bin:$PATH" >> ~/.bashrc);
    (type -a python | grep -q python3) || echo 'alias python=python3' >> ~/.bashrc;
    (type -a pip | grep -q pip3) || echo 'alias pip=pip3' >> ~/.bashrc;
    {{ conda_installation_commands }}
    source ~/.bashrc;
    (pip3 list | grep "ray " | grep {{ray_version}} 2>&1 > /dev/null || pip3 install --exists-action w -U ray[default]=={{ray_version}}) && mkdir -p ~/sky_workdir && mkdir -p ~/.sky/sky_app && touch ~/.sudo_as_admin_successful;
    (pip3 list | grep "skypilot " && [ "$(cat {{sky_remote_path}}/current_sky_wheel_hash)" == "{{sky_wheel_hash}}" ]) || (pip3 uninstall skypilot -y; pip3 install "$(echo {{sky_remote_path}}/{{sky_wheel_hash}}/skypilot-{{sky_version}}*.whl)[azure]" && echo "{{sky_wheel_hash}}" > {{sky_remote_path}}/current_sky_wheel_hash || exit 1);
    sudo bash -c 'rm -rf /etc/security/limits.d; echo "* soft nofile 1048576" >> /etc/security/limits.conf; echo "* hard nofile 1048576" >> /etc/security/limits.conf';
    sudo grep -e '^DefaultTasksMax' /etc/systemd/system.conf || (sudo bash -c 'echo "DefaultTasksMax=infinity" >> /etc/systemd/system.conf'); sudo systemctl set-property user-$(id -u $(whoami)).slice TasksMax=infinity; sudo systemctl daemon-reload;
    mkdir -p ~/.ssh; (grep -Pzo -q "Host \*\n  StrictHostKeyChecking no" ~/.ssh/config) || printf "Host *\n  StrictHostKeyChecking no\n" >> ~/.ssh/config;
    python3 -c "from sky.skylet.ray_patches import patch; patch()" || exit 1;
    [ -f /etc/fuse.conf ] && sudo sed -i 's/#user_allow_other/user_allow_other/g' /etc/fuse.conf || (sudo sh -c 'echo "user_allow_other" > /etc/fuse.conf');

# Command to start ray on the head node. You don't need to change this.
# NOTE: these are very performance-sensitive. Each new item opens/closes an SSH
# connection, which is expensive. Try your best to co-locate commands into fewer
# items! The same comment applies for worker_start_ray_commands.
#
# Increment the following for catching performance bugs easier:
#   current num items (num SSH connections): 2
head_start_ray_commands:
  # Start skylet daemon. (Should not place it in the head_setup_commands, otherwise it will run before skypilot is installed.)
  # NOTE: --disable-usage-stats in `ray start` saves 10 seconds of idle wait.
  - ((ps aux | grep -v nohup | grep -v grep | grep -q -- "python3 -m sky.skylet.skylet") || nohup python3 -m sky.skylet.skylet >> ~/.sky/skylet.log 2>&1 &);
    ray stop; RAY_SCHEDULER_EVENTS=0 RAY_DEDUP_LOGS=0 ray start --disable-usage-stats --head --port={{ray_port}} --dashboard-port={{ray_dashboard_port}} --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml {{"--resources='%s'" % custom_resources if custom_resources}} --temp-dir {{ray_temp_dir}} || exit 1;
    which prlimit && for id in $(pgrep -f raylet/raylet); do sudo prlimit --nofile=1048576:1048576 --pid=$id || true; done;
    {{dump_port_command}};

{%- if num_nodes > 1 %}
worker_start_ray_commands:
  - ray stop; RAY_SCHEDULER_EVENTS=0 RAY_DEDUP_LOGS=0 ray start --disable-usage-stats --address=$RAY_HEAD_IP:{{ray_port}} --object-manager-port=8076 {{"--resources='%s'" % custom_resources if custom_resources}} --temp-dir {{ray_temp_dir}} || exit 1;
    which prlimit && for id in $(pgrep -f raylet/raylet); do sudo prlimit --nofile=1048576:1048576 --pid=$id || true; done;
{%- else %}
worker_start_ray_commands: []
{%- endif %}

head_node: {}
worker_nodes: {}

# These fields are required for external cloud providers.
head_setup_commands: []
worker_setup_commands: []
cluster_synced_files: []
file_mounts_sync_continuously: False
