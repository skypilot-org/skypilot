# The template for the jobs controller

name: {{dag_name}}

file_mounts:
  {%- if pre_created_job_ids is defined and pre_created_job_ids is not none %}
  {{remote_original_user_yaml_path}}: {{original_user_dag_path}}
  {{remote_user_yaml_path}}: {{user_yaml_path}}
  {%- else %}
  {{remote_original_user_yaml_path}}: {{original_user_dag_path}}
  {{remote_user_yaml_path}}: {{user_yaml_path}}
  {%- endif %}
  {%- if local_user_config_path is not none %}
  {{remote_user_config_path}}: {{local_user_config_path}}
  {%- endif %}
  {%- for remote_catalog_path, local_catalog_path in modified_catalogs.items() %}
  {{remote_catalog_path}}: {{local_catalog_path}}
  {%- endfor %}
  {%- for controller_file_mount_path, local_file_mount_path in local_to_controller_file_mounts.items() %}
  {{controller_file_mount_path}}: {{local_file_mount_path}}
  {%- endfor %}

# NOTE(dev): This needs to be a subset of sky/templates/sky-serve-controller.yaml.j2.
# It is because we use the --fast flag to submit jobs and no --fast flag to launch pools.
# So when we launch a new pool, it will install the required dependencies.
# TODO(tian): Add --fast to launch pools as well, and figure out the dependency installation.
# Maybe in the --fast implementation, we can store the hash of setup commands that used to be
# run and don't skip setup phase if the hash is different.
setup: |
  {{ sky_activate_python_env }}
  # Disable the pip version check to avoid the warning message, which makes the
  # output hard to read.
  export PIP_DISABLE_PIP_VERSION_CHECK=1

  {%- for cmd in cloud_dependencies_installation_commands %}
  {{cmd}}
  {%- endfor %}

  {% if controller_envs.get('SKYPILOT_DEV') != '0' %}
  grep -q 'export SKYPILOT_DEV=' ~/.bashrc || echo 'export SKYPILOT_DEV=1' >> ~/.bashrc
  grep -q 'alias sky-env=' ~/.bashrc || echo 'alias sky-env="{{ sky_activate_python_env }}"' >> ~/.bashrc
  {% endif %}

  # This is used by the skylet events to check if we are a jobs controller.
  touch {{job_controller_indicator_file}}

run: |
  {%- if consolidation_mode_job_ids is none %}
  {{ sky_activate_python_env }}
  {%- endif %}

  # Write env vars to a file
  {%- for env_name, env_value in controller_envs.items() %}
  echo "export {{env_name}}='{{env_value}}'" >> {{remote_env_file_path}}
  {%- endfor %}

  {%- if job_id_to_rank is defined and job_id_to_rank is not none %}
  # Write job_id_to_rank mapping to env file for rank lookup
  {%- if consolidation_mode_job_ids is defined and consolidation_mode_job_ids is not none %}
  # For consolidation mode, job_id_to_rank is already fully set with all job IDs
  export _TMP_JOB_ID_TO_RANK='{{job_id_to_rank | tojson}}'
  python3 -c "import json, os, sys; d=json.loads(os.environ['_TMP_JOB_ID_TO_RANK']); sys.stdout.write('export SKYPILOT_JOB_ID_TO_RANK='); json.dump(d, sys.stdout); sys.stdout.write('\n')" >> {{remote_env_file_path}} 2>&1
  unset _TMP_JOB_ID_TO_RANK
  {%- else %}
  # For pre_created_job_ids mode, add the controller task's ray job ID to the
  # job_id_to_rank dictionary. The last rank is num_jobs - 1
  if [ -n "$SKYPILOT_INTERNAL_JOB_ID" ]; then
    export _TMP_JOB_ID_TO_RANK='{{job_id_to_rank | tojson}}'
    python3 -c "import json, os, sys; d=json.loads(os.environ['_TMP_JOB_ID_TO_RANK']); d[str(os.environ['SKYPILOT_INTERNAL_JOB_ID'])]={{num_jobs - 1}}; sys.stdout.write('export SKYPILOT_JOB_ID_TO_RANK='); json.dump(d, sys.stdout); sys.stdout.write('\n')" >> {{remote_env_file_path}} 2>&1
    unset _TMP_JOB_ID_TO_RANK
  else
    echo "Warning: SKYPILOT_INTERNAL_JOB_ID not set, cannot add to job_id_to_rank" >> {{remote_env_file_path}} 2>&1
  fi
  {%- endif %}
  {%- endif %}

  # Submit the job(s) to the scheduler.
  # Note: The job is already in the `spot` table, marked as PENDING.
  # CloudVmRayBackend._exec_code_on_head() calls
  # managed_job_codegen.set_pending() before we get here.
  {%- if pre_created_job_ids is defined and pre_created_job_ids is not none %}
  # Batch submission: loop through pre-created job IDs and append the controller
  # task's ray job ID ($SKYPILOT_INTERNAL_JOB_ID) as the last job ID
  job_ids_array=({% for job_id in pre_created_job_ids %}{{job_id}} {% endfor %}$SKYPILOT_INTERNAL_JOB_ID)
  for job_id in "${job_ids_array[@]}"; do
    python \
      -u -m sky.jobs.scheduler {{remote_user_yaml_path}} \
      --user-yaml-path {{remote_original_user_yaml_path}} \
      --job-id $job_id \
      --env-file {{remote_env_file_path}} \
      {%- if pool is not none %}
      --pool {{pool}} \
      {%- endif %}
      --priority {{priority}}
  done
  {%- elif consolidation_mode_job_ids is defined and consolidation_mode_job_ids is not none %}
  # Consolidation mode batch submission: loop through all consolidation mode job IDs
  job_ids_array=({% for job_id in consolidation_mode_job_ids %}{{job_id}} {% endfor %})
  for job_id in "${job_ids_array[@]}"; do
    python \
      -u -m sky.jobs.scheduler {{remote_user_yaml_path}} \
      --user-yaml-path {{remote_original_user_yaml_path}} \
      --job-id $job_id \
      --env-file {{remote_env_file_path}} \
      {%- if pool is not none %}
      --pool {{pool}} \
      {%- endif %}
      --priority {{priority}}
  done
  {%- elif consolidation_mode_job_id is not none %}
  {{sky_python_cmd}} \
    -u -m sky.jobs.scheduler {{remote_user_yaml_path}} \
    --user-yaml-path {{remote_original_user_yaml_path}} \
    --job-id {{consolidation_mode_job_id}} \
    --env-file {{remote_env_file_path}} \
    {%- if pool is not none %}
    --pool {{pool}} \
    {%- endif %}
    --priority {{priority}}
  {%- else %}
  python \
    -u -m sky.jobs.scheduler {{remote_user_yaml_path}} \
    --user-yaml-path {{remote_original_user_yaml_path}} \
    --job-id $SKYPILOT_INTERNAL_JOB_ID \
    --env-file {{remote_env_file_path}} \
    {%- if pool is not none %}
    --pool {{pool}} \
    {%- endif %}
    --priority {{priority}}
  {%- endif %}


envs:
{%- for env_name, env_value in controller_envs.items() %}
  {{env_name}}: {{env_value}}
{%- endfor %}
