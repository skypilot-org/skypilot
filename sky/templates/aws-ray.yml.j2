cluster_name: {{cluster_name}}

# The maximum number of workers nodes to launch in addition to the head node.
max_workers: {{num_nodes - 1}}
upscaling_speed: {{num_nodes - 1}}
idle_timeout_minutes: 60

provider:
  type: aws
  region: {{region}}
  availability_zone: {{zones}}
  # Keep (otherwise cannot reuse when re-provisioning).
  # teardown(terminate=True) will override this.
  cache_stopped_nodes: True

auth:
  ssh_user: ubuntu

available_node_types:
  ray.head.default:
    resources: {}
    node_config:
      InstanceType: {{instance_type}}
      ImageId: {{aws_default_ami}}  # Deep Learning AMI (Ubuntu 18.04); see aws.py.
      BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
            VolumeSize: {{disk_size}}
      {% if use_spot %}
      InstanceMarketOptions:
          MarketType: spot
          # Additional options can be found in the boto docs, e.g.
          #   SpotOptions:
          #       MaxPrice: MAX_HOURLY_PRICE
      {% endif %}
{% if num_nodes > 1 %}
  ray.worker.default:
    min_workers: {{num_nodes - 1}}
    max_workers: {{num_nodes - 1}}
    resources: {}
    node_config:
      InstanceType: {{instance_type}}
      ImageId: {{aws_default_ami}}  # Deep Learning AMI (Ubuntu 18.04); see aws.py.
      BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
            VolumeSize: {{disk_size}}
      {% if use_spot %}
      InstanceMarketOptions:
          MarketType: spot
          # Additional options can be found in the boto docs, e.g.
          #   SpotOptions:
          #       MaxPrice: MAX_HOURLY_PRICE
      {% endif %}
{%- endif %}

head_node_type: ray.head.default

# Format: `REMOTE_PATH : LOCAL_PATH`
file_mounts: {
  "{{sky_remote_path}}": "{{sky_local_path}}",
{%- for remote_path, local_path in credentials.items() %}
  "{{remote_path}}": "{{local_path}}",
{%- endfor %}
}

rsync_exclude: [
{%- for exclude_file in credential_excludes %}
  "{{exclude_file}}",
{%- endfor %}
]

# List of shell commands to run to set up nodes.
setup_commands:
  # This AMI's system Python is version 2+.
  - pip3 install -U ray[default]=={{ray_version}} && mkdir -p ~/sky_workdir && mkdir -p ~/.sky/sky_app
  - pip3 uninstall sky -y &> /dev/null; pip3 install {{sky_remote_path}}/*.whl && python3 -c "from sky.skylet.ray_patches import patch; patch()" # patch the buggy ray file
  - sudo kill -9 `sudo lsof /var/lib/dpkg/lock-frontend | awk '{print $2}' | tail -n 1`;
      sudo pkill -9 apt-get;
      sudo pkill -9 dpkg;
      sudo dpkg --configure -a;

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
  # Set the ulimit as suggested by ray docs for performance. https://docs.ray.io/en/latest/cluster/guide.html?highlight=ulimit#system-configuration
  # Solution from https://discuss.ray.io/t/setting-ulimits-on-ec2-instances/590
  # This line is intentionally separated from the next line to reload the session after the ulimit is set.
  - sudo bash -c 'rm -rf /etc/security/limits.d; echo "* soft nofile 65535" >> /etc/security/limits.conf; echo "* hard nofile 65535" >> /etc/security/limits.conf;'
  - ray stop; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml {{"--resources='%s'" % custom_resources if custom_resources}}

{%- if num_nodes > 1 %}
worker_start_ray_commands:
  - sudo bash -c 'rm -rf /etc/security/limits.d; echo "* soft nofile 65535" >> /etc/security/limits.conf; echo "* hard nofile 65535" >> /etc/security/limits.conf;'
  - ray stop; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 {{"--resources='%s'" % custom_resources if custom_resources}}
{%- endif %}
