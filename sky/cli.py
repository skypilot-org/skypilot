"""The 'sky' command line tool.

Example usage:

  # See available commands.
  >> sky

  # Run a task, described in a yaml file.
  # Provisioning, setup, file syncing are handled.
  >> sky launch task.yaml
  >> sky launch [-c cluster_name] task.yaml

  # Show the list of running clusters.
  >> sky status

  # Tear down a specific cluster.
  >> sky down cluster_name

  # Tear down all existing clusters.
  >> sky down -a

TODO:
- Add support for local Docker backend.  Currently this module is very coupled
  with CloudVmRayBackend, as seen by the many use of ray commands.

NOTE: the order of command definitions in this file corresponds to how they are
listed in "sky --help".  Take care to put logically connected commands close to
each other.
"""
import copy
import functools
import getpass
import shlex
import time
from typing import Any, Dict, List, Optional, Tuple
import yaml

import click
import pendulum
from rich.console import Console

import sky
from sky import backends
from sky import check as sky_check
from sky import global_user_state
from sky import sky_logging
from sky import clouds
from sky.backends import backend as backend_lib
from sky.backends import backend_utils
from sky.backends import cloud_vm_ray_backend
from sky.clouds import service_catalog
from sky.skylet import util_lib

logger = sky_logging.init_logger(__name__)
console = Console()

_CLUSTER_FLAG_HELP = """\
A cluster name. If provided, either reuse an existing cluster with that name or
provision a new cluster with that name. Otherwise provision a new cluster with
an autogenerated name."""
_INTERACTIVE_NODE_TYPES = ('cpunode', 'gpunode', 'tpunode')
_INTERACTIVE_NODE_DEFAULT_RESOURCES = {
    'cpunode': sky.Resources(cloud=None,
                             instance_type=None,
                             accelerators=None,
                             use_spot=False),
    'gpunode': sky.Resources(cloud=None,
                             instance_type=None,
                             accelerators={'K80': 1},
                             use_spot=False),
    'tpunode': sky.Resources(cloud=sky.GCP(),
                             instance_type=None,
                             accelerators={'tpu-v3-8': 1},
                             accelerator_args={'tf_version': '2.5.0'},
                             use_spot=False),
}


def _truncate_long_string(s: str, max_length: int = 50) -> str:
    if len(s) <= max_length:
        return s
    splits = s.split(' ')
    if len(splits[0]) > max_length:
        return splits[0][:max_length] + '...'  # Use 'â€¦'?
    # Truncate on word boundary.
    i = 0
    total = 0
    for i, part in enumerate(splits):
        total += len(part)
        if total >= max_length:
            break
    return ' '.join(splits[:i]) + ' ...'


def _parse_accelerator_options(accelerator_options: str) -> Dict[str, float]:
    """Parses accelerator options: e.g., V100:8 into {'V100': 8}."""
    accelerators = {}
    accelerator_options = accelerator_options.split(':')
    if len(accelerator_options) == 1:
        accelerator_type = accelerator_options[0]
        accelerators[accelerator_type] = 1
    elif len(accelerator_options) == 2:
        accelerator_type = accelerator_options[0]
        accelerator_count = float(accelerator_options[1])
        accelerators[accelerator_type] = accelerator_count
    else:
        raise ValueError(f'Invalid accelerator option: {accelerator_options}')
    return accelerators


def _interactive_node_cli_command(cli_func):
    """Click command decorator for interactive node commands."""
    assert cli_func.__name__ in _INTERACTIVE_NODE_TYPES, cli_func.__name__

    cluster_option = click.option('--cluster',
                                  '-c',
                                  default=None,
                                  type=str,
                                  required=False,
                                  help=_CLUSTER_FLAG_HELP)
    port_forward_option = click.option(
        '--port-forward',
        '-p',
        multiple=True,
        default=[],
        type=int,
        required=False,
        help=('Port to be forwarded. To forward multiple ports, '
              'use this option multiple times.'))
    screen_option = click.option('--screen',
                                 default=False,
                                 is_flag=True,
                                 help='If true, attach using screen.')
    tmux_option = click.option('--tmux',
                               default=False,
                               is_flag=True,
                               help='If true, attach using tmux.')
    cloud_option = click.option('--cloud',
                                default=None,
                                type=str,
                                help='Cloud provider to use.')
    instance_type_option = click.option('--instance-type',
                                        '-t',
                                        default=None,
                                        type=str,
                                        help='Instance type to use.')
    gpus = click.option('--gpus',
                        default=None,
                        type=str,
                        help=('Type and number of GPUs to use '
                              '(e.g., --gpus=V100:8 or --gpus=V100).'))
    tpus = click.option(
        '--tpus',
        default=None,
        type=str,
        help=('Type and number of TPUs to use (e.g., --tpus=tpu-v3-8:4 or '
              '--tpus=tpu-v3-8).'))

    spot_option = click.option('--spot',
                               default=None,
                               is_flag=True,
                               help='If true, use spot instances.')

    disk_size = click.option('--disk-size',
                             default=None,
                             type=int,
                             required=False,
                             help=('OS disk size in GBs.'))

    click_decorators = [
        cli.command(cls=_DocumentedCodeCommand),
        cluster_option,
        port_forward_option,

        # Resource options
        *([cloud_option] if cli_func.__name__ != 'tpunode' else []),
        instance_type_option,
        *([gpus] if cli_func.__name__ == 'gpunode' else []),
        *([tpus] if cli_func.__name__ == 'tpunode' else []),
        spot_option,

        # Attach options
        screen_option,
        tmux_option,
        disk_size,
    ]
    decorator = functools.reduce(lambda res, f: f(res),
                                 reversed(click_decorators), cli_func)

    return decorator


def _default_interactive_node_name(node_type: str):
    """Returns a deterministic name to refer to the same node."""
    # FIXME: this technically can collide in Azure/GCP with another
    # same-username user.  E.g., sky-gpunode-ubuntu.  Not a problem on AWS
    # which is the current cloud for interactive nodes.
    assert node_type in _INTERACTIVE_NODE_TYPES, node_type
    return f'sky-{node_type}-{getpass.getuser()}'


def _infer_interactive_node_type(resources: sky.Resources):
    """Determine interactive node type from resources."""
    accelerators = resources.get_accelerators()
    cloud = resources.cloud
    if accelerators:
        # We only support homogenous accelerators for now.
        assert len(accelerators) == 1, resources
        acc, _ = list(accelerators.items())[0]
        is_gcp = cloud is not None and cloud.is_same_cloud(sky.GCP())
        if is_gcp and 'tpu' in acc:
            return 'tpunode'
        return 'gpunode'
    return 'cpunode'


def _check_interactive_node_resources_match(
        node_type: str,
        resources: sky.Resources,
        launched_resources: sky.Resources,
        user_requested_resources: Optional[bool] = False) -> None:
    """Check matching resources when reusing an existing cluster.

    The only exception is when [cpu|tpu|gpu]node -c cluster_name is used with no
    additional arguments, then login succeeds.

    Args:
        node_type: Type of interactive node.
        resources: Resources to attach to VM.
        launched_resources: Existing launched resources associated with cluster.
        user_requested_resources: If true, user requested resources explicitly.
    """
    # In the case where the user specifies no cloud, we infer the cloud from
    # the launched resources before performing the check.
    # e.g. user launches sky gpunode --gpus V100 creates an AWS(p3.2xlarge)
    # but when the resource check will fail because is_same_resources expects
    # resources.cloud and handle.launched_resources to match.
    if resources.cloud is None:
        assert launched_resources.cloud is not None, launched_resources
        resources.cloud = launched_resources.cloud

    # TODO: Check for same number of launched_nodes if multi-node support is
    # added for gpu/cpu/tpunode.
    inferred_node_type = _infer_interactive_node_type(launched_resources)
    node_type_match = inferred_node_type == node_type
    launched_resources_match = resources.is_same_resources(launched_resources)
    no_resource_requests = not user_requested_resources  # e.g. sky gpunode
    if not (node_type_match and
            (no_resource_requests or launched_resources_match)):
        raise click.UsageError(
            'Resources cannot change for an existing cluster.\n'
            f'Existing: {inferred_node_type} with {launched_resources}\n'
            f'Requested: {node_type} with {resources}\n')


# TODO: skip installing ray to speed up provisioning.
def _create_and_ssh_into_node(
    node_type: str,
    resources: sky.Resources,
    cluster_name: str,
    backend: Optional[backend_lib.Backend] = None,
    port_forward: Optional[List[int]] = None,
    session_manager: Optional[str] = None,
    user_requested_resources: Optional[bool] = False,
):
    """Creates and attaches to an interactive node.

    Args:
        node_type: Type of the interactive node: { 'cpunode', 'gpunode' }.
        resources: Resources to attach to VM.
        cluster_name: a cluster name to identify the interactive node.
        backend: the Backend to use (currently only CloudVmRayBackend).
        port_forward: List of ports to forward.
        session_manager: Attach session manager: { 'screen', 'tmux' }.
        user_requested_resources: If true, user requested resources explicitly.
    """
    assert node_type in _INTERACTIVE_NODE_TYPES, node_type
    assert session_manager in (None, 'screen', 'tmux'), session_manager
    with sky.Dag() as dag:
        # TODO: Add conda environment replication
        # should be setup =
        # 'conda env export | grep -v "^prefix: " > environment.yml'
        # && conda env create -f environment.yml
        task = sky.Task(
            node_type,
            workdir=None,
            setup=None,
            run='',
        )
        task.set_resources(resources)
        task.update_file_mounts(sky_check.get_cloud_credential_file_mounts())

    backend = backend if backend is not None else backends.CloudVmRayBackend()
    handle = global_user_state.get_handle_from_cluster_name(cluster_name)
    if handle is not None:
        # Avoid reusing clusters with requested resource mismatches.
        _check_interactive_node_resources_match(node_type, resources,
                                                handle.launched_resources,
                                                user_requested_resources)
    if handle is None or handle.head_ip is None:
        # head_ip would be None if previous provisioning failed.

        # This handles stopped interactive nodes where they are restarted by
        # skipping sky start and directly calling sky [cpu|tpu|gpu]node.
        cluster_status = global_user_state.get_status_from_cluster_name(
            cluster_name)
        if cluster_status == global_user_state.ClusterStatus.STOPPED:
            assert handle.launched_resources is not None, handle
            to_provision = None
            task.set_resources(handle.launched_resources)
            task.num_nodes = handle.launched_nodes
        else:
            dag = sky.optimize(dag)
            task = dag.tasks[0]
            backend.register_info(dag=dag)
            to_provision = task.best_resources

        handle = backend.provision(task,
                                   to_provision=to_provision,
                                   dryrun=False,
                                   stream_logs=False,
                                   cluster_name=cluster_name)

    # Use ssh rather than 'ray attach' to suppress ray messages, speed up
    # connection, and for allowing adding 'cd workdir' in the future.
    # Disable check, since the returncode could be non-zero if the user Ctrl-D.
    commands = []
    if session_manager == 'screen':
        commands += ['screen', '-D', '-R']
    elif session_manager == 'tmux':
        commands += ['tmux', 'attach', '||', 'tmux', 'new']
    commands = backend.run_on_head(handle,
                                   commands,
                                   port_forward=port_forward,
                                   ssh_mode=backend_utils.SshMode.LOGIN)
    cluster_name = global_user_state.get_cluster_name_from_handle(handle)

    click.echo('To attach to it again:  ', nl=False)
    if cluster_name == _default_interactive_node_name(node_type):
        option = ''
    else:
        option = f' -c {cluster_name}'
    click.secho(f'sky {node_type}{option}', bold=True)
    click.echo('To stop the node:\t', nl=False)
    click.secho(f'sky stop {cluster_name}', bold=True)
    click.echo('To tear down the node:\t', nl=False)
    click.secho(f'sky down {cluster_name}', bold=True)
    click.echo('To upload a folder:\t', nl=False)
    click.secho(f'rsync -rP /local/path {cluster_name}:/remote/path', bold=True)
    click.echo('To download a folder:\t', nl=False)
    click.secho(f'rsync -rP {cluster_name}:/remote/path /local/path', bold=True)


def _check_yaml(entrypoint: str) -> bool:
    """Checks if entrypoint is a readable YAML file."""
    is_yaml = True
    try:
        with open(entrypoint, 'r') as f:
            try:
                config = yaml.safe_load(f)
                if isinstance(config, str):
                    # 'sky exec cluster ./my_script.sh'
                    is_yaml = False
            except yaml.YAMLError:
                is_yaml = False
    except OSError:
        is_yaml = False
    if not is_yaml:
        shell_splits = shlex.split(entrypoint)
        if len(shell_splits) == 1 and (shell_splits[0].endswith('.yaml') or
                                       shell_splits[0].endswith('.yml')):
            click.confirm(
                f'{entrypoint!r} looks like a yaml path but yaml.safe_load() '
                'failed to return a dict (check if it exists or it\'s valid).\n'
                'It will be treated as a command to be run remotely. Continue?',
                abort=True)
    return is_yaml


class _NaturalOrderGroup(click.Group):
    """Lists commands in the order they are defined in this script.

    Reference: https://github.com/pallets/click/issues/513
    """

    def list_commands(self, ctx):
        return self.commands.keys()


class _DocumentedCodeCommand(click.Command):
    """Corrects help strings for documented commands such that --help displays
    properly and code blocks are rendered in the official web documentation.
    """

    def get_help(self, ctx):
        help_str = ctx.command.help
        ctx.command.help = help_str.replace('.. code-block:: bash\n', '\b')
        return super().get_help(ctx)


@click.group(cls=_NaturalOrderGroup)
def cli():
    pass


@cli.command(cls=_DocumentedCodeCommand)
@click.argument('entrypoint', required=True, type=str, nargs=-1)
@click.option('--cluster',
              '-c',
              default=None,
              type=str,
              help=_CLUSTER_FLAG_HELP)
@click.option('--dryrun',
              default=False,
              is_flag=True,
              help='If True, do not actually run the job.')
@click.option('--detach-run',
              '-d',
              default=False,
              is_flag=True,
              help='If True, run setup first (blocking), '
              'then detach from the job\'s execution.')
@click.option('--docker',
              'backend_name',
              flag_value=backends.LocalDockerBackend.NAME,
              default=False,
              help='If used, runs locally inside a docker container.')
@click.option(
    '--workdir',
    required=False,
    type=click.Path(exists=True, file_okay=False),
    help=('If specified, sync this dir to the remote working directory, '
          'where the task will be invoked. '
          'Overrides the "workdir" config in the YAML if both are supplied.'))
@click.option(
    '--cloud',
    required=False,
    type=str,
    help='The cloud to use. If specified, override the "resources.cloud".')
@click.option(
    '--gpus',
    required=False,
    type=str,
    help=('Type and number of GPUs to use. Example values: '
          '"V100:8", "V100" (short for a count of 1), or "V100:0.5" '
          '(fractional counts are supported by the scheduling framework). '
          'If a new cluster is being launched by this command, this is the '
          'resources to provision. If an existing cluster is being reused, this'
          ' is seen as the task demand, which must fit the cluster\'s total '
          'resources and is used for scheduling the task. '
          'Overrides the "accelerators" '
          'config in the YAML if both are supplied.'))
@click.option(
    '--use-spot/--no-use-spot',
    required=False,
    default=None,
    help=('Whether to request spot instances. If specified, override the '
          '"resources.use_spot".'))
@click.option('--name',
              '-n',
              required=False,
              type=str,
              help=('Task name. Overrides the "name" '
                    'config in the YAML if both are supplied.'))
@click.option('--disk-size',
              default=None,
              type=int,
              required=False,
              help=('OS disk size in GBs.'))
def launch(entrypoint: str, cluster: Optional[str], dryrun: bool,
           detach_run: bool, backend_name: str, workdir: Optional[str],
           cloud: Optional[str], gpus: Optional[str], use_spot: Optional[bool],
           name: Optional[str], disk_size: Optional[int]):
    """Launch a task from a YAML or a command (rerun setup if cluster exists).

    If entrypoint points to a valid YAML file, it is read in as the task
    specification. Otherwise, it is interpreted as a bash command to be
    executed on the head node of the cluster.
    """
    if backend_name is None:
        backend_name = backends.CloudVmRayBackend.NAME
    entrypoint = ' '.join(entrypoint)
    with sky.Dag() as dag:
        if _check_yaml(entrypoint):
            # Treat entrypoint as a yaml.
            click.secho('Task from YAML spec: ', fg='yellow', nl=False)
            click.secho(entrypoint, bold=True)
            task = sky.Task.from_yaml(entrypoint)
        else:
            # Treat entrypoint as a bash command.
            click.secho('Task from command: ', fg='yellow', nl=False)
            click.secho(entrypoint, bold=True)
            task = sky.Task(name='sky-cmd', run=entrypoint)
            task.set_resources({sky.Resources()})
        # Override.
        if workdir is not None:
            task.workdir = workdir

        assert len(task.resources) == 1
        new_resources = copy.deepcopy(list(task.resources)[0])

        if cloud is not None:
            if cloud not in clouds.CLOUD_REGISTRY:
                raise click.UsageError(
                    f'Cloud \'{cloud}\' is not supported. '
                    f'Supported clouds: {list(clouds.CLOUD_REGISTRY.keys())}')
            new_resources.cloud = clouds.CLOUD_REGISTRY[cloud]
        if gpus is not None:
            new_resources.accelerators = _parse_accelerator_options(gpus)
        if use_spot is not None:
            new_resources.use_spot = use_spot
        if disk_size is not None:
            new_resources.disk_size = disk_size
        task.set_resources({new_resources})
        if name is not None:
            task.name = name

    if cluster is not None:
        click.secho(f'Running task on cluster {cluster}...', fg='yellow')

    if backend_name == backends.LocalDockerBackend.NAME:
        backend = backends.LocalDockerBackend()
    elif backend_name == backends.CloudVmRayBackend.NAME:
        backend = backends.CloudVmRayBackend()
    else:
        raise ValueError(f'{backend_name} backend is not supported.')

    sky.launch(dag,
               dryrun=dryrun,
               stream_logs=True,
               cluster_name=cluster,
               detach_run=detach_run,
               backend=backend)


@cli.command(cls=_DocumentedCodeCommand)
@click.argument('cluster', required=True, type=str)
@click.argument('entrypoint', required=True, type=str, nargs=-1)
@click.option('--detach-run',
              '-d',
              default=False,
              is_flag=True,
              help='If True, run setup first (blocking), '
              'then detach from the job\'s execution.')
@click.option(
    '--workdir',
    required=False,
    type=click.Path(exists=True, file_okay=False),
    help=('If specified, sync this dir to the remote working directory, '
          'where the task will be invoked. '
          'Overrides the "workdir" config in the YAML if both are supplied.'))
@click.option(
    '--gpus',
    required=False,
    type=str,
    help=('Task demands: Type and number of GPUs to use. Example values: '
          '"V100:8", "V100" (short for a count of 1), or "V100:0.5" '
          '(fractional counts are supported by the scheduling framework). '
          'This is used for scheduling the task, so it must fit the '
          'cluster\'s total resources. Overrides the "accelerators" '
          'config in the YAML if both are supplied.'))
@click.option('--name',
              '-n',
              required=False,
              type=str,
              help=('Task name. Overrides the "name" '
                    'config in the YAML if both are supplied.'))
# pylint: disable=redefined-builtin
def exec(cluster: str, entrypoint: str, detach_run: bool,
         workdir: Optional[str], gpus: Optional[str], name: Optional[str]):
    """Execute a task or a command on a cluster (skip setup).

    If entrypoint points to a valid YAML file, it is read in as the task
    specification. Otherwise, it is interpreted as a bash command to be
    executed on the head node of the cluster. The task will be submitted to
    the job queue of the cluster, except if a bash command is used without
    providing --gpus.

    \b
    Actions performed by this command only include:
    - workdir syncing (optional; specified in the YAML spec or via --workdir)
    - executing the task's run command (if entrypoint is a yaml; executed
      either on the cluster's head node or optionally on all nodes), or a
      bash command (only executed on the head node)

    `sky exec` is thus typically faster than `sky launch`, provided a cluster
    already exists.

    All setup steps (provisioning, setup commands, file mounts syncing) are
    skipped.  If any of those specifications changed, this command will not
    reflect those changes.  To ensure a cluster's setup is up to date, use `sky
    launch` instead.

    Typical workflow:

    .. code-block:: bash

        # First command: set up the cluster once.
        sky launch -c mycluster app.yaml

    .. code-block:: bash

        # For iterative development, simply execute the task on the launched
        # cluster.
        sky exec mycluster app.yaml

    .. code-block:: bash

        # Do "sky launch" again if anything other than Task.run is modified:
        sky launch -c mycluster app.yaml

    Advanced use cases:

    .. code-block:: bash

        #  Pass in commands for execution
        sky exec mycluster -- echo Hello World

    """
    entrypoint = ' '.join(entrypoint)
    handle = global_user_state.get_handle_from_cluster_name(cluster)
    if handle is None:
        raise click.BadParameter(f'Cluster \'{cluster}\' not found.  '
                                 'Use `sky launch` to provision first.')
    backend = backend_utils.get_backend_from_handle(handle)

    with sky.Dag() as dag:
        if _check_yaml(entrypoint):
            # Treat entrypoint as a yaml file
            click.secho('Task from YAML spec: ', fg='yellow', nl=False)
            click.secho(entrypoint, bold=True)
            task = sky.Task.from_yaml(entrypoint)
        else:
            # Treat entrypoint as a bash command.
            click.secho('Task from command: ', fg='yellow', nl=False)
            click.secho(entrypoint, bold=True)
            task = sky.Task(name='sky-cmd', run=entrypoint)
            task.set_resources({sky.Resources()})

            if isinstance(backend, backends.CloudVmRayBackend):
                # Run inline commands directly on head node if the resources are
                # not set. User should take the responsibility to not overload
                # the cluster.
                if gpus is None:
                    if workdir is not None:
                        backend.sync_workdir(handle, workdir)
                    backend.run_on_head(
                        handle,
                        entrypoint,
                        stream_logs=True,
                        ssh_mode=backend_utils.SshMode.INTERACTIVE,
                        under_remote_workdir=True)
                    return

        # Override.
        if workdir is not None:
            task.workdir = workdir
        if gpus is not None:
            assert len(task.resources) == 1
            copied = copy.deepcopy(list(task.resources)[0])
            copied.accelerators = _parse_accelerator_options(gpus)
            task.set_resources({copied})
        if name is not None:
            task.name = name

    click.secho(f'Executing task on cluster {cluster}...', fg='yellow')
    sky.exec(dag, backend=backend, cluster_name=cluster, detach_run=detach_run)


def _readable_time_duration(start_time: int):
    duration = pendulum.now().subtract(seconds=time.time() - start_time)
    diff = duration.diff_for_humans()
    diff = diff.replace('second', 'sec')
    diff = diff.replace('minute', 'min')
    diff = diff.replace('hour', 'hr')
    return diff


@cli.command()
@click.option('--all',
              '-a',
              default=False,
              is_flag=True,
              required=False,
              help='Show all information in full.')
def status(all: bool):  # pylint: disable=redefined-builtin
    """Show clusters."""
    show_all = all
    clusters_status = global_user_state.get_clusters()
    cluster_table = util_lib.create_table([
        'NAME',
        'LAUNCHED',
        'RESOURCES',
        'COMMAND',
        'STATUS',
    ])

    for cluster_status in clusters_status:
        launched_at = cluster_status['launched_at']
        handle = cluster_status['handle']
        resources_str = '<initializing>'
        if isinstance(handle, backends.LocalDockerBackend.ResourceHandle):
            resources_str = 'docker'
        elif isinstance(handle, backends.CloudVmRayBackend.ResourceHandle):
            if (handle.launched_nodes is not None and
                    handle.launched_resources is not None):
                launched_resource_str = str(handle.launched_resources)
                if not show_all:
                    launched_resource_str = _truncate_long_string(
                        launched_resource_str)
                resources_str = (f'{handle.launched_nodes}x '
                                 f'{launched_resource_str}')
        else:
            raise ValueError(f'Unknown handle type {type(handle)} encountered.')
        cluster_table.add_row([
            # NAME
            cluster_status['name'],
            # LAUNCHED
            _readable_time_duration(launched_at),
            # RESOURCES
            resources_str,
            # COMMAND
            cluster_status['last_use']
            if show_all else _truncate_long_string(cluster_status['last_use']),
            # STATUS
            cluster_status['status'].value,
        ])
    if clusters_status:
        click.echo(cluster_table)
    else:
        click.echo('No existing clusters.')


@cli.command()
@click.option('--all-users',
              '-a',
              default=False,
              is_flag=True,
              required=False,
              help='Show all users\' information in full.')
@click.option('--skip-finished',
              '-s',
              default=False,
              is_flag=True,
              required=False,
              help='Show only pending/running jobs\' information.')
@click.argument('clusters', required=False, type=str, nargs=-1)
def queue(clusters: Tuple[str], skip_finished: bool, all_users: bool):
    """Show the job queue for cluster(s)."""
    click.secho('Fetching and parsing job queue...', fg='yellow')
    all_jobs = not skip_finished

    codegen = backend_utils.JobLibCodeGen()
    username = getpass.getuser()
    if all_users:
        username = None
    codegen.show_jobs(username, all_jobs)
    code = codegen.build()

    if clusters:
        handles = [
            global_user_state.get_handle_from_cluster_name(c) for c in clusters
        ]
    else:
        cluster_infos = global_user_state.get_clusters()
        clusters = [c['name'] for c in cluster_infos]
        handles = [c['handle'] for c in cluster_infos]

    unsupported_clusters = []
    for cluster, handle in zip(clusters, handles):
        backend = backend_utils.get_backend_from_handle(handle)
        if isinstance(backend, backends.LocalDockerBackend):
            # LocalDockerBackend does not support job queues
            unsupported_clusters.append(cluster)
            continue
        _show_job_queue_on_cluster(cluster, handle, backend, code)
    if unsupported_clusters:
        click.secho(
            f'Note: Job queues are not supported on clusters: '
            f'{", ".join(unsupported_clusters)}',
            fg='yellow')


def _show_job_queue_on_cluster(cluster: str, handle: Optional[Any],
                               backend: backend_lib.Backend, code: str):
    if handle is None:
        print(f'Cluster {cluster} was not found. Skipping.')
        return

    click.echo(f'\nSky Job Queue of Cluster {cluster}')
    if handle.head_ip is None:
        click.echo(
            f'Cluster {cluster} has been stopped or not properly set up. '
            'Please re-launch it with `sky launch` to view the job queue.')
        return

    job_table = backend.run_on_head(handle, code)[1]
    click.echo(f'{job_table}')


@cli.command()
@click.option(
    '--sync-down',
    '-s',
    is_flag=True,
    default=False,
    help='Sync down the logs of the job (This is useful for distributed jobs to'
    'download separate log for each job from all the workers).')
@click.argument('cluster', required=True, type=str)
@click.argument('job_id', required=True, type=str)
def logs(cluster: str, job_id: str, sync_down: bool):
    """Tail the log of a job."""
    # TODO: Add an option for downloading logs.
    cluster_name = cluster
    handle = global_user_state.get_handle_from_cluster_name(cluster_name)
    if handle is None:
        raise click.BadParameter(f'Cluster \'{cluster_name}\' not found'
                                 ' (see `sky status`).')
    if isinstance(handle, backends.LocalDockerBackend.ResourceHandle):
        raise click.UsageError('Sky logs is not available with '
                               'LocalDockerBackend.')
    backend = backend_utils.get_backend_from_handle(handle)

    if sync_down:
        click.secho('Syncing down logs to local...', fg='yellow')
        backend.sync_down_logs(handle, job_id)
    else:
        backend.tail_logs(handle, job_id)


@cli.command()
@click.argument('cluster', required=True, type=str)
@click.option('--all',
              '-a',
              default=False,
              is_flag=True,
              required=False,
              help='Cancel all jobs.')
@click.argument('jobs', required=False, type=int, nargs=-1)
def cancel(cluster: str, all: bool, jobs: List[int]):  # pylint: disable=redefined-builtin
    """Cancel job(s)."""
    if len(jobs) == 0 and not all:
        raise click.UsageError(
            'sky cancel requires either a job id '
            f'(see `sky queue {cluster} -s`) or the --all flag.')

    handle = global_user_state.get_handle_from_cluster_name(cluster)
    if handle is None:
        raise click.BadParameter(f'Cluster \'{cluster}\' not found'
                                 ' (see `sky status`).')

    if all:
        click.secho(f'Cancelling all jobs on cluster {cluster}...', fg='yellow')
        jobs = None
    else:
        jobs_str = ', '.join(map(str, jobs))
        click.secho(f'Cancelling jobs ({jobs_str}) on cluster {cluster}...',
                    fg='yellow')

    codegen = backend_utils.JobLibCodeGen()
    codegen.cancel_jobs(jobs)
    code = codegen.build()

    # FIXME: Assumes a specific backend.
    backend = backends.CloudVmRayBackend()
    backend.run_on_head(handle, code, stream_logs=False, check=True)


@cli.command(cls=_DocumentedCodeCommand)
@click.argument('clusters', nargs=-1, required=False)
@click.option('--all',
              '-a',
              default=None,
              is_flag=True,
              help='Tear down all existing clusters.')
def stop(
        clusters: Tuple[str],
        all: Optional[bool],  # pylint: disable=redefined-builtin
):
    """Stop cluster(s).

    CLUSTER is the name of the cluster to stop.  If both CLUSTER and --all are
    supplied, the latter takes precedence.

    Currently, spot-instance clusters cannot be stopped.

    Examples:

    .. code-block:: bash

        # Stop a specific cluster.
        sky stop cluster_name

    .. code-block:: bash

        # Stop multiple clusters.
        sky stop cluster1 cluster2

    .. code-block:: bash

        # Stop all existing clusters.
        sky stop -a

    """
    _terminate_or_stop_clusters(clusters, apply_to_all=all, terminate=False)


@cli.command(cls=_DocumentedCodeCommand)
@click.argument('clusters', nargs=-1, required=False)
def start(clusters: Tuple[str]):
    """Restart cluster(s).

    If a cluster is previously stopped (status == STOPPED) or failed in
    provisioning/a task's setup (status == INIT), this command will attempt to
    start the cluster.  (In the second case, any failed setup steps are not
    performed and only a request to start the machines is attempted.)

    If a cluster is already in an UP status, this command has no effect on it.

    Examples:

    .. code-block:: bash

      # Restart a specific cluster.
      sky start cluster_name

    .. code-block:: bash

      # Restart multiple clusters.
      sky start cluster1 cluster2

    """
    to_start = []
    if clusters:

        def _filter(name, all_clusters):
            for cluster_record in all_clusters:
                if name == cluster_record['name']:
                    return cluster_record
            return None

        all_clusters = global_user_state.get_clusters()
        for name in clusters:
            record = _filter(name, all_clusters)
            if record is None:
                console.log(f'[red]Cluster {name} was not found.')
                continue
            # A cluster may have one of the following states:
            #
            #  STOPPED - ok to restart
            #    (currently, only AWS/GCP non-spot clusters can be in this
            #    state)
            #
            #  UP - skipped, see below
            #
            #  INIT - ok to restart:
            #    1. It can be a failed-to-provision cluster, so it isn't up
            #      (Ex: gpunode --gpus=A100:8).  Running `sky start` enables
            #      retrying the provisioning - without setup steps being
            #      completed. (Arguably the original command that failed should
            #      be used instead; but using start isn't harmful - after it
            #      gets provisioned successfully the user can use the original
            #      command).
            #
            #    2. It can be an up cluster that failed one of the setup steps.
            #      This way 'sky start' can change its status to UP, enabling
            #      'sky ssh' to debug things (otherwise `sky ssh` will fail an
            #      INIT state cluster due to head_ip not being cached).
            #
            #      This can be replicated by adding `exit 1` to Task.setup.
            if record['status'] == global_user_state.ClusterStatus.UP:
                # An UP cluster; skipping 'sky start' because:
                #  1. For a really up cluster, this has no effects (ray up -y
                #    --no-restart) anyway.
                #  2. A cluster may show as UP but is manually stopped in the
                #    UI.  If Azure/GCP: ray autoscaler doesn't support reusing,
                #    so 'sky start existing' will actually launch a new
                #    cluster with this name, leaving the original cluster
                #    zombied (remains as stopped in the cloud's UI).
                #
                #    This is dangerous and unwanted behavior!
                console.log(f'[yellow]Cluster {name} already has status UP.')
                continue
            assert record['status'] in (
                global_user_state.ClusterStatus.INIT,
                global_user_state.ClusterStatus.STOPPED), record
            to_start.append({'name': name, 'handle': record['handle']})
    if not to_start:
        return
    # FIXME: Assumes a specific backend.
    backend = cloud_vm_ray_backend.CloudVmRayBackend()
    for record in to_start:
        name = record['name']
        handle = record['handle']
        with sky.Dag():
            dummy_task = sky.Task().set_resources(handle.launched_resources)
            dummy_task.num_nodes = handle.launched_nodes

        with console.status(f'[bold green]Starting {name}...') as status:
            backend.provision(dummy_task,
                            to_provision=handle.launched_resources,
                            dryrun=False,
                            stream_logs=False,
                            cluster_name=name)
            console.log(f'[green]Cluster {name} started.')


@cli.command(cls=_DocumentedCodeCommand)
@click.argument('clusters', nargs=-1, required=False)
@click.option('--all',
              '-a',
              default=None,
              is_flag=True,
              help='Tear down all existing clusters.')
def down(
        clusters: Tuple[str],
        all: Optional[bool],  # pylint: disable=redefined-builtin
):
    """Tear down cluster(s).

    CLUSTER is the name of the cluster to tear down.  If both CLUSTER and --all
    are supplied, the latter takes precedence.

    Accelerators (e.g., TPU) that are part of the cluster will be deleted too.

    Examples:

    .. code-block:: bash

        # Tear down a specific cluster.
        sky down cluster_name

    .. code-block:: bash

        # Tear down multiple clusters.
        sky down cluster1 cluster2

    .. code-block:: bash

        # Tear down all existing clusters.
        sky down -a

    """
    names = clusters
    if not all and not names:
        return
    _terminate_or_stop_clusters(names, apply_to_all=all, terminate=True)


def _terminate_or_stop_clusters(names: Tuple[str], apply_to_all: Optional[bool],
                                terminate: bool) -> None:
    """Terminates or stops a cluster (or all clusters)."""
    command = 'down' if terminate else 'stop'
    if not names and apply_to_all is None:
        raise click.UsageError(
            f'sky {command} requires either a cluster name (see `sky status`) '
            'or --all.')

    to_down = []
    if len(names) > 0:
        for name in names:
            handle = global_user_state.get_handle_from_cluster_name(name)
            if handle is not None:
                to_down.append({'name': name, 'handle': handle})
            else:
                console.log(f'[red]Cluster {name} not found.')
    if apply_to_all:
        to_down = global_user_state.get_clusters()
        if len(names) > 0:
            console.log(f'[yellow]Both --all and cluster(s) specified for sky {command}. '
                  'Letting --all take effect.')
            names = []
    if not to_down and not names:
        console.log('[yellow]No existing clusters found (see [bold green]sky status[/]).')

    for record in to_down:  # TODO: parallelize.
        name = record['name']
        handle = record['handle']
        backend = backend_utils.get_backend_from_handle(handle)
        if (isinstance(backend, backends.CloudVmRayBackend) and
                handle.launched_resources.use_spot and not terminate):
            # TODO(suquark): enable GCP+spot to be stopped in the future.
            console.log(
                f'[yellow]Stopping cluster {name}... skipped, because spot '
                '[yellow]instances may lose attached volumes. ')
            console.log(f'To terminate the cluster, run: [bold]sky down {name}')
            continue

        teardown_verb = 'Terminating' if terminate else 'Stopping'
        with console.status(f"[bold green]{teardown_verb} {name}...") as status:
            # TODO: do not fail silently
            backend.teardown(handle, terminate=terminate)

            if terminate:
                console.log(f'Terminated cluster {name}')
            else:
                console.log(f'Stopped cluster {name}. To restart, run: [bold green]sky start {name}')


@_interactive_node_cli_command
def gpunode(cluster: str, port_forward: Optional[List[int]],
            cloud: Optional[str], instance_type: Optional[str],
            gpus: Optional[str], spot: Optional[bool], screen: Optional[bool],
            tmux: Optional[bool], disk_size: Optional[int]):
    """Launch or attach to an interactive GPU node.

    Example:

    .. code-block:: bash

        # Launch a default gpunode.
        sky gpunode

    .. code-block:: bash

        # Do work, then log out. The node is kept running. Attach back to the
        # same node and do more work.
        sky gpunode

    .. code-block:: bash

        # Create many interactive nodes by assigning names via --cluster (-c).
        sky gpunode -c node0
        sky gpunode -c node1

    .. code-block:: bash

        # Port forward.
        sky gpunode --port-forward 8080 --port-forward 4650 -c cluster_name
        sky gpunode -p 8080 -p 4650 -c cluster_name

    .. code-block:: bash

        # Sync current working directory to ~/workdir on the node.
        rsync -r . cluster_name:~/workdir

    """
    # TODO: Factor out the shared logic below for [gpu|cpu|tpu]node.
    if screen and tmux:
        raise click.UsageError('Cannot use both screen and tmux.')

    session_manager = None
    if screen or tmux:
        session_manager = 'tmux' if tmux else 'screen'
    name = cluster
    if name is None:
        name = _default_interactive_node_name('gpunode')

    user_requested_resources = not (cloud is None and instance_type is None and
                                    gpus is None and spot is None)
    default_resources = _INTERACTIVE_NODE_DEFAULT_RESOURCES['gpunode']
    cloud_provider = clouds.CLOUD_REGISTRY.get(cloud, default_resources.cloud)
    if cloud is not None and cloud not in clouds.CLOUD_REGISTRY:
        raise click.UsageError(
            f'Cloud \'{cloud}\' is not supported. '
            f'Supported clouds: {list(clouds.CLOUD_REGISTRY.keys())}')
    if gpus is not None:
        gpus = _parse_accelerator_options(gpus)
    elif instance_type is None:
        # Use this request if both gpus and instance_type are not specified.
        gpus = default_resources.accelerators
        instance_type = default_resources.instance_type
    if spot is None:
        spot = default_resources.use_spot
    resources = sky.Resources(cloud=cloud_provider,
                              instance_type=instance_type,
                              accelerators=gpus,
                              use_spot=spot,
                              disk_size=disk_size)

    _create_and_ssh_into_node(
        'gpunode',
        resources,
        cluster_name=name,
        port_forward=port_forward,
        session_manager=session_manager,
        user_requested_resources=user_requested_resources,
    )


@_interactive_node_cli_command
def cpunode(cluster: str, port_forward: Optional[List[int]],
            cloud: Optional[str], instance_type: Optional[str],
            spot: Optional[bool], screen: Optional[bool], tmux: Optional[bool],
            disk_size: Optional[int]):
    """Launch or attach to an interactive CPU node.

    Example:

    .. code-block:: bash

        # Launch a default cpunode.
        sky cpunode

    .. code-block:: bash

        # Do work, then log out. The node is kept running. Attach back to the
        # same node and do more work.
        sky cpunode

    .. code-block:: bash

        # Create many interactive nodes by assigning names via --cluster (-c).
        sky cpunode -c node0
        sky cpunode -c node1

    .. code-block:: bash

        # Port forward.
        sky cpunode --port-forward 8080 --port-forward 4650 -c cluster_name
        sky cpunode -p 8080 -p 4650 -c cluster_name

    .. code-block:: bash

        # Sync current working directory to ~/workdir on the node.
        rsync -r . cluster_name:~/workdir

    """
    if screen and tmux:
        raise click.UsageError('Cannot use both screen and tmux.')

    session_manager = None
    if screen or tmux:
        session_manager = 'tmux' if tmux else 'screen'
    name = cluster
    if name is None:
        name = _default_interactive_node_name('cpunode')

    user_requested_resources = not (cloud is None and instance_type is None and
                                    spot is None)
    default_resources = _INTERACTIVE_NODE_DEFAULT_RESOURCES['cpunode']
    cloud_provider = clouds.CLOUD_REGISTRY.get(cloud, None)
    if cloud is not None and cloud not in clouds.CLOUD_REGISTRY:
        raise click.UsageError(
            f'Cloud \'{cloud}\' is not supported. ' + \
            f'Supported clouds: {list(clouds.CLOUD_REGISTRY.keys())}'
        )
    if instance_type is None:
        instance_type = default_resources.instance_type
    if spot is None:
        spot = default_resources.use_spot
    resources = sky.Resources(cloud=cloud_provider,
                              instance_type=instance_type,
                              use_spot=spot,
                              disk_size=disk_size)

    _create_and_ssh_into_node(
        'cpunode',
        resources,
        cluster_name=name,
        port_forward=port_forward,
        session_manager=session_manager,
        user_requested_resources=user_requested_resources,
    )


@_interactive_node_cli_command
def tpunode(cluster: str, port_forward: Optional[List[int]],
            instance_type: Optional[str], tpus: Optional[str],
            spot: Optional[bool], screen: Optional[bool], tmux: Optional[bool],
            disk_size: Optional[int]):
    """Launch or attach to an interactive TPU node.

    Example:

    .. code-block:: bash

        # Launch a default tpunode.
        sky tpunode

    .. code-block:: bash

        # Do work, then log out. The node is kept running. Attach back to the
        # same node and do more work.
        sky tpunode

    .. code-block:: bash

        # Create many interactive nodes by assigning names via --cluster (-c).
        sky tpunode -c node0
        sky tpunode -c node1

    .. code-block:: bash

        # Port forward.
        sky tpunode --port-forward 8080 --port-forward 4650 -c cluster_name
        sky tpunode -p 8080 -p 4650 -c cluster_name

    .. code-block:: bash

        # Sync current working directory to ~/workdir on the node.
        rsync -r . cluster_name:~/workdir

    """
    if screen and tmux:
        raise click.UsageError('Cannot use both screen and tmux.')

    session_manager = None
    if screen or tmux:
        session_manager = 'tmux' if tmux else 'screen'
    name = cluster
    if name is None:
        name = _default_interactive_node_name('tpunode')

    user_requested_resources = not (instance_type is None and tpus is None and
                                    spot is None)
    default_resources = _INTERACTIVE_NODE_DEFAULT_RESOURCES['tpunode']
    if instance_type is None:
        instance_type = default_resources.instance_type
    if tpus is None:
        tpus = default_resources.accelerators
    else:
        tpus = _parse_accelerator_options(tpus)
    if spot is None:
        spot = default_resources.use_spot
    resources = sky.Resources(cloud=sky.GCP(),
                              instance_type=instance_type,
                              accelerators=tpus,
                              use_spot=spot,
                              disk_size=disk_size)

    _create_and_ssh_into_node(
        'tpunode',
        resources,
        cluster_name=name,
        port_forward=port_forward,
        session_manager=session_manager,
        user_requested_resources=user_requested_resources,
    )


@cli.command()
def check():
    """Determine the set of clouds available to use.

    This checks access credentials for AWS, Azure and GCP; on failure, it shows
    the reason and suggests correction steps. Sky tasks will only run on clouds
    that you have access to.
    """
    sky_check.check()


@cli.command()
@click.argument('gpu_name', required=False)
@click.option('--all',
              '-a',
              is_flag=True,
              default=False,
              help='Show details of all GPU/TPU/accelerator offerings.')
def show_gpus(gpu_name: Optional[str], all: bool):  # pylint: disable=redefined-builtin
    """Show supported GPU/TPU/accelerators.

    To show the detailed information of a GPU/TPU type (which clouds offer it,
    the quantity in each VM type, etc.), use `sky show-gpus <gpu>`.

    To show all GPUs, including less common ones and their detailed
    information, use `sky show-gpus --all`.
    """
    show_all = all
    if show_all and gpu_name is not None:
        raise click.UsageError('--all is only allowed without a GPU name.')

    def _list_to_str(lst):
        return ', '.join([str(e) for e in lst])

    def _output():
        gpu_table = util_lib.create_table(
            ['NVIDIA_GPU', 'AVAILABLE_QUANTITIES'])
        tpu_table = util_lib.create_table(
            ['GOOGLE_TPU', 'AVAILABLE_QUANTITIES'])
        other_table = util_lib.create_table(
            ['OTHER_GPU', 'AVAILABLE_QUANTITIES'])

        if gpu_name is None:
            result = service_catalog.list_accelerator_counts(gpus_only=True)

            # NVIDIA GPUs
            for gpu in service_catalog.get_common_gpus():
                gpu_table.add_row([gpu, _list_to_str(result.pop(gpu))])
            yield from gpu_table.get_string()
            yield '\n\n'

            # Google TPUs
            for tpu in service_catalog.get_tpus():
                tpu_table.add_row([tpu, _list_to_str(result.pop(tpu))])
            yield from tpu_table.get_string()

            # Other GPUs
            if show_all:
                yield '\n\n'
                for gpu, qty in sorted(result.items()):
                    other_table.add_row([gpu, _list_to_str(qty)])
                yield from other_table.get_string()
            else:
                return

        # Show detailed accelerator information
        result = service_catalog.list_accelerators(gpus_only=True,
                                                   name_filter=gpu_name)
        import pandas as pd  # pylint: disable=import-outside-toplevel
        for i, (gpu, items) in enumerate(result.items()):
            accelerator_table = util_lib.create_table([
                'GPU',
                'QTY',
                'CLOUD',
                'INSTANCE_TYPE',
                'HOST_MEMORY',
            ])
            for item in items:
                instance_type_str = item.instance_type if not pd.isna(
                    item.instance_type) else '(attachable)'
                mem_str = f'{item.memory:.0f}GB' if item.memory > 0 else '-'
                accelerator_table.add_row([
                    item.accelerator_name, item.accelerator_count, item.cloud,
                    instance_type_str, mem_str
                ])

            if i != 0 or gpu_name is None:
                yield '\n\n'
            yield from accelerator_table.get_string()

    if show_all:
        click.echo_via_pager(_output())
    else:
        for out in _output():
            click.echo(out, nl=False)
        click.echo()


def main():
    return cli()


if __name__ == '__main__':
    main()
