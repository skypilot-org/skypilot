"""The 'sky' command line tool.

Example usage:

  # See available commands.
  >> sky

  # Run a task, described in a yaml file.
  # Provisioning, setup, file syncing are handled.
  >> sky launch task.yaml
  >> sky launch [-c cluster_name] task.yaml

  # Show the list of running clusters.
  >> sky status

  # Tear down a specific cluster.
  >> sky down cluster_name

  # Tear down all existing clusters.
  >> sky down -a

TODO:
- Add support for local Docker backend.  Currently this module is very coupled
  with CloudVmRayBackend, as seen by the many use of ray commands.

NOTE: the order of command definitions in this file corresponds to how they are
listed in "sky --help".  Take care to put logically connected commands close to
each other.
"""
import functools
import getpass
import os
import shlex
import sys
import tempfile
import typing
from typing import Any, Dict, List, Optional, Tuple
import yaml

import click
import colorama
from rich import progress as rich_progress

import sky
from sky import backends
from sky import check as sky_check
from sky import clouds
from sky import data
from sky import global_user_state
from sky import sky_logging
from sky import spot as spot_lib
from sky.backends import backend_utils
from sky.clouds import service_catalog
from sky.data import data_utils
from sky.data.storage import StoreType
from sky.skylet import job_lib
from sky.skylet.utils import log_utils
from sky.utils import command_runner
from sky.utils import subprocess_utils
from sky.utils import ux_utils
from sky.utils.cli_utils import status_utils

if typing.TYPE_CHECKING:
    from sky.backends import backend as backend_lib

logger = sky_logging.init_logger(__name__)

_CLUSTER_FLAG_HELP = """\
A cluster name. If provided, either reuse an existing cluster with that name or
provision a new cluster with that name. Otherwise provision a new cluster with
an autogenerated name."""
_INTERACTIVE_NODE_TYPES = ('cpunode', 'gpunode', 'tpunode')
_INTERACTIVE_NODE_DEFAULT_RESOURCES = {
    'cpunode': sky.Resources(cloud=None,
                             instance_type=None,
                             accelerators=None,
                             use_spot=False),
    'gpunode': sky.Resources(cloud=None,
                             instance_type=None,
                             accelerators={'K80': 1},
                             use_spot=False),
    'tpunode': sky.Resources(cloud=sky.GCP(),
                             instance_type=None,
                             accelerators={'tpu-v3-8': 1},
                             accelerator_args={'runtime_version': '2.5.0'},
                             use_spot=False),
}


def _get_glob_clusters(clusters: List[str]) -> List[str]:
    """Returns a list of clusters that match the glob pattern."""
    glob_clusters = []
    for cluster in clusters:
        glob_cluster = global_user_state.get_glob_cluster_names(cluster)
        if len(glob_cluster) == 0:
            print(f'Cluster {cluster} not found.')
        glob_clusters.extend(glob_cluster)
    return list(set(glob_clusters))


def _interactive_node_cli_command(cli_func):
    """Click command decorator for interactive node commands."""
    assert cli_func.__name__ in _INTERACTIVE_NODE_TYPES, cli_func.__name__

    cluster_option = click.option('--cluster',
                                  '-c',
                                  default=None,
                                  type=str,
                                  required=False,
                                  help=_CLUSTER_FLAG_HELP)
    port_forward_option = click.option(
        '--port-forward',
        '-p',
        multiple=True,
        default=[],
        type=int,
        required=False,
        help=('Port to be forwarded. To forward multiple ports, '
              'use this option multiple times.'))
    screen_option = click.option('--screen',
                                 default=False,
                                 is_flag=True,
                                 help='If true, attach using screen.')
    tmux_option = click.option('--tmux',
                               default=False,
                               is_flag=True,
                               help='If true, attach using tmux.')
    cloud_option = click.option('--cloud',
                                default=None,
                                type=str,
                                help='Cloud provider to use.')
    instance_type_option = click.option('--instance-type',
                                        '-t',
                                        default=None,
                                        type=str,
                                        help='Instance type to use.')
    gpus = click.option('--gpus',
                        default=None,
                        type=str,
                        help=('Type and number of GPUs to use '
                              '(e.g., ``--gpus=V100:8`` or ``--gpus=V100``).'))
    tpus = click.option(
        '--tpus',
        default=None,
        type=str,
        help=('Type and number of TPUs to use (e.g., ``--tpus=tpu-v3-8:4`` or '
              '``--tpus=tpu-v3-8``).'))

    spot_option = click.option('--use-spot',
                               default=None,
                               is_flag=True,
                               help='If true, use spot instances.')

    disk_size = click.option('--disk-size',
                             default=None,
                             type=int,
                             required=False,
                             help=('OS disk size in GBs.'))
    no_confirm = click.option('--yes',
                              '-y',
                              is_flag=True,
                              default=False,
                              required=False,
                              help='Skip confirmation prompt.')

    click_decorators = [
        cli.command(cls=_DocumentedCodeCommand),
        cluster_option,
        no_confirm,
        port_forward_option,

        # Resource options
        *([cloud_option] if cli_func.__name__ != 'tpunode' else []),
        instance_type_option,
        *([gpus] if cli_func.__name__ == 'gpunode' else []),
        *([tpus] if cli_func.__name__ == 'tpunode' else []),
        spot_option,

        # Attach options
        screen_option,
        tmux_option,
        disk_size,
    ]
    decorator = functools.reduce(lambda res, f: f(res),
                                 reversed(click_decorators), cli_func)

    return decorator


def _parse_env_var(env_var: str) -> Tuple[str, str]:
    """Parse env vars into a (KEY, VAL) pair."""
    if '=' not in env_var:
        value = os.environ.get(env_var)
        if value is None:
            raise click.UsageError(
                f'{env_var} is not set in local environment.')
        return (env_var, value)
    return tuple(env_var.split('=', 1))


_TASK_OPTIONS = [
    click.option('--name',
                 '-n',
                 required=False,
                 type=str,
                 help=('Task name. Overrides the "name" '
                       'config in the YAML if both are supplied.')),
    click.option(
        '--workdir',
        required=False,
        type=click.Path(exists=True, file_okay=False),
        help=('If specified, sync this dir to the remote working directory, '
              'where the task will be invoked. '
              'Overrides the "workdir" config in the YAML if both are supplied.'
             )),
    click.option(
        '--cloud',
        required=False,
        type=str,
        help=('The cloud to use. If specified, overrides the "resources.cloud" '
              'config. Passing "none" resets the config.')),
    click.option(
        '--region',
        required=False,
        type=str,
        help=('The region to use. If specified, overrides the '
              '"resources.region" config. Passing "none" resets the config.')),
    click.option(
        '--gpus',
        required=False,
        type=str,
        help=
        ('Type and number of GPUs to use. Example values: '
         '"V100:8", "V100" (short for a count of 1), or "V100:0.5" '
         '(fractional counts are supported by the scheduling framework). '
         'If a new cluster is being launched by this command, this is the '
         'resources to provision. If an existing cluster is being reused, this'
         ' is seen as the task demand, which must fit the cluster\'s total '
         'resources and is used for scheduling the task. '
         'Overrides the "accelerators" '
         'config in the YAML if both are supplied. '
         'Passing "none" resets the config.')),
    click.option(
        '--num-nodes',
        required=False,
        type=int,
        help=('Number of nodes to execute the task on. '
              'Overrides the "num_nodes" config in the YAML if both are '
              'supplied.')),
    click.option(
        '--use-spot/--no-use-spot',
        required=False,
        default=None,
        help=('Whether to request spot instances. If specified, overrides the '
              '"resources.use_spot" config.')),
    click.option('--image-id',
                 required=False,
                 default=None,
                 help=('Custom image id for launching the instances. '
                       'Passing "none" resets the config.')),
    click.option(
        '--env',
        required=False,
        type=_parse_env_var,
        multiple=True,
        help="""
        Environment variable to set on the remote node.
        It can be specified multiple times.
        Examples:

        \b
        1. ``--env MY_ENV=1``: set ``$MY_ENV`` on the cluster to be 1.

        2. ``--env MY_ENV2=$HOME``: set ``$MY_ENV2`` on the cluster to be the
        same value of ``$HOME`` in the local environment where the sky command
        is run.

        3. ``--env MY_ENV3``: set ``$MY_ENV3`` on the cluster to be the
        same value of ``$MY_ENV3`` in the local environment.""",
    )
]


def _add_click_options(options: List[click.Option]):
    """A decorator for adding a list of click option decorators."""

    def _add_options(func):
        for option in reversed(options):
            func = option(func)
        return func

    return _add_options


def _default_interactive_node_name(node_type: str):
    """Returns a deterministic name to refer to the same node."""
    # FIXME: this technically can collide in Azure/GCP with another
    # same-username user.  E.g., sky-gpunode-ubuntu.  Not a problem on AWS
    # which is the current cloud for interactive nodes.
    assert node_type in _INTERACTIVE_NODE_TYPES, node_type
    return f'sky-{node_type}-{getpass.getuser()}'


def _infer_interactive_node_type(resources: sky.Resources):
    """Determine interactive node type from resources."""
    accelerators = resources.accelerators
    cloud = resources.cloud
    if accelerators:
        # We only support homogenous accelerators for now.
        assert len(accelerators) == 1, resources
        acc, _ = list(accelerators.items())[0]
        is_gcp = cloud is not None and cloud.is_same_cloud(sky.GCP())
        if is_gcp and 'tpu' in acc:
            return 'tpunode'
        return 'gpunode'
    return 'cpunode'


def _check_resources_match(backend: backends.Backend,
                           cluster_name: str,
                           task: 'sky.Task',
                           node_type: Optional[str] = None) -> None:
    """Check matching resources when reusing an existing cluster.

    The only exception is when [cpu|tpu|gpu]node -c cluster_name is used with no
    additional arguments, then login succeeds.

    Args:
        cluster_name: The name of the cluster.
        task: The task requested to be run on the cluster.
        node_type: Only used for interactive node. Node type to attach to VM.
    """
    handle = global_user_state.get_handle_from_cluster_name(cluster_name)
    if handle is None:
        return

    if node_type is not None:
        inferred_node_type = _infer_interactive_node_type(
            handle.launched_resources)
        if node_type != inferred_node_type:
            name_arg = ''
            if cluster_name != _default_interactive_node_name(
                    inferred_node_type):
                name_arg = f' -c {cluster_name}'
            raise click.UsageError(
                f'Failed to attach to interactive node {cluster_name}. '
                f'Please use: {colorama.Style.BRIGHT}'
                f'sky {inferred_node_type}{name_arg}{colorama.Style.RESET_ALL}')
        return
    backend.check_resources_fit_cluster(handle, task)


def _launch_with_confirm(
    dag: sky.Dag,
    backend: backends.Backend,
    cluster: str,
    *,
    dryrun: bool,
    detach_run: bool,
    no_confirm: bool = False,
    idle_minutes_to_autostop: int = -1,
    retry_until_up: bool = False,
    node_type: Optional[str] = None,
):
    """Launch a cluster with a DAG."""
    if cluster is None:
        cluster = backend_utils.generate_cluster_name()
    maybe_status, _ = backend_utils.refresh_cluster_status_handle(cluster)
    if maybe_status is None:
        # Show the optimize log before the prompt if the cluster does not exist.
        dag = sky.optimize(dag)
    task = dag.tasks[0]

    _check_resources_match(backend, cluster, task, node_type=node_type)

    confirm_shown = False
    if not no_confirm:
        # Prompt if (1) --cluster is None, or (2) cluster doesn't exist, or (3)
        # it exists but is STOPPED.
        prompt = None
        if maybe_status is None:
            cluster_str = '' if cluster is None else f' {cluster!r}'
            prompt = f'Launching a new cluster{cluster_str}. Proceed?'
        elif maybe_status == global_user_state.ClusterStatus.STOPPED:
            prompt = f'Restarting the stopped cluster {cluster!r}. Proceed?'
        if prompt is not None:
            confirm_shown = True
            click.confirm(prompt, default=True, abort=True, show_default=True)

    if node_type is not None:
        if maybe_status != global_user_state.ClusterStatus.UP:
            click.secho(f'Setting up interactive node {cluster}...',
                        fg='yellow')
    elif not confirm_shown:
        click.secho(f'Running task on cluster {cluster}...', fg='yellow')

    if node_type is None or maybe_status != global_user_state.ClusterStatus.UP:
        # No need to sky.launch again when interactive node is already up.
        sky.launch(dag,
                   dryrun=dryrun,
                   stream_logs=True,
                   cluster_name=cluster,
                   detach_run=detach_run,
                   backend=backend,
                   idle_minutes_to_autostop=idle_minutes_to_autostop,
                   retry_until_up=retry_until_up)


# TODO: skip installing ray to speed up provisioning.
def _create_and_ssh_into_node(
    node_type: str,
    resources: sky.Resources,
    cluster_name: str,
    backend: Optional['backend_lib.Backend'] = None,
    port_forward: Optional[List[int]] = None,
    session_manager: Optional[str] = None,
    user_requested_resources: Optional[bool] = False,
    no_confirm: bool = False,
):
    """Creates and attaches to an interactive node.

    Args:
        node_type: Type of the interactive node: { 'cpunode', 'gpunode' }.
        resources: Resources to attach to VM.
        cluster_name: a cluster name to identify the interactive node.
        backend: the Backend to use (currently only CloudVmRayBackend).
        port_forward: List of ports to forward.
        session_manager: Attach session manager: { 'screen', 'tmux' }.
        user_requested_resources: If true, user requested resources explicitly.
        no_confirm: If true, skips confirmation prompt presented to user.
    """
    assert node_type in _INTERACTIVE_NODE_TYPES, node_type
    assert session_manager in (None, 'screen', 'tmux'), session_manager
    with sky.Dag() as dag:
        # TODO: Add conda environment replication
        # should be setup =
        # 'conda env export | grep -v "^prefix: " > environment.yml'
        # && conda env create -f environment.yml
        task = sky.Task(
            node_type,
            workdir=None,
            setup=None,
            run='',
        )
        task.set_resources(resources)

    backend = backend if backend is not None else backends.CloudVmRayBackend()
    maybe_status, _ = backend_utils.refresh_cluster_status_handle(cluster_name)
    if maybe_status is not None and user_requested_resources:
        name_arg = ''
        if cluster_name != _default_interactive_node_name(node_type):
            name_arg = f' -c {cluster_name}'
        raise click.UsageError(
            'Resources cannot be specified for an existing interactive node '
            f'{cluster_name!r}. To login to the cluster, use: '
            f'{colorama.Style.BRIGHT}'
            f'sky {node_type}{name_arg}{colorama.Style.RESET_ALL}')

    _launch_with_confirm(
        dag,
        backend,
        cluster_name,
        dryrun=False,
        detach_run=True,
        no_confirm=no_confirm,
        node_type=node_type,
    )
    handle = global_user_state.get_handle_from_cluster_name(cluster_name)

    # Use ssh rather than 'ray attach' to suppress ray messages, speed up
    # connection, and for allowing adding 'cd workdir' in the future.
    # Disable check, since the returncode could be non-zero if the user Ctrl-D.
    commands = []
    if session_manager == 'screen':
        commands += ['screen', '-D', '-R']
    elif session_manager == 'tmux':
        commands += ['tmux', 'attach', '||', 'tmux', 'new']
    backend.run_on_head(handle,
                        commands,
                        port_forward=port_forward,
                        ssh_mode=command_runner.SshMode.LOGIN)
    cluster_name = handle.cluster_name

    click.echo('To attach to it again:  ', nl=False)
    if cluster_name == _default_interactive_node_name(node_type):
        option = ''
    else:
        option = f' -c {cluster_name}'
    click.secho(f'sky {node_type}{option}', bold=True)
    click.echo('To stop the node:\t', nl=False)
    click.secho(f'sky stop {cluster_name}', bold=True)
    click.echo('To tear down the node:\t', nl=False)
    click.secho(f'sky down {cluster_name}', bold=True)
    click.echo('To upload a folder:\t', nl=False)
    click.secho(f'rsync -rP /local/path {cluster_name}:/remote/path', bold=True)
    click.echo('To download a folder:\t', nl=False)
    click.secho(f'rsync -rP {cluster_name}:/remote/path /local/path', bold=True)


def _check_yaml(entrypoint: str) -> bool:
    """Checks if entrypoint is a readable YAML file."""
    is_yaml = True
    shell_splits = shlex.split(entrypoint)
    yaml_file_provided = len(shell_splits) == 1 and \
        (shell_splits[0].endswith('yaml') or shell_splits[0].endswith('.yml'))
    try:
        with open(entrypoint, 'r') as f:
            try:
                config = yaml.safe_load(f)
                if isinstance(config, str):
                    # 'sky exec cluster ./my_script.sh'
                    is_yaml = False
            except yaml.YAMLError as e:
                if yaml_file_provided:
                    logger.debug(e)
                    invalid_reason = ('contains an invalid configuration. '
                                      ' Please check syntax.')
                is_yaml = False
    except OSError:
        if yaml_file_provided:
            entry_point_path = os.path.expanduser(entrypoint)
            if not os.path.exists(entry_point_path):
                invalid_reason = ('does not exist. Please check if the path'
                                  ' is correct.')
            elif not os.path.isfile(entry_point_path):
                invalid_reason = ('is not a file. Please check if the path'
                                  ' is correct.')
            else:
                invalid_reason = ('yaml.safe_load() failed. Please check if the'
                                  ' path is correct.')
        is_yaml = False
    if not is_yaml:
        if yaml_file_provided:
            click.confirm(
                f'{entrypoint!r} looks like a yaml path but {invalid_reason}\n'
                'It will be treated as a command to be run remotely. Continue?',
                abort=True)
    return is_yaml


def _make_dag_from_entrypoint_with_overrides(
    entrypoint: List[str],
    *,
    name: Optional[str] = None,
    workdir: Optional[str] = None,
    cloud: Optional[str] = None,
    region: Optional[str] = None,
    gpus: Optional[str] = None,
    num_nodes: Optional[int] = None,
    use_spot: Optional[bool] = None,
    image_id: Optional[str] = None,
    disk_size: Optional[int] = None,
    env: List[Dict[str, str]] = None,
    # spot launch specific
    spot_recovery: Optional[str] = None,
) -> sky.Dag:
    entrypoint = ' '.join(entrypoint)

    with sky.Dag() as dag:
        if _check_yaml(entrypoint):
            # Treat entrypoint as a yaml.
            click.secho('Task from YAML spec: ', fg='yellow', nl=False)
            click.secho(entrypoint, bold=True)
            task = sky.Task.from_yaml(entrypoint)
        else:
            if not entrypoint:
                entrypoint = None
            else:
                # Treat entrypoint as a bash command.
                click.secho('Task from command: ', fg='yellow', nl=False)
                click.secho(entrypoint, bold=True)
            task = sky.Task(name='sky-cmd', run=entrypoint)
            task.set_resources({sky.Resources()})
        # Override.
        if workdir is not None:
            task.workdir = workdir

        override_params = {}
        if cloud is not None:
            if cloud.lower() == 'none':
                override_params['cloud'] = None
            else:
                override_params['cloud'] = clouds.CLOUD_REGISTRY.from_str(cloud)
        if region is not None:
            if region.lower() == 'none':
                override_params['region'] = None
            else:
                override_params['region'] = region
        if gpus is not None:
            if gpus.lower() == 'none':
                override_params['accelerators'] = None
            else:
                override_params['accelerators'] = gpus
        if use_spot is not None:
            override_params['use_spot'] = use_spot
        if disk_size is not None:
            override_params['disk_size'] = disk_size
        if image_id is not None:
            if image_id.lower() == 'none':
                override_params['image_id'] = None
            else:
                override_params['image_id'] = image_id

        # Spot launch specific.
        if spot_recovery is not None:
            if spot_recovery.lower() == 'none':
                override_params['spot_recovery'] = None
            else:
                override_params['spot_recovery'] = spot_recovery

        assert len(task.resources) == 1
        old_resources = list(task.resources)[0]
        new_resources = old_resources.copy(**override_params)

        task.set_resources({new_resources})

        if num_nodes is not None:
            task.num_nodes = num_nodes
        if name is not None:
            task.name = name
        task.set_envs(env)
        # TODO(wei-lin): move this validation into Python API.
        if new_resources.accelerators is not None:
            acc, _ = list(new_resources.accelerators.items())[0]
            if acc.startswith('tpu-') and task.num_nodes > 1:
                raise ValueError('Multi-node TPU cluster is not supported. '
                                 f'Got num_nodes={task.num_nodes}.')
    return dag


def _start_cluster(cluster_name: str,
                   idle_minutes_to_autostop: Optional[int] = None,
                   retry_until_up: bool = False):
    handle = global_user_state.get_handle_from_cluster_name(cluster_name)
    backend = backend_utils.get_backend_from_handle(handle)
    assert isinstance(backend, backends.CloudVmRayBackend)
    with sky.Dag():
        dummy_task = sky.Task().set_resources(handle.launched_resources)
        dummy_task.num_nodes = handle.launched_nodes
    handle = backend.provision(dummy_task,
                               to_provision=handle.launched_resources,
                               dryrun=False,
                               stream_logs=True,
                               cluster_name=cluster_name,
                               retry_until_up=retry_until_up)
    if idle_minutes_to_autostop is not None:
        backend.set_autostop(handle, idle_minutes_to_autostop)
    return handle


class _NaturalOrderGroup(click.Group):
    """Lists commands in the order they are defined in this script.

    Reference: https://github.com/pallets/click/issues/513
    """

    def list_commands(self, ctx):
        return self.commands.keys()


class _DocumentedCodeCommand(click.Command):
    """Corrects help strings for documented commands such that --help displays
    properly and code blocks are rendered in the official web documentation.
    """

    def get_help(self, ctx):
        help_str = ctx.command.help
        ctx.command.help = help_str.replace('.. code-block:: bash\n', '\b')
        return super().get_help(ctx)


@click.group(cls=_NaturalOrderGroup)
def cli():
    pass


@cli.command(cls=_DocumentedCodeCommand)
@click.argument('entrypoint', required=True, type=str, nargs=-1)
@click.option('--cluster',
              '-c',
              default=None,
              type=str,
              help=_CLUSTER_FLAG_HELP)
@click.option('--dryrun',
              default=False,
              is_flag=True,
              help='If True, do not actually run the job.')
@click.option('--detach-run',
              '-d',
              default=False,
              is_flag=True,
              help='If True, run setup first (blocking), '
              'then detach from the job\'s execution.')
@click.option('--docker',
              'backend_name',
              flag_value=backends.LocalDockerBackend.NAME,
              default=False,
              help='If used, runs locally inside a docker container.')
@_add_click_options(_TASK_OPTIONS)
@click.option('--disk-size',
              default=None,
              type=int,
              required=False,
              help=('OS disk size in GBs.'))
@click.option(
    '--idle-minutes-to-autostop',
    '-i',
    default=None,
    type=int,
    required=False,
    help=('Automatically stop the cluster after this many minutes '
          'of idleness, i.e., no running or pending jobs in the cluster\'s job '
          'queue. Idleness starts counting after setup/file_mounts are done; '
          'the clock gets reset whenever there are running/pending jobs in the '
          'job queue. '
          'Setting this flag is equivalent to '
          'running ``sky launch -d ...`` and then ``sky autostop -i <minutes>``'
          '. If not set, the cluster will not be auto-stopped.'))
@click.option(
    '--retry-until-up',
    '-r',
    default=False,
    is_flag=True,
    required=False,
    help=('Whether to retry provisioning infinitely until the cluster is up, '
          'if sky fails to launch the cluster on any possible region/cloud due '
          'to unavailability errors.'))
@click.option('--yes',
              '-y',
              is_flag=True,
              default=False,
              required=False,
              help='Skip confirmation prompt.')
def launch(
    entrypoint: str,
    cluster: Optional[str],
    dryrun: bool,
    detach_run: bool,
    backend_name: Optional[str],
    name: Optional[str],
    workdir: Optional[str],
    cloud: Optional[str],
    region: Optional[str],
    gpus: Optional[str],
    num_nodes: Optional[int],
    use_spot: Optional[bool],
    image_id: Optional[str],
    env: List[Dict[str, str]],
    disk_size: Optional[int],
    idle_minutes_to_autostop: Optional[int],
    retry_until_up: bool,
    yes: bool,
):
    """Launch a task from a YAML or a command (rerun setup if cluster exists).

    If ENTRYPOINT points to a valid YAML file, it is read in as the task
    specification. Otherwise, it is interpreted as a bash command.

    In both cases, the commands are run under the task's workdir (if specified)
    and they undergo job queue scheduling.
    """
    backend_utils.check_cluster_name_not_reserved(
        cluster, operation_str='Launching task on it')
    if backend_name is None:
        backend_name = backends.CloudVmRayBackend.NAME

    dag = _make_dag_from_entrypoint_with_overrides(
        entrypoint=entrypoint,
        name=name,
        workdir=workdir,
        cloud=cloud,
        region=region,
        gpus=gpus,
        num_nodes=num_nodes,
        use_spot=use_spot,
        image_id=image_id,
        env=env,
        disk_size=disk_size,
    )

    if backend_name == backends.LocalDockerBackend.NAME:
        backend = backends.LocalDockerBackend()
    elif backend_name == backends.CloudVmRayBackend.NAME:
        backend = backends.CloudVmRayBackend()
    else:
        with ux_utils.print_exception_no_traceback():
            raise ValueError(f'{backend_name} backend is not supported.')

    _launch_with_confirm(
        dag,
        backend,
        cluster,
        dryrun=dryrun,
        detach_run=detach_run,
        no_confirm=yes,
        idle_minutes_to_autostop=idle_minutes_to_autostop,
        retry_until_up=retry_until_up,
    )


@cli.command(cls=_DocumentedCodeCommand)
@click.argument('cluster', required=True, type=str)
@click.argument('entrypoint', required=True, type=str, nargs=-1)
@click.option('--detach-run',
              '-d',
              default=False,
              is_flag=True,
              help='If True, run workdir syncing first (blocking), '
              'then detach from the job\'s execution.')
@_add_click_options(_TASK_OPTIONS)
# pylint: disable=redefined-builtin
def exec(
    cluster: str,
    entrypoint: str,
    detach_run: bool,
    name: Optional[str],
    cloud: Optional[str],
    region: Optional[str],
    workdir: Optional[str],
    gpus: Optional[str],
    num_nodes: Optional[int],
    use_spot: Optional[bool],
    image_id: Optional[str],
    env: List[Dict[str, str]],
):
    """Execute a task or a command on a cluster (skip setup).

    If ENTRYPOINT points to a valid YAML file, it is read in as the task
    specification. Otherwise, it is interpreted as a bash command.

    \b
    Actions performed by ``sky exec``:

    \b
    (1) workdir syncing, if:
      - ENTRYPOINT is a YAML and ``workdir`` is specified inside; or
      - ENTRYPOINT is a command and flag ``--workdir=<local_path>`` is set.
    (2) executing the specified task's ``run`` commands / the bash command.

    ``sky exec`` is thus typically faster than ``sky launch``, provided a
    cluster already exists.

    All setup steps (provisioning, setup commands, file mounts syncing) are
    skipped.  If any of those specifications changed, this command will not
    reflect those changes.  To ensure a cluster's setup is up to date, use ``sky
    launch`` instead.

    \b
    Execution and scheduling behavior:

    \b
    - The task/command will undergo job queue scheduling, respecting any
      specified resource requirement. It can be executed on any node of the
      cluster with enough resources.
    - The task/command is run under the workdir (if specified).
    - The task/command is run non-interactively (without a pseudo-terminal or
      pty), so interactive commands such as ``htop`` do not work.
      Use ``ssh my_cluster`` instead.

    Typical workflow:

    .. code-block:: bash

      # First command: set up the cluster once.
      sky launch -c mycluster app.yaml
      \b
      # For iterative development, simply execute the task on the launched
      # cluster.
      sky exec mycluster app.yaml
      \b
      # Do "sky launch" again if anything other than Task.run is modified:
      sky launch -c mycluster app.yaml
      \b
      # Pass in commands for execution.
      sky exec mycluster python train_cpu.py
      sky exec mycluster --gpus=V100:1 python train_gpu.py
      \b
      # Pass environment variables to the task.
      sky exec mycluster --env WANDB_API_KEY python train_gpu.py

    """
    backend_utils.check_cluster_name_not_reserved(
        cluster, operation_str='Executing task on it')
    handle = global_user_state.get_handle_from_cluster_name(cluster)
    if handle is None:
        raise click.BadParameter(f'Cluster {cluster!r} not found. '
                                 'Use `sky launch` to provision first.')
    backend = backend_utils.get_backend_from_handle(handle)

    dag = _make_dag_from_entrypoint_with_overrides(
        entrypoint=entrypoint,
        name=name,
        workdir=workdir,
        cloud=cloud,
        region=region,
        gpus=gpus,
        use_spot=use_spot,
        image_id=image_id,
        num_nodes=num_nodes,
        env=env,
    )

    click.secho(f'Executing task on cluster {cluster}...', fg='yellow')
    sky.exec(dag, backend=backend, cluster_name=cluster, detach_run=detach_run)


@cli.command()
@click.option('--all',
              '-a',
              default=False,
              is_flag=True,
              required=False,
              help='Show all information in full.')
@click.option('--refresh',
              '-r',
              default=False,
              is_flag=True,
              required=False,
              help='Query cluster status from the cloud provider.')
def status(all: bool, refresh: bool):  # pylint: disable=redefined-builtin
    """Show clusters.

    The following metadata for each cluster is stored: cluster name, time since
    last launch, resources, region, status, duration, autostop, command, hourly
    price. Display all metadata using ``sky status -a``.

    \b
    Each cluster can have one of the following statuses:

    \b
    - INIT: The cluster may be live or down. It can happen in following cases:
      (1) undergoing provisioning or runtime setup. (In other words, a
      ``sky launch`` has started but has not completed.)
      (2) Or, the cluster is in an abnormal state, e.g., some cluster nodes are
      down, or the sky runtime has crashed.
    - UP: Provisioning and runtime setup have succeeded and the cluster is
      live.  (The most recent ``sky launch`` has completed successfully.)
    - STOPPED: The cluster is stopped and the storage is persisted. Use
      ``sky start`` to restart the cluster.
    """
    status_utils.show_status_table(all, refresh)


@cli.command()
@click.option('--all-users',
              '-a',
              default=False,
              is_flag=True,
              required=False,
              help='Show all users\' information in full.')
@click.option('--skip-finished',
              '-s',
              default=False,
              is_flag=True,
              required=False,
              help='Show only pending/running jobs\' information.')
@click.argument('clusters', required=False, type=str, nargs=-1)
def queue(clusters: Tuple[str], skip_finished: bool, all_users: bool):
    """Show the job queue for cluster(s)."""
    click.secho('Fetching and parsing job queue...', fg='yellow')
    all_jobs = not skip_finished

    username = getpass.getuser()
    if all_users:
        username = None
    code = job_lib.JobLibCodeGen.show_jobs(username, all_jobs)

    if clusters:
        clusters = _get_glob_clusters(clusters)
    else:
        cluster_infos = global_user_state.get_clusters()
        clusters = [c['name'] for c in cluster_infos]

    unsupported_clusters = []
    for cluster in clusters:
        cluster_status, handle = backend_utils.refresh_cluster_status_handle(
            cluster)
        backend = backend_utils.get_backend_from_handle(handle)
        if isinstance(backend, backends.LocalDockerBackend):
            # LocalDockerBackend does not support job queues
            unsupported_clusters.append(cluster)
            continue
        if cluster_status != global_user_state.ClusterStatus.UP:
            click.secho(
                f'Cluster {cluster} is not up (status: {cluster_status.value});'
                ' skipped.',
                fg='yellow')
            continue
        _show_job_queue_on_cluster(cluster, handle, backend, code)
    if unsupported_clusters:
        click.secho(
            f'Note: Job queues are not supported on clusters: '
            f'{", ".join(unsupported_clusters)}',
            fg='yellow')


def _show_job_queue_on_cluster(cluster: str, handle: Optional[Any],
                               backend: 'backend_lib.Backend', code: str):
    click.echo(f'\nSky Job Queue of Cluster {cluster}')
    if handle.head_ip is None:
        click.echo(
            f'Cluster {cluster} has been stopped or not properly set up. '
            'Please re-launch it with `sky launch` to view the job queue.')
        return

    returncode, job_table, stderr = backend.run_on_head(handle,
                                                        code,
                                                        require_outputs=True)
    if returncode != 0:
        click.echo(stderr)
        click.secho(f'Failed to get job queue on cluster {cluster}.', fg='red')
    click.echo(f'{job_table}')


@cli.command()
@click.option(
    '--sync-down',
    '-s',
    is_flag=True,
    default=False,
    help='Sync down the logs of the job (this is useful for distributed jobs to'
    'download a separate log for each job from all the workers).')
@click.option(
    '--status',
    is_flag=True,
    default=False,
    help=('If specified, do not show logs but exit with a status code for the '
          'job\'s status: 0 for succeeded, or 1 for all other statuses.'))
@click.argument('cluster', required=True, type=str)
@click.argument('job_id', required=False, type=str)
# TODO(zhwu): support logs by job name
def logs(cluster: str, job_id: Optional[str], sync_down: bool, status: bool):  # pylint: disable=redefined-outer-name
    """Tail the log of a job.

    If JOB_ID is not provided, tails the logs of the last job on the cluster.
    """
    cluster_name = cluster
    cluster_status, handle = backend_utils.refresh_cluster_status_handle(
        cluster_name)
    if handle is None:
        raise click.BadParameter(f'Cluster \'{cluster_name}\' not found'
                                 ' (see `sky status`).')
    backend = backend_utils.get_backend_from_handle(handle)
    if isinstance(backend, backends.LocalDockerBackend):
        raise click.UsageError('Sky logs is not available with '
                               'LocalDockerBackend.')
    if cluster_status != global_user_state.ClusterStatus.UP:
        click.secho(
            f'Cluster {cluster_name} is not up '
            f'(status: {cluster_status.value}).',
            fg='yellow')
        return

    if sync_down and status:
        raise click.UsageError(
            'Both --sync_down and --status are specified '
            '(ambiguous). To fix: specify at most one of them.')

    if sync_down:
        click.secho('Syncing down logs to local...', fg='yellow')
        backend.sync_down_logs(handle, job_id)
        return

    if job_id is not None and not job_id.isdigit():
        click.secho(
            'Only single job ID supported for streaming or status check, '
            'consider using --sync_down to download logs for multiple jobs.',
            fg='yellow')
        return
    job_id = int(job_id) if job_id is not None else job_id
    if status:
        job_status = backend.get_job_status(handle, job_id)
        if job_status == job_lib.JobStatus.SUCCEEDED:
            sys.exit(0)
        if job_status is None:
            id_str = '' if job_id is None else str(job_id) + ' '
            click.secho(f'Job {id_str}not found', fg='red')
        sys.exit(1)
    else:
        backend.tail_logs(handle, job_id)


@cli.command()
@click.argument('cluster', required=True, type=str)
@click.option('--all',
              '-a',
              default=False,
              is_flag=True,
              required=False,
              help='Cancel all jobs on the specified cluster.')
@click.argument('jobs', required=False, type=int, nargs=-1)
def cancel(cluster: str, all: bool, jobs: List[int]):  # pylint: disable=redefined-builtin
    """Cancel job(s)."""
    if len(jobs) == 0 and not all:
        raise click.UsageError(
            'sky cancel requires either a job id '
            f'(see `sky queue {cluster} -s`) or the --all flag.')

    backend_utils.check_cluster_name_not_reserved(
        cluster, operation_str='Cancelling jobs')

    # Check the status of the cluster.
    cluster_status, handle = backend_utils.refresh_cluster_status_handle(
        cluster)
    if handle is None:
        raise click.BadParameter(f'Cluster {cluster!r} not found'
                                 ' (see `sky status`).')
    backend = backend_utils.get_backend_from_handle(handle)
    if not isinstance(backend, backends.CloudVmRayBackend):
        raise click.UsageError(
            'Job cancelling is only supported for '
            f'{backends.CloudVmRayBackend.NAME}, but cluster {cluster!r} '
            f'is created by {backend.NAME}.')
    if cluster_status != global_user_state.ClusterStatus.UP:
        click.secho(
            f'Cluster {cluster} is not up (status: {cluster_status.value}); '
            'skipped.',
            fg='yellow')
        return

    if all:
        click.secho(f'Cancelling all jobs on cluster {cluster}...', fg='yellow')
        jobs = None
    else:
        jobs_str = ', '.join(map(str, jobs))
        click.secho(f'Cancelling jobs ({jobs_str}) on cluster {cluster}...',
                    fg='yellow')

    backend.cancel_jobs(handle, jobs)


@cli.command(cls=_DocumentedCodeCommand)
@click.argument('clusters', nargs=-1, required=False)
@click.option('--all',
              '-a',
              default=None,
              is_flag=True,
              help='Tear down all existing clusters.')
@click.option('--yes',
              '-y',
              is_flag=True,
              default=False,
              required=False,
              help='Skip confirmation prompt.')
def stop(
    clusters: Tuple[str],
    all: Optional[bool],  # pylint: disable=redefined-builtin
    yes: bool,
):
    """Stop cluster(s).

    CLUSTER is the name (or glob pattern) of the cluster to stop.  If both
    CLUSTER and ``--all`` are supplied, the latter takes precedence.

    Data on attached disks is not lost when a cluster is stopped.  Billing for
    the instances will stop while the disks will still be charged.  Those disks
    will be reattached when restarting the cluster.

    Currently, spot instance clusters cannot be stopped.

    Examples:

    .. code-block:: bash

      # Stop a specific cluster.
      sky stop cluster_name
      \b
      # Stop multiple clusters.
      sky stop cluster1 cluster2
      \b
      # Stop all clusters matching glob pattern 'cluster*'.
      sky stop "cluster*"
      \b
      # Stop all existing clusters.
      sky stop -a

    """
    _terminate_or_stop_clusters(clusters,
                                apply_to_all=all,
                                terminate=False,
                                no_confirm=yes)


@cli.command(cls=_DocumentedCodeCommand)
@click.argument('clusters', nargs=-1, required=False)
@click.option('--all',
              '-a',
              default=None,
              is_flag=True,
              help='Apply this command to all existing clusters.')
@click.option('--idle-minutes',
              '-i',
              type=int,
              default=None,
              required=False,
              help='Set the idle minutes before auto-stopping the cluster.')
@click.option('--cancel',
              default=False,
              is_flag=True,
              required=False,
              help='Cancel the auto-stopping.')
@click.option('--yes',
              '-y',
              is_flag=True,
              default=False,
              required=False,
              help='Skip confirmation prompt.')
def autostop(
    clusters: Tuple[str],
    all: Optional[bool],  # pylint: disable=redefined-builtin
    idle_minutes: Optional[int],
    cancel: bool,  # pylint: disable=redefined-outer-name
    yes: bool,
):
    """Schedule or cancel auto-stopping for cluster(s).

    CLUSTERS are the name (or glob pattern) of the clusters to stop.  If both
    CLUSTERS and ``--all`` are supplied, the latter takes precedence.

    ``--idle-minutes`` is the number of minutes of idleness (no pending/running
    jobs) after which the cluster will be stopped automatically.

    ``--cancel`` will cancel the autostopping. If the cluster was not scheduled
    autostop, this will do nothing to autostop.

    If ``--idle-minutes`` and ``--cancel`` are not specified, default to 5
    minutes.

    Examples:

    .. code-block:: bash

        # Set auto stopping for a specific cluster.
        sky autostop cluster_name -i 60
        \b
        # Cancel auto stopping for a specific cluster.
        sky autostop cluster_name --cancel

    """
    if cancel and idle_minutes is not None:
        raise click.UsageError(
            'Only one of --idle-minutes and --cancel should be specified. '
            f'cancel: {cancel}, idle_minutes: {idle_minutes}')
    if cancel:
        idle_minutes = -1
    elif idle_minutes is None:
        idle_minutes = 5
    _terminate_or_stop_clusters(clusters,
                                apply_to_all=all,
                                terminate=False,
                                no_confirm=yes,
                                idle_minutes_to_autostop=idle_minutes)


@cli.command(cls=_DocumentedCodeCommand)
@click.argument('clusters', nargs=-1, required=True)
@click.option('--yes',
              '-y',
              is_flag=True,
              default=False,
              required=False,
              help='Skip confirmation prompt.')
@click.option(
    '--retry-until-up',
    '-r',
    default=False,
    is_flag=True,
    required=False,
    help=('Retry provisioning infinitely until the cluster is up, '
          'if sky fails to start the cluster due to unavailability errors.'))
def start(clusters: Tuple[str], yes: bool, retry_until_up: bool):
    """Restart cluster(s).

    If a cluster is previously stopped (status is STOPPED) or failed in
    provisioning/Sky runtime setup (status is INIT), this command will attempt
    to start the cluster.  (In the second case, any failed setup steps are not
    performed and only a request to start the machines is attempted.)

    Auto-failover provisioning is not used when restarting stopped
    clusters. They will be started on the same cloud and region that was chosen
    before.

    If a cluster is already in the UP status, this command has no effect on it.

    Examples:

    .. code-block:: bash

      # Restart a specific cluster.
      sky start cluster_name
      \b
      # Restart multiple clusters.
      sky start cluster1 cluster2

    """
    to_start = []
    if clusters:
        # Get GLOB cluster names
        clusters = _get_glob_clusters(clusters)

        for name in clusters:
            cluster_status, _ = backend_utils.refresh_cluster_status_handle(
                name)
            # A cluster may have one of the following states:
            #
            #  STOPPED - ok to restart
            #    (currently, only AWS/GCP non-spot clusters can be in this
            #    state)
            #
            #  UP - skipped, see below
            #
            #  INIT - ok to restart:
            #    1. It can be a failed-to-provision cluster, so it isn't up
            #      (Ex: gpunode --gpus=A100:8).  Running `sky start` enables
            #      retrying the provisioning - without setup steps being
            #      completed. (Arguably the original command that failed should
            #      be used instead; but using start isn't harmful - after it
            #      gets provisioned successfully the user can use the original
            #      command).
            #
            #    2. It can be an up cluster that failed one of the setup steps.
            #      This way 'sky start' can change its status to UP, enabling
            #      'sky ssh' to debug things (otherwise `sky ssh` will fail an
            #      INIT state cluster due to head_ip not being cached).
            #
            #      This can be replicated by adding `exit 1` to Task.setup.
            if cluster_status == global_user_state.ClusterStatus.UP:
                # An UP cluster; skipping 'sky start' because:
                #  1. For a really up cluster, this has no effects (ray up -y
                #    --no-restart) anyway.
                #  2. A cluster may show as UP but is manually stopped in the
                #    UI.  If Azure/GCP: ray autoscaler doesn't support reusing,
                #    so 'sky start existing' will actually launch a new
                #    cluster with this name, leaving the original cluster
                #    zombied (remains as stopped in the cloud's UI).
                #
                #    This is dangerous and unwanted behavior!
                print(f'Cluster {name} already has status UP.')
                continue
            assert cluster_status in (
                global_user_state.ClusterStatus.INIT,
                global_user_state.ClusterStatus.STOPPED), cluster_status
            to_start.append(name)
    if not to_start:
        return

    if not yes:
        cluster_str = 'clusters' if len(to_start) > 1 else 'cluster'
        cluster_list = ', '.join(to_start)
        click.confirm(
            f'Restarting {len(to_start)} {cluster_str}: '
            f'{cluster_list}. Proceed?',
            default=True,
            abort=True,
            show_default=True)

    for name in to_start:
        _start_cluster(name, retry_until_up=retry_until_up)
        click.secho(f'Cluster {name} started.', fg='green')


@cli.command(cls=_DocumentedCodeCommand)
@click.argument('clusters', nargs=-1, required=False)
@click.option('--all',
              '-a',
              default=None,
              is_flag=True,
              help='Tear down all existing clusters.')
@click.option('--yes',
              '-y',
              is_flag=True,
              default=False,
              required=False,
              help='Skip confirmation prompt.')
@click.option('--purge',
              '-p',
              is_flag=True,
              default=False,
              required=False,
              help='Ignore cloud provider errors (if any). '
              'Useful for cleaning up manually deleted cluster(s).')
def down(
    clusters: Tuple[str],
    all: Optional[bool],  # pylint: disable=redefined-builtin
    yes: bool,
    purge: bool,
):
    """Tear down cluster(s).

    CLUSTER is the name of the cluster (or glob pattern) to tear down.  If both
    CLUSTER and ``--all`` are supplied, the latter takes precedence.

    Terminating a cluster will delete all associated resources (all billing
    stops), and any data on the attached disks will be lost.

    Accelerators (e.g., TPUs) that are part of the cluster will be deleted too.

    Examples:

    .. code-block:: bash

      # Tear down a specific cluster.
      sky down cluster_name
      \b
      # Tear down multiple clusters.
      sky down cluster1 cluster2
      \b
      # Tear down all clusters matching glob pattern 'cluster*'.
      sky down "cluster*"
      \b
      # Tear down all existing clusters.
      sky down -a

    """
    _terminate_or_stop_clusters(clusters,
                                apply_to_all=all,
                                terminate=True,
                                no_confirm=yes,
                                purge=purge)


def _terminate_or_stop_clusters(
        names: Tuple[str],
        apply_to_all: Optional[bool],
        terminate: bool,
        no_confirm: bool,
        purge: bool = False,
        idle_minutes_to_autostop: Optional[int] = None) -> None:
    """Terminates or (auto-)stops a cluster (or all clusters).

    Reserved clusters (spot controller) can only be terminated if the cluster
    name is explicitly and uniquely specified (not via glob) and purge is set
    to True.
    """
    assert idle_minutes_to_autostop is None or not terminate, (
        idle_minutes_to_autostop, terminate)
    command = 'down' if terminate else 'stop'
    if not names and apply_to_all is None:
        raise click.UsageError(
            f'sky {command} requires either a cluster name (see `sky status`) '
            'or --all.')

    operation = 'Terminating' if terminate else 'Stopping'
    if idle_minutes_to_autostop is not None:
        verb = 'Scheduling' if idle_minutes_to_autostop >= 0 else 'Cancelling'
        operation = f'{verb} auto-stop on'

    if len(names) > 0:
        reserved_clusters = [
            name for name in names
            if name in backend_utils.SKY_RESERVED_CLUSTER_NAMES
        ]
        reserved_clusters_str = ', '.join(map(repr, reserved_clusters))
        names = [
            name for name in _get_glob_clusters(names)
            if name not in backend_utils.SKY_RESERVED_CLUSTER_NAMES
        ]
        # Make sure the reserved clusters are explicitly specified without other
        # normal clusters and purge is True.
        if len(reserved_clusters) > 0:
            if not purge:
                msg = (f'{operation} Sky reserved cluster(s) '
                       f'{reserved_clusters_str} is not supported.')
                if terminate:
                    msg += (
                        '\nPlease specify --purge to force termination of the '
                        'reserved cluster(s).')
                raise click.UsageError(msg)
            if len(names) != 0:
                names_str = ', '.join(map(repr, names))
                raise click.UsageError(
                    f'{operation} Sky reserved cluster(s) '
                    f'{reserved_clusters_str} with multiple other cluster(s) '
                    f'{names_str} is not supported.\n'
                    f'Please omit the reserved cluster(s) {reserved_clusters}.')
        names += reserved_clusters

    if apply_to_all:
        all_clusters = global_user_state.get_clusters()
        if len(names) > 0:
            print(f'Both --all and cluster(s) specified for sky {command}. '
                  'Letting --all take effect.')
        # We should not remove reserved clusters when --all is specified.
        # Otherwise, it would be very easy to accidentally delete a reserved
        # cluster.
        names = [
            record['name']
            for record in all_clusters
            if record['name'] not in backend_utils.SKY_RESERVED_CLUSTER_NAMES
        ]

    clusters = []
    for name in names:
        handle = global_user_state.get_handle_from_cluster_name(name)
        if handle is None:
            # This codepath is used for 'sky down -p <controller>' when the
            # controller is not in 'sky status'.  Cluster-not-found message
            # should've been printed by _get_glob_clusters() above.
            continue
        clusters.append({'name': name, 'handle': handle})

    if not clusters:
        print('\nCluster(s) not found (tip: see `sky status`).')
        return

    if not no_confirm and len(clusters) > 0:
        cluster_str = 'clusters' if len(clusters) > 1 else 'cluster'
        cluster_list = ', '.join([r['name'] for r in clusters])
        click.confirm(
            f'{operation} {len(clusters)} {cluster_str}: '
            f'{cluster_list}. Proceed?',
            default=True,
            abort=True,
            show_default=True)
        # Add a blank line to separate the confirmation prompt from the
        # progress bar.
        click.echo()

    plural = 's' if len(clusters) > 1 else ''
    progress = rich_progress.Progress(transient=True,
                                      redirect_stdout=False,
                                      redirect_stderr=False)
    task = progress.add_task(
        f'[bold cyan]{operation} {len(clusters)} cluster{plural}[/]',
        total=len(clusters))

    def _terminate_or_stop(record):
        name = record['name']
        handle = record['handle']
        backend = backend_utils.get_backend_from_handle(handle)
        success_progress = False
        if (isinstance(backend, backends.CloudVmRayBackend) and
                handle.launched_resources.use_spot and not terminate):
            # Disable spot instances to be stopped.
            # TODO(suquark): enable GCP+spot to be stopped in the future.
            message = (
                f'{colorama.Fore.YELLOW}Stopping cluster {name}... skipped.'
                f'{colorama.Style.RESET_ALL}\n'
                '  Stopping spot instances is not supported as the attached '
                'disks will be lost.\n'
                '  To terminate the cluster instead, run: '
                f'{colorama.Style.BRIGHT}sky down {name}'
                f'{colorama.Style.RESET_ALL}')
        elif idle_minutes_to_autostop is not None:
            (cluster_status,
             handle) = backend_utils.refresh_cluster_status_handle(name)
            if not isinstance(backend, backends.CloudVmRayBackend):
                message = (f'{colorama.Fore.YELLOW}{operation} cluster '
                           f'{name}... skipped{colorama.Style.RESET_ALL}'
                           '\n  Auto-stopping is only supported by backend: '
                           f'{backends.CloudVmRayBackend.NAME}')
            else:
                if cluster_status != global_user_state.ClusterStatus.UP:
                    message = (
                        f'{colorama.Fore.YELLOW}{operation} cluster '
                        f'{name} (status: {cluster_status.value})... skipped'
                        f'{colorama.Style.RESET_ALL}'
                        '\n  Auto-stop can only be set/unset for '
                        f'{global_user_state.ClusterStatus.UP.value} clusters.')
                else:
                    backend.set_autostop(handle, idle_minutes_to_autostop)
                    message = (
                        f'{colorama.Fore.GREEN}{operation} '
                        f'cluster {name}...done{colorama.Style.RESET_ALL}')
                    if idle_minutes_to_autostop >= 0:
                        message += (
                            f'\n  The cluster will be stopped after '
                            f'{idle_minutes_to_autostop} minutes of idleness.'
                            '\n  To cancel the autostop, run: '
                            f'{colorama.Style.BRIGHT}'
                            f'sky autostop {name} --cancel'
                            f'{colorama.Style.RESET_ALL}')
                    success_progress = True
        else:
            success = backend.teardown(handle, terminate=terminate, purge=purge)
            if success:
                message = (
                    f'{colorama.Fore.GREEN}{operation} cluster {name}...done.'
                    f'{colorama.Style.RESET_ALL}')
                if not terminate:
                    message += ('\n  To restart the cluster, run: '
                                f'{colorama.Style.BRIGHT}sky start {name}'
                                f'{colorama.Style.RESET_ALL}')
                success_progress = True
            else:
                message = (
                    f'{colorama.Fore.RED}{operation} cluster {name}...failed. '
                    'Please check the logs and try again.'
                    f'{colorama.Style.RESET_ALL}')
        progress.stop()
        click.echo(message)
        if success_progress:
            progress.update(task, advance=1)
        progress.start()

    with progress:
        subprocess_utils.run_in_parallel(_terminate_or_stop, clusters)
        progress.live.transient = False
        # Make sure the progress bar not mess up the terminal.
        progress.refresh()


@_interactive_node_cli_command
# pylint: disable=redefined-outer-name
def gpunode(cluster: str, yes: bool, port_forward: Optional[List[int]],
            cloud: Optional[str], instance_type: Optional[str],
            gpus: Optional[str], use_spot: Optional[bool],
            screen: Optional[bool], tmux: Optional[bool],
            disk_size: Optional[int]):
    """Launch or attach to an interactive GPU node.

    Examples:

    .. code-block:: bash

        # Launch a default gpunode.
        sky gpunode
        \b
        # Do work, then log out. The node is kept running. Attach back to the
        # same node and do more work.
        sky gpunode
        \b
        # Create many interactive nodes by assigning names via --cluster (-c).
        sky gpunode -c node0
        sky gpunode -c node1
        \b
        # Port forward.
        sky gpunode --port-forward 8080 --port-forward 4650 -c cluster_name
        sky gpunode -p 8080 -p 4650 -c cluster_name
        \b
        # Sync current working directory to ~/workdir on the node.
        rsync -r . cluster_name:~/workdir

    """
    # TODO: Factor out the shared logic below for [gpu|cpu|tpu]node.
    if screen and tmux:
        raise click.UsageError('Cannot use both screen and tmux.')

    session_manager = None
    if screen or tmux:
        session_manager = 'tmux' if tmux else 'screen'
    name = cluster
    if name is None:
        name = _default_interactive_node_name('gpunode')

    user_requested_resources = not (cloud is None and instance_type is None and
                                    gpus is None and use_spot is None)
    default_resources = _INTERACTIVE_NODE_DEFAULT_RESOURCES['gpunode']
    cloud_provider = clouds.CLOUD_REGISTRY.from_str(cloud)
    if gpus is None and instance_type is None:
        # Use this request if both gpus and instance_type are not specified.
        gpus = default_resources.accelerators
        instance_type = default_resources.instance_type
    if use_spot is None:
        use_spot = default_resources.use_spot
    resources = sky.Resources(cloud=cloud_provider,
                              instance_type=instance_type,
                              accelerators=gpus,
                              use_spot=use_spot,
                              disk_size=disk_size)

    _create_and_ssh_into_node(
        'gpunode',
        resources,
        cluster_name=name,
        port_forward=port_forward,
        session_manager=session_manager,
        user_requested_resources=user_requested_resources,
        no_confirm=yes,
    )


@_interactive_node_cli_command
# pylint: disable=redefined-outer-name
def cpunode(cluster: str, yes: bool, port_forward: Optional[List[int]],
            cloud: Optional[str], instance_type: Optional[str],
            use_spot: Optional[bool], screen: Optional[bool],
            tmux: Optional[bool], disk_size: Optional[int]):
    """Launch or attach to an interactive CPU node.

    Examples:

    .. code-block:: bash

        # Launch a default cpunode.
        sky cpunode
        \b
        # Do work, then log out. The node is kept running. Attach back to the
        # same node and do more work.
        sky cpunode
        \b
        # Create many interactive nodes by assigning names via --cluster (-c).
        sky cpunode -c node0
        sky cpunode -c node1
        \b
        # Port forward.
        sky cpunode --port-forward 8080 --port-forward 4650 -c cluster_name
        sky cpunode -p 8080 -p 4650 -c cluster_name
        \b
        # Sync current working directory to ~/workdir on the node.
        rsync -r . cluster_name:~/workdir

    """
    if screen and tmux:
        raise click.UsageError('Cannot use both screen and tmux.')

    session_manager = None
    if screen or tmux:
        session_manager = 'tmux' if tmux else 'screen'
    name = cluster
    if name is None:
        name = _default_interactive_node_name('cpunode')

    user_requested_resources = not (cloud is None and instance_type is None and
                                    use_spot is None)
    default_resources = _INTERACTIVE_NODE_DEFAULT_RESOURCES['cpunode']
    cloud_provider = clouds.CLOUD_REGISTRY.from_str(cloud)
    if instance_type is None:
        instance_type = default_resources.instance_type
    if use_spot is None:
        use_spot = default_resources.use_spot
    resources = sky.Resources(cloud=cloud_provider,
                              instance_type=instance_type,
                              use_spot=use_spot,
                              disk_size=disk_size)

    _create_and_ssh_into_node(
        'cpunode',
        resources,
        cluster_name=name,
        port_forward=port_forward,
        session_manager=session_manager,
        user_requested_resources=user_requested_resources,
        no_confirm=yes,
    )


@_interactive_node_cli_command
# pylint: disable=redefined-outer-name
def tpunode(cluster: str, yes: bool, port_forward: Optional[List[int]],
            instance_type: Optional[str], tpus: Optional[str],
            use_spot: Optional[bool], screen: Optional[bool],
            tmux: Optional[bool], disk_size: Optional[int]):
    """Launch or attach to an interactive TPU node.

    Examples:

    .. code-block:: bash

        # Launch a default tpunode.
        sky tpunode
        \b
        # Do work, then log out. The node is kept running. Attach back to the
        # same node and do more work.
        sky tpunode
        \b
        # Create many interactive nodes by assigning names via --cluster (-c).
        sky tpunode -c node0
        sky tpunode -c node1
        \b
        # Port forward.
        sky tpunode --port-forward 8080 --port-forward 4650 -c cluster_name
        sky tpunode -p 8080 -p 4650 -c cluster_name
        \b
        # Sync current working directory to ~/workdir on the node.
        rsync -r . cluster_name:~/workdir

    """
    if screen and tmux:
        raise click.UsageError('Cannot use both screen and tmux.')

    session_manager = None
    if screen or tmux:
        session_manager = 'tmux' if tmux else 'screen'
    name = cluster
    if name is None:
        name = _default_interactive_node_name('tpunode')

    user_requested_resources = not (instance_type is None and tpus is None and
                                    use_spot is None)
    default_resources = _INTERACTIVE_NODE_DEFAULT_RESOURCES['tpunode']
    if instance_type is None:
        instance_type = default_resources.instance_type
    if tpus is None:
        tpus = default_resources.accelerators
    if use_spot is None:
        use_spot = default_resources.use_spot
    resources = sky.Resources(cloud=sky.GCP(),
                              instance_type=instance_type,
                              accelerators=tpus,
                              use_spot=use_spot,
                              disk_size=disk_size)

    _create_and_ssh_into_node(
        'tpunode',
        resources,
        cluster_name=name,
        port_forward=port_forward,
        session_manager=session_manager,
        user_requested_resources=user_requested_resources,
        no_confirm=yes,
    )


@cli.command()
def check():
    """Determine the set of clouds available to use.

    This checks access credentials for AWS, Azure and GCP; on failure, it shows
    the reason and suggests correction steps. Sky tasks will only run on clouds
    that you have access to.
    """
    sky_check.check()


@cli.command()
@click.argument('gpu_name', required=False)
@click.option('--all',
              '-a',
              is_flag=True,
              default=False,
              help='Show details of all GPU/TPU/accelerator offerings.')
@click.option('--cloud',
              default=None,
              type=str,
              help='Cloud provider to query.')
def show_gpus(gpu_name: Optional[str], all: bool, cloud: Optional[str]):  # pylint: disable=redefined-builtin
    """Show supported GPU/TPU/accelerators.

    To show the detailed information of a GPU/TPU type (which clouds offer it,
    the quantity in each VM type, etc.), use ``sky show-gpus <gpu>``.

    To show all GPUs, including less common ones and their detailed
    information, use ``sky show-gpus --all``.

    NOTE: The price displayed for each instance type is the lowest across all
    regions for both on-demand and spot instances.
    """
    show_all = all
    if show_all and gpu_name is not None:
        raise click.UsageError('--all is only allowed without a GPU name.')

    def _list_to_str(lst):
        return ', '.join([str(e) for e in lst])

    def _output():
        gpu_table = log_utils.create_table(
            ['NVIDIA_GPU', 'AVAILABLE_QUANTITIES'])
        tpu_table = log_utils.create_table(
            ['GOOGLE_TPU', 'AVAILABLE_QUANTITIES'])
        other_table = log_utils.create_table(
            ['OTHER_GPU', 'AVAILABLE_QUANTITIES'])

        if gpu_name is None:
            result = service_catalog.list_accelerator_counts(gpus_only=True,
                                                             clouds=cloud)
            # NVIDIA GPUs
            for gpu in service_catalog.get_common_gpus():
                if gpu in result:
                    gpu_table.add_row([gpu, _list_to_str(result.pop(gpu))])
            yield from gpu_table.get_string()

            # Google TPUs
            for tpu in service_catalog.get_tpus():
                if tpu in result:
                    tpu_table.add_row([tpu, _list_to_str(result.pop(tpu))])
            if len(tpu_table.get_string()) > 0:
                yield '\n\n'
            yield from tpu_table.get_string()

            # Other GPUs
            if show_all:
                yield '\n\n'
                for gpu, qty in sorted(result.items()):
                    other_table.add_row([gpu, _list_to_str(qty)])
                yield from other_table.get_string()
                yield '\n\n'
            else:
                return

        # Show detailed accelerator information
        result = service_catalog.list_accelerators(gpus_only=True,
                                                   name_filter=gpu_name,
                                                   clouds=cloud)
        if len(result) == 0:
            yield f'Resources \'{gpu_name}\' not found. '
            yield 'Try \'sky show-gpus --all\' '
            yield 'to show available accelerators.'
            return

        yield '*NOTE*: for most GCP accelerators, '
        yield 'INSTANCE_TYPE == (attachable) means '
        yield 'the host VM\'s cost is not included.\n\n'
        import pandas as pd  # pylint: disable=import-outside-toplevel
        for i, (gpu, items) in enumerate(result.items()):
            accelerator_table = log_utils.create_table([
                'GPU',
                'QTY',
                'CLOUD',
                'INSTANCE_TYPE',
                'HOST_MEMORY',
                'HOURLY_PRICE',
                'HOURLY_SPOT_PRICE',
            ])
            for item in items:
                instance_type_str = item.instance_type if not pd.isna(
                    item.instance_type) else '(attachable)'
                mem_str = f'{item.memory:.0f}GB' if item.memory > 0 else '-'
                price_str = f'$ {item.price:.3f}' if not pd.isna(
                    item.price) else '-'
                spot_price_str = f'$ {item.spot_price:.3f}' if not pd.isna(
                    item.spot_price) else '-'
                accelerator_table.add_row([
                    item.accelerator_name, item.accelerator_count, item.cloud,
                    instance_type_str, mem_str, price_str, spot_price_str
                ])

            if i != 0:
                yield '\n\n'
            yield from accelerator_table.get_string()

    if show_all:
        click.echo_via_pager(_output())
    else:
        for out in _output():
            click.echo(out, nl=False)
        click.echo()


@cli.group(cls=_NaturalOrderGroup)
def storage():
    """Storage related commands."""
    pass


@storage.command('ls', cls=_DocumentedCodeCommand)
def storage_ls():
    """List storage objects created."""
    storage_stat = global_user_state.get_storage()
    storage_table = log_utils.create_table([
        'NAME',
        'CREATED',
        'STORE',
        'COMMAND',
        'STATUS',
    ])

    for row in storage_stat:
        launched_at = row['launched_at']
        storage_table.add_row([
            # NAME
            row['name'],
            # LAUNCHED
            log_utils.readable_time_duration(launched_at),
            # CLOUDS
            ', '.join([s.value for s in row['handle'].sky_stores.keys()]),
            # COMMAND
            row['last_use'],
            # STATUS
            row['status'].value,
        ])
    if storage_stat:
        click.echo(storage_table)
    else:
        click.echo('No existing storage.')


@storage.command('delete', cls=_DocumentedCodeCommand)
@click.option('--all',
              '-a',
              default=False,
              is_flag=True,
              required=False,
              help='Delete all storage objects.')
@click.argument('name', required=False, type=str, nargs=-1)
def storage_delete(all: bool, name: str):  # pylint: disable=redefined-builtin
    """Delete storage objects.

    Examples:

    .. code-block:: bash

        # Delete two storage objects.
        sky storage delete imagenet cifar10
        \b
        # Delete all storage objects.
        sky storage delete -a
    """
    if all:
        click.echo('Deleting all storage objects.')
        storages = global_user_state.get_storage()
        for row in storages:
            store_object = data.Storage(name=row['name'],
                                        source=row['handle'].source,
                                        sync_on_reconstruction=False)
            store_object.delete()
    elif name:
        for n in name:
            handle = global_user_state.get_handle_from_storage_name(n)
            if handle is None:
                click.echo(f'Storage name {n} not found.')
            else:
                click.echo(f'Deleting storage object {n}.')
                store_object = data.Storage(name=handle.storage_name,
                                            source=handle.source,
                                            sync_on_reconstruction=False)
                store_object.delete()
    else:
        raise click.ClickException(
            'Must pass in \'-a/--all\' or storage names to \'sky '
            'storage delete\'.')


# Managed Spot CLIs


def _is_spot_controller_up(
    stopped_message: str,
) -> Tuple[Optional[global_user_state.ClusterStatus],
           Optional[backends.Backend.ResourceHandle]]:
    controller_status, handle = backend_utils.refresh_cluster_status_handle(
        spot_lib.SPOT_CONTROLLER_NAME, force_refresh=True)
    if controller_status is None:
        click.echo('No managed spot job has been run.')
    elif controller_status != global_user_state.ClusterStatus.UP:
        msg = (f'Spot controller {spot_lib.SPOT_CONTROLLER_NAME} '
               f'is {controller_status.value}.')
        if controller_status == global_user_state.ClusterStatus.STOPPED:
            msg += f'\n{stopped_message}'
        if controller_status == global_user_state.ClusterStatus.INIT:
            msg += '\nPlease wait for the controller to be ready.'
        click.echo(msg)
        handle = None
    return controller_status, handle


@cli.group(cls=_NaturalOrderGroup)
def spot():
    """Managed spot instances related commands."""
    pass


@spot.command('launch', cls=_DocumentedCodeCommand)
@click.argument('entrypoint', required=True, type=str, nargs=-1)
# TODO(zhwu): Add --dryrun option to test the launch command.
@_add_click_options(_TASK_OPTIONS)
@click.option('--spot-recovery',
              default=None,
              type=str,
              help='Spot recovery strategy to use for the managed spot task.')
@click.option('--disk-size',
              default=None,
              type=int,
              required=False,
              help=('OS disk size in GBs.'))
@click.option('--detach-run',
              '-d',
              default=False,
              is_flag=True,
              help='If True, run setup first (blocking), '
              'then detach from the job\'s execution.')
@click.option('--yes',
              '-y',
              is_flag=True,
              default=False,
              required=False,
              help='Skip confirmation prompt.')
# FIXME(suquark): adding the following @timeline.event decorator makes docs
# unable to show the docstring for this CLI command.
# @timeline.event
def spot_launch(
    entrypoint: str,
    name: Optional[str],
    workdir: Optional[str],
    cloud: Optional[str],
    region: Optional[str],
    gpus: Optional[str],
    num_nodes: Optional[int],
    use_spot: Optional[bool],
    image_id: Optional[str],
    spot_recovery: Optional[str],
    env: List[Dict[str, str]],
    disk_size: Optional[int],
    detach_run: bool,
    yes: bool,
):
    """Launch a managed spot job."""
    if name is None:
        name = backend_utils.generate_cluster_name()
    else:
        backend_utils.check_cluster_name_is_valid(name)

    dag = _make_dag_from_entrypoint_with_overrides(
        entrypoint,
        name=name,
        workdir=workdir,
        cloud=cloud,
        region=region,
        gpus=gpus,
        num_nodes=num_nodes,
        use_spot=use_spot,
        image_id=image_id,
        env=env,
        disk_size=disk_size,
        spot_recovery=spot_recovery,
    )

    if not yes:
        prompt = f'Launching a new spot task {name!r}. Proceed?'
        if prompt is not None:
            click.confirm(prompt, default=True, abort=True, show_default=True)

    assert len(dag.tasks) == 1, dag
    task = dag.tasks[0]
    assert len(task.resources) == 1, task
    resources = list(task.resources)[0]

    change_default_value = dict()
    if not resources.use_spot_specified:
        logger.info('Field use_spot not specified; defaulting to True.')
        change_default_value['use_spot'] = True
    if resources.spot_recovery is None:
        logger.info('No spot recovery strategy specified; defaulting to '
                    f'{spot_lib.SPOT_DEFAULT_STRATEGY}.')
        change_default_value['spot_recovery'] = spot_lib.SPOT_DEFAULT_STRATEGY

    new_resources = resources.copy(**change_default_value)
    task.set_resources({new_resources})

    if task.run is None:
        click.secho(
            'Skipping the managed spot task as the run section is not set.',
            fg='green')
        return

    # TODO(zhwu): Refactor the Task (as Resources), so that we can enforce the
    # following validations.
    # Check the file mounts in the task.
    # Disallow all local file mounts (copy mounts).
    if task.workdir is not None:
        raise click.UsageError('Workdir is not allowed for managed spot jobs.')
    copy_mounts = task.get_local_to_remote_file_mounts()
    if copy_mounts:
        copy_mounts_str = '\n\t'.join(': '.join(m) for m in copy_mounts)
        raise click.UsageError(
            'Local file mounts are not allowed for managed spot jobs, '
            f'but following are found: {copy_mounts_str}')

    # Copy the local source to a bucket. The task will not be executed locally,
    # so we need to copy the files to the bucket manually here before sending to
    # the remote spot controller.
    task.add_storage_mounts()

    # Replace the source field that is local path in all storage_mounts with
    # bucket URI and remove the name field.
    for storage_obj in task.storage_mounts.values():
        if (storage_obj.source is not None and
                not data_utils.is_cloud_store_url(storage_obj.source)):
            # Need to replace the local path with bucket URI, and remove the
            # name field, so that the sky storage mount can work on the spot
            # controller.
            store_types = list(storage_obj.stores.keys())
            assert len(store_types) == 1, (
                'We only support one store type for now.', storage_obj.stores)
            store_type = store_types[0]
            if store_type == StoreType.S3:
                storage_obj.source = f's3://{storage_obj.name}'
            elif store_type == StoreType.GCS:
                storage_obj.source = f'gs://{storage_obj.name}'
            else:
                with ux_utils.print_exception_no_traceback():
                    raise ValueError(f'Unsupported store type: {store_type}')
            storage_obj.name = None

    with tempfile.NamedTemporaryFile(prefix=f'sky-spot-task-{name}-',
                                     mode='w') as f:
        task_config = task.to_yaml_config()
        backend_utils.dump_yaml(f.name, task_config)

        controller_name = spot_lib.SPOT_CONTROLLER_NAME
        yaml_path = backend_utils.fill_template(
            spot_lib.SPOT_CONTROLLER_TEMPLATE, {
                'remote_user_yaml_prefix': spot_lib.SPOT_TASK_YAML_PREFIX,
                'user_yaml_path': f.name,
                'spot_controller': controller_name,
                'cluster_name': name,
                'sky_remote_path': backend_utils.SKY_REMOTE_PATH,
            },
            output_prefix=spot_lib.SPOT_CONTROLLER_YAML_PREFIX)
        with sky.Dag() as dag:
            controller_task = sky.Task.from_yaml(yaml_path)
            controller_task.spot_task = task
            assert len(controller_task.resources) == 1
        click.secho(
            f'Launching managed spot job {name} from spot controller...',
            fg='yellow')
        click.echo('Launching spot controller...')
        sky.launch(dag,
                   stream_logs=True,
                   cluster_name=controller_name,
                   detach_run=detach_run,
                   idle_minutes_to_autostop=spot_lib.
                   SPOT_CONTROLLER_IDLE_MINUTES_TO_AUTOSTOP,
                   is_spot_controller_task=True)


@spot.command('status', cls=_DocumentedCodeCommand)
@click.option('--all',
              '-a',
              default=False,
              is_flag=True,
              required=False,
              help='Show all information in full.')
@click.option(
    '--refresh',
    '-r',
    default=False,
    is_flag=True,
    required=False,
    help='Query the latest statuses, restarting the spot controller if stopped.'
)
# pylint: disable=redefined-builtin
def spot_status(all: bool, refresh: bool):
    """Show statuses of managed spot jobs.

    \b
    Each spot job can have one of the following statuses:

    \b
    - SUBMITTED: The job is submitted to the spot controller.
    - STARTING: The job is starting (starting a spot cluster).
    - RUNNING: The job is running.
    - RECOVERING: The spot cluster is recovering from a preemption.
    - SUCCEEDED: The job succeeded.
    - FAILED: The job failed due to an error from the job itself.
    - FAILED_NO_RESOURCES: The job failed due to resources being unavailable
        after a maximum number of retry attempts.
    - FAILED_CONTROLLER: The job failed due to an unexpected error in the spot
        controller.
    - CANCELLED: The job was cancelled by the user.

    If the job failed, either due to user code or spot unavailability, the error
    log can be found with ``sky logs sky-spot-controller job_id``.
    """
    click.secho('Fetching managed spot job statuses...', fg='yellow')
    cache = spot_lib.load_job_table_cache()
    stop_msg = ''
    if not refresh:
        stop_msg = 'To view the latest job table: sky spot status --refresh'
    controller_status, handle = _is_spot_controller_up(stop_msg)

    if (refresh and controller_status in [
            global_user_state.ClusterStatus.STOPPED,
            global_user_state.ClusterStatus.INIT
    ]):
        click.secho('Restarting controller for latest status...', fg='yellow')
        handle = _start_cluster(spot_lib.SPOT_CONTROLLER_NAME,
                                idle_minutes_to_autostop=spot_lib.
                                SPOT_CONTROLLER_IDLE_MINUTES_TO_AUTOSTOP)

    if handle is None:
        job_table_str = 'No cached job status table found.'
        if cache is not None:
            readable_time = log_utils.readable_time_duration(cache[0])
            job_table_str = (
                f'\n{colorama.Fore.YELLOW}Cached job status table '
                f'[last updated: {readable_time}]:{colorama.Style.RESET_ALL}\n'
                f'{cache[1]}\n')
        click.echo(job_table_str)
        return

    backend = backend_utils.get_backend_from_handle(handle)
    assert isinstance(backend, backends.CloudVmRayBackend)

    code = spot_lib.SpotCodeGen.show_jobs(show_all=all)
    returncode, job_table_str, stderr = backend.run_on_head(
        handle, code, require_outputs=True, stream_logs=False)
    subprocess_utils.handle_returncode(returncode, code,
                                       'Failed to fetch managed job statuses',
                                       job_table_str + stderr)

    spot_lib.dump_job_table_cache(job_table_str)
    click.echo(f'Managed spot jobs:\n{job_table_str}')


@spot.command('cancel', cls=_DocumentedCodeCommand)
@click.option('--name',
              '-n',
              required=False,
              type=str,
              help='Managed spot job name to cancel.')
@click.argument('job_ids', default=None, type=int, required=False, nargs=-1)
@click.option('--all',
              '-a',
              is_flag=True,
              default=False,
              required=False,
              help='Cancel all managed spot jobs.')
@click.option('--yes',
              '-y',
              is_flag=True,
              default=False,
              required=False,
              help='Skip confirmation prompt.')
# pylint: disable=redefined-builtin
def spot_cancel(name: Optional[str], job_ids: Tuple[int], all: bool, yes: bool):
    """Cancel managed spot jobs.

    You can provide either a job name or a list of job ids to be cancelled.
    They are exclusive options.
    Examples:

    .. code-block:: bash

        # Cancel managed spot job with name 'my-job'
        $ sky spot cancel -n my-job

        # Cancel managed spot jobs with IDs 1, 2, 3
        $ sky spot cancel 1 2 3

    """

    _, handle = _is_spot_controller_up(
        'All managed spot jobs should have finished.')
    if handle is None:
        return

    job_id_str = ','.join(map(str, job_ids))
    if sum([len(job_ids) > 0, name is not None, all]) != 1:
        argument_str = f'--job-ids {job_id_str}' if len(job_ids) > 0 else ''
        argument_str += f' --name {name}' if name is not None else ''
        argument_str += ' --all' if all else ''
        raise click.UsageError(
            'Can only specify one of JOB_IDS or --name or --all. '
            f'Provided {argument_str!r}.')

    if not yes:
        job_identity_str = f'with IDs {job_id_str}' if job_ids else repr(name)
        if all:
            job_identity_str = 'all managed spot jobs'
        click.confirm(
            f'Cancelling managed spot job {job_identity_str}. Proceed?',
            default=True,
            abort=True,
            show_default=True)

    backend = backend_utils.get_backend_from_handle(handle)
    assert isinstance(backend, backends.CloudVmRayBackend)
    codegen = spot_lib.SpotCodeGen
    if all:
        code = codegen.cancel_jobs_by_id(None)
    elif job_ids:
        code = codegen.cancel_jobs_by_id(job_ids)
    else:
        code = codegen.cancel_job_by_name(name)
    # The stderr is redirected to stdout
    returncode, stdout, _ = backend.run_on_head(handle,
                                                code,
                                                require_outputs=True,
                                                stream_logs=False)
    subprocess_utils.handle_returncode(returncode, code,
                                       'Failed to cancel managed spot job',
                                       stdout)

    click.echo(stdout)
    if 'Multiple jobs found with name' in stdout:
        click.echo('Please specify the job ID instead of the job name.')


@spot.command('logs', cls=_DocumentedCodeCommand)
@click.option('--name',
              '-n',
              required=False,
              type=str,
              help='Managed spot job name.')
@click.argument('job_id', required=False, type=int)
def spot_logs(name: Optional[str], job_id: Optional[int]):
    """Tail the log of a managed spot job."""
    # TODO(zhwu): Automatically restart the spot controller
    _, handle = _is_spot_controller_up(
        'Please restart the spot controller with '
        '`sky start sky-spot-controller`.')
    if handle is None:
        return

    if name is not None and job_id is not None:
        click.UsageError('Cannot specify both --name and --job-id.')
    backend = backend_utils.get_backend_from_handle(handle)
    # Stream the realtime logs
    backend.tail_spot_logs(handle, job_id=job_id, job_name=name)


def main():
    return cli()


if __name__ == '__main__':
    main()
